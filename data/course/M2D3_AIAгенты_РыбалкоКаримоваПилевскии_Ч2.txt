... но он шире, то есть он защищает от таких, ну, учительных тем. То есть модель, там, не, там, политику, что не хорошо говорить, она там возвращает заглушку просто. Вы, как пользователь, можете, там, типа, отлавливать ее, заменять своими, если нужно. Это не прополит все. И туда же встают, например, там, пробки, герции, вот, и все вещи. Да, например, у нас на сайте появляется агент, который, там, условно, консультирует по меню. Да. И я его начинаю, вдруг у меня там в конце составляют какую-нибудь фитнес-программу, там, на неделю. Ну, вообще 100% такой бюджет нельзя сейчас. То есть нужно какие-то свои... Это как бы решается отчасти промдол. Хорошие модельки, большие, они достаточно строго следуют промдол, их тяжело вывести. Дополнительно навешивается, обычно еще легкая моделька, которая условно делает проверку. То есть, например, в нее влетает запрос, и она проверяет, пользователь говорит, а нужна ли тематика или нет. Такой бинарный конструктор, условно. Тематика моя или нет? Если не моя, то просто, опять же, после загрузка, пожалуйста, давайте унемся к теме, поговорим в том. То есть это решается каскадом из двух моделей. Базовая модель, отвечая на вопрос, воплога действий, и одна такая модель, проверка, которая проверяет. Ладно, спасибо большое. Obrigado. Мы поговорили, по сути, давайте чуть вспомним, какая канва, что мы хотим рассказать. У нас есть LLM, обогащаем эту LLM инструментами, и мы познакомились с двумя инструментами. Первый инструмент – это вызов, по сути, внешних опишек, где может быть много разных инструментов через MCP. Второй инструмент – это инструмент поиска, где мы ищем какую-то информацию по базе знаний, по текстовым данным. Но часто есть история, я думаю, что тоже сталкивались. Если в какой-нибудь OpenAge от GPT спросить информацию про самые последние новости, что было вчера, какой курс акций вчера был. то модель сама не может отвечать об этих знаниях. Она пишет, что я пошла в интернет. И, соответственно, третий инструмент, о котором я хотел сегодня поговорить, это инструмент поиска по интернету. Но на самом деле, если смотреть опять же на его базовую реализацию, мы сегодня смотрим скорее не на конечные решения, а на такие реализации, как это работает технически, то по сути это тот же самый рак. Только в качестве базы знаний в этом раге используется не ваша база и не ваши документы, а весь поисковый индекс, который в нашем случае есть в Яндексе. То есть Яндекс, у него есть условно роботы, которые ходят по сайтам, сканируют эти сайты, складывают в поисковый индекс, преобрабатывают и так далее, и все это там лежит. Мы можем воспользоваться им как поисковым индексом, и процесс будет выглядеть следующим образом. Модель понимает, что нужно сказать в интернет, обращается к этому поиску, Поиск идет, ищет нужные страницы, нужную информацию, берет контент страниц, который он там нашел, передает в модель, и дальше модель отвечает по этим страницам. И вот у нас есть первый базовый кубик, чтобы собирать условно депресеч сложных агентов. То есть, по сути, если мы говорим, что мы хотим от этого перейти к депресечу, мы дальше уже поверх этого реализуем агентскую логику. Типа мы в инструкции можем прописать, сходи сначала туда, поищи такую-то информацию, потом поищи другую. И так происходит многократный вызов инструментов поиска, чтобы решить какую-то задачу. Пару слов про сценарии. Сценариев может быть очень много из тех, что я часто сталкивался. Например, это проверка терминологии. Мы говорим про медицинскую тематику, и там может быть какая-то специфическая терминология, которую нужно проверять на конкретном медицинском сайте, где эти понятия описаны. То есть, может быть, модель сама не очень хорошо понимает, недообучалась в медицинской тематике, но мы можем ходить на сайт, найти нужный термин, подложить его в контекст, и модель будет знать про этот термин. Проверка фактов, актуальные данные, расскажи мне про новости, отвечай по новостям. В общем-то, все, где нужна самая последняя информация в интернете, все это решается этим инструментом. Тут, наверное, достаточно просто, Поэтому долго останавливаться не будем. И мы поговорили про инструменты. Самые базовые блоки инструмента мы здесь покрыли. Есть еще, вот часто спрашивают сейчас, еще один инструмент, который во многих провайдерах есть. У нас в Яндексе пока нет, но мы его тоже сейчас делаем. Это инструмент выполнения кода. Что здесь имеется в виду? По сути, точно так же, как и со всеми другими инструментами, это штука, которая стоит рядом с моделью. Это контейнер, виртуалка, неважно, какая-то среда, где может выполняться код. Как выглядит процесс? Мы делаем модели запрос, она генерирует код, чтобы решить задачу. Дальше этот код выполняется в каком-то соседнем окружении. Но, опять же, возможно галлюцинации, этот код может не выполниться, может выполниться с ошибкой. Возвращается ошибка, модель дебажит, исправляет, снова передает, Код выполняется и так до тех пор, пока не получается работающий код, который возвращает конечный результат. И многие задачи, например, самый частый пример это математика. Сложные задачки, модель сама по себе, если она не специфическая, если она специально не обучалась под математику, она их, скорее всего, решать хорошо не будет. Ну, не знаю, какие-то очень долгие вычисления, перемножения. Чаще всего это решается просто вызовом инструмента. Модель, опять же, понимает, ага, мне нужно вызвать инструмент генерации кода, чтобы решить эту задачу. Она генерирует код, чтобы посчитать там А плюс Б, возвращает результат. Ну и получается, что да, вот модель моя, ЛМК, решает математические примеры. Хотя на самом деле решает это условно питоновский код, который моделька просто сгенерировала. Это вот еще один такой популярный инструмент. Он лежит, например, в основе вот этих более продвинутых агентов, уже конечных, которые там предлагает антропик для реализации кода. Это все, что касается базовых блоков, на которых сейчас строятся почти все агенты, которые вы видите, которые вы работаете. Manus работает по такой же схеме. Дальше, чтобы агент работал хорошо, чтобы он научал корректно, его нужно оснастить памятью. То есть, если я буду ему что-то говорить, он будет отвечать. И дальше следующий вопрос никак не будет учитывать предыдущий мой вопрос. Такой агент будет глуповат. И теперь мы говорим про отдельный модуль, модуль памяти. И это, опять же, важно. Это не LLM, которая запоминает. Это отдельный инструмент памяти, который строится рядом с LLM. Про него расскажет Настя. На самом деле тут особо сейчас рассказывать, наверное, нечего. Но давайте поговорим про память. Можно выделить два вида памяти, short-term и long-term. Короткая память для чего нужна? Она обеспечивает связанность с диалогом. В каждой реплике вы не передаете весь имеющийся контекст, общаясь с агентом, вы его дополняете. Например, при построении визарда, вы сначала заполняете свои определенные данные, где живете, паспорт и так далее. Формируете какие-то предпочтения, и на основании этого модель подбирает вам контрагента. Когда мы обеспечиваем понимание связанности диалога, память о предыдущих репликах и то, о чем шла речь, в каком-то коротком пространстве. Наверное, вы все эту память как так или иначе видели в условных чатах GPT, в Алисе. Есть еще долговременная память. Это память, которая уже не говорит про абсолютно все пункты взаимодействия, но помнит какие-то, например, базовые оттуда вещи, знает факты, предпочтения о пользователе. И здесь сейчас, в текущем виде, наверное, она чаще всего реализуется у конечных продуктов. Потому что зачастую долговременная память – это, наверное, более B2C-шная история, когда вам нужно сохранить предпочтения клиента, помнить все его обращения, то есть формировать такой хороший профиль, может быть, основанный на каких-то персональных данных, может быть, основанный на взаимодействии с вами, все зависит от ваших предпочтений. и на основании вот этой памяти позволять модели генерировать максимально адаптированное взаимодействие с клиентом. Давай, наверное, дальше раскроем больше про короткую память. То есть повторюсь, что короткая память – это отвечающая за связанность диалога. Это может показаться таким, скажем так, следующим, достаточно простой в реализации, и по факту на самом деле это так и есть. Например, в популярных аппе типа Responses реализация короткой памяти выглядит как указание предыдущего респонса, на который нужно сослаться. То есть какая реплика для текущей была предыдущей, обрати внимание. И вот так вот по цепочке с помощью поля, немножко, извините, техническую терминологию previous response ID цепляется память. При этом вы можете строить разнообразные цепочки, исключать какие-то куски взаимодействия, то есть выкидывать их из шорт-тем памяти агента. В принципе, когда строится взаимодействие, это отчасти не то что черепикинг до какой-то степени. То есть каждый раз у ЛЛМки есть контекст весь диалога, но относительно последней реплики она оперирует только теми фактами из этого контекста, которые к нему привязаны. и не оперирует, не таскает весь мусор, который присутствовал в диалоге. Давайте дальше. Long-term memory – это чаще реализуют всякие интерфейсы типа чат GPT. Если вы общаясь, например, кажется, Дима сегодня рассказывал ту историю, что чат GPT ему говорит, что ты интересуешься опишкой, у тебя какие-то предпочтения. Вот, например, в таких интерфейсах можно сохранять эти факты о предпочтении клиента и не спрашивать его повторно какую-то информацию. Например, у вас есть чат GPT, который помогает с путешествиями, клиент обращается к вам периодически, и вы прекрасно знаете, что у него такая-то семья, у него какие-то предпочтения, он предпочитает такой-то отдых, и советуете ему не все подряд, а исходя из того, что было сказано им ранее. Как реализуется лонг-мемори? На самом деле есть несколько вариантов реализации. В Long Memory, например, могут попадать факты, которые были указаны в ходе диалога определенными механизмами. Можно сохранять какие-то факты в Long Memory, как условно набор ключ-значений, потом использовать в дальнейшем. Можно вообще, в принципе, сделать следующую историю. Каждый диалог как-то суммаризировать и сохранять это условно в индекс. Можно не суммаризировать и работать с этим. И сейчас идут больше эксперименты, и у нас больше концепции LogMemory, чем практика их использования. На самом деле, практика мы видим в основном сейчас в интерфейсах типа ChatGPT. В условной Алисте, когда вы общаетесь с колонкой и говорите лайк, дизлайк, и она уже понимает на видео, на аудио, что и какие ваши предпочтения сохраняет их, и потом как-то с этим и работает. Я помню, на ранних этапах люди вообще хотели, знаете, что запоминать в LogMemory, как зовут их домашних питомцев. Ну, в принципе, вот как раз вариант того, что можно туда складывать. Дим? А, нет, кстати, вот как раз-таки слайд с описанием. Как раз как факты из short-term memory могут попадать через чекпоинтер, выделение определенных моментов в long-term и складироваться туда. Но и опять-таки, история диалога, которая была в процессе взаимодействия, может попадать в некий индекс, по которому вы все так же можете потом интерьироваться рагом. Да, ну то есть, краткая summary этого слайда. Short-term memory это словно сессионная память. Я вот общаюсь, и вот прямо сейчас модель должна помнить мои предыдущие сообщения, которые я только что и сказал. Долгосрочная память это общие мои предпочтения, какие-то факты обо мне. Сюда же, скорее всего, рано или поздно будет развиваться какая-то условно рекомендательная система и рекламная история в B2C решениях. Но это интересное направление, оно очень активно исследуется, но действительно пока есть в Open Source неплохие реализации, но не так, чтобы супермассово это сейчас использовалось. Ладно, давайте теперь попробуем подвести первый... Про память вопросик. А между ними есть какой-то, как сказать, коннект? То есть что-то, наверное, из short-term должно через некоторое время превратиться в long-term? Супер, да, отличный вопрос. То есть, по сути, что такое шорт-тем? Это весь контекст диалога. Как именно из него строится long-term? Строится она следующим образом. Ну, давайте, самый простой вариант. Мы берем этот диалог и, например, загоняем его в модельку. Вот отдельно просто. И говорим, вот тебе диалог, выдели из него ключевые факты, которые нужно знать. Моделька анализирует, говорит, ага, там пользователь любит кошечек, любит собачек и ходит всегда покупать пятерочку рядом с домом. Эта информация дальше обрабатывается, складывается в эту долгосрочную память. А кем обрабатывается? Ну, смотрите, как бы сходили, достали информацию, дальше нам надо ее сложить в какую-то базу, чтобы она хранилась. То есть, например, мы строим по ней какую-то табличку, либо сохраняем там… Профиль пользователя, так скажем, формированный, можно сказать. Ну да, профиль пользователя. То есть там может лежать факты, может лежать summary общие о каком-то пользователе, может быть история переписок, это просто может быть очень много разных реализаций в зависимости от того, что важно хранить. Но глобально это какая-то отдельная база, в которой лежат вот эти значения, как бы пользователь, информация о нем, summary. То есть можно так сказать, что, например, собирается шоттерн, какими-то там снапшотами с какой-то периодичностью выгружается, агрегируется, там той же LLM, выделяются тут какие-то смыслы и кладется, например, по колоночкам про юзер 1 и про него вот такая вот такая вот такая вот история. В идеале, да. Абсолютно. Да, да, да. Но на практике чаще складываются, может быть, какие-то конкретные факты, которые подразумеваются в диалоге. Идеальная картина вы расписали. Алиса также запоминает всякие разные… Примерно, да. На самом деле, я когда общаюсь с ЧАД-GPT, там был такой интересный факт, я порекомендую, пожалуйста, литературу для нового СММщика. И вот она среди список, материал, точнее, спросила. И она говорит, а вот помнишь, ты готовилась к вебинару и спрашивала меня, Вот ты этот вебинар провела, вот дай ссылку СММщику на этот вебинар. Будет очень полезно. То есть она мне не только дала список общедоступной информации, а напомнила проваженные вехи какие-то и сказала, что я еще могу там и сама рассказать. Это было, скажем так, вау даже для меня. Да, мы же там все идентифицированы, получается, в этой ЛЛМке. То есть нет возможности пользоваться в неавторизованной зоне практически. Ну, это если вы решаетесь так реализовывать. ЛЛМка, она ничего про пользователей не знает. Что вы подали как контекст, данные про пользователя, сам prompt, с этим она и работает. Сама по себе она пользователь недели, для нее все одно. Чтобы пользоваться любой лэмкой, ты должен все-таки агрессиваться. Да, тут на самом деле начинается такой скорее интересный бизнесовый момент. Все-таки если говорить чисто про ChatGPT как интерфейс, который мы знаем. У OpenAI же есть два интерфейса. Если ChatGPT есть платформа OpenAI. Платформа OpenAI это то, что AI Studio это инструменты для разработчиков. Чат GPT это всякие конечные решения. И мы много слышим о том, что, например, чат GPT не очень понимает, как монетизироваться. И он там все время в минусе и еще что-то. И один, кажется, одно из направлений, которое реально можно монетизировать, это вся информация, которую мы, опять же, кремя пользователей. То есть не новая модель, но глобально эта память, это что мы знаем, по сути, из диалогов все преференции пользователя. И ничего не мешает в какой-то момент сказать, ага, ты вот ищешь SMM, ты любишь кошечек и так далее, а вот дальше ты спрашиваешь, а где мне купить еду? И тут мы тебе можем для моей кошечки подсказать, что мы знаем, что ты любишь там, это живешь там, то еще что-то, вот мы тебе дадим рекомендацию сходить туда-то. И это как бы реклама, которая может встраиваться. Но это опять же, это JGPT B2C решение. То, что мы сегодня обсуждаем, это скорее, опять же, набор компонентов, на которых такое можно собирать, можно не собирать. Да, это пример для вдохновения скорее. И здесь раньше вы общались с поисковой строчкой, Теперь вы общаетесь с агентами. Вот так вот и собираются ваши предпочтения. Профиль так теперь формируется. Можно вопрос? Да. Про короткую и большую память. Если смотреть на организацию в целом, то для нас важна еще и база знаний. Если это 1000 или 100 моделей, то, соответственно, какое-то количество ирагов, там где это целесообразно. И тогда следующий момент, это как эту базу знаний собирать внутреннюю. мы пришли к пониманию доменных областей для того, чтобы каталог сделать. Опять же, ваш опыт. Сталкивались с такими базами знаний, какие есть здесь затыки, и наоборот, может быть, какие-то подсказки, как это сделать рационально? Именно с доменными базами знаний? Ну, конечно. Какие здесь есть проблемы с доменными? Ну, часть баз знаний, например, закрыта. Если мы говорим про юридический домен, то есть такие вещи, как Garant, Consultant Plus. Они не дают, например, всего доступа просто к любому пользователю, к их данным. То есть, если мы хотим строить какого-то юридического консультанта или еще что-то, нам как-то нужно получить доступ к этим базам. Это нетривиальная задача. Есть конечные решения у того же Яндекса, которые умеют работать с такими базами, но тут может быть проблема. Если мы говорим, например, про банки, то здесь скорее проблема не в том, что этих данных нет, то, что к ним очень сложно настроить доступы. То есть, когда мы говорим про банковскую сферу, это самое проблемное, это чтобы eBay разрешило ходить в базы, которые нам нужны. То есть, да, есть проблемы с форматированием, там еще с чем-то, но в целом банки достаточно хорошо автоматизированы. А если консервативно и понятно, что ты к базам не сходишь, тогда он премис, тогда у тебя надо наладить хранилище. Сталкивались с опытом? Есть понимание, кто это уже сделал? Как этот опыт? Да, у нас был проект с одним топ-5 банков России. Не был он идиот до сих пор. Там полностью он прем-инсталляция iStudio. И надо сказать, что там действительно самый большой блокер – это всегда получить доступ к нужной базе. Даже в он премии, оно стоит в контуре, но даже сотрудники конечной функции должны пойти и договориться с эбэшниками, что да, мы даем доступ к этой базе, чтобы туда ходить. Это всегда, ну, давайте так, все начинается еще на этапе проверки контейнеров, то есть, когда вы даже ставите контейнер в контур AI-студию, там сначала идет сканирование его, что может занимать даже 2-3 месяца. Это что касается обновления, после того, как оно стало в контейнер, дальше к этому контейнеру нужно дать доступы в систему, это еще несколько месяцев, в зависимости от того, насколько система сложная. Это вот правда может быть блокером-проблемой. Ну, на самом деле, вот сейчас… Сканеры обычные или там специфические тоже должны быть? Обычные контейнеры, да. Да, просто политика безопасности, она обычно жестче, и начинаются вот эти бесконечные парсинги, и Б выискивает исправление, и эти итерации, они очень сильно растягиваются. Бывает, что фича появилась в облаке, мы ее показали клиенту, клиент, да-да-да, доволен и так далее, дайте мне он прем, и мы отдаем, и вот пока до него доедет фича, пройдет достаточно много времени. Да, и тут еще, наверное, отчасти есть проблема, что сотрудники информационной безопасности, они не всегда, по понятным причинам, погружены в детали именно специфики этих ИИ-решений. А DevSecOps, он решает эту задачу или нет? DevSecOps решает, но его надо выстроить. То есть это… Ну, DevOps понятно, а DevSecOps это когда security вставляется во все этапы разработки и тогда… является частью процесса разработки и создателем решения, которые уже безопасно устойчивы. Внутри сразу же. То есть ты сканируешь по ходу, а потом пошли согласно. А не в конце, когда у тебя уже фича идет. Да, абсолютно так. И на самом деле это, наверное, если честно, единственная работающая стратегия, то есть подключать коллег из ЭБ на начальных этапах проектов и проходить все стадии вместе с ними, потому что иначе Но это будет просто блокером, это ничего не сработает. Поэтому да, это, конечно, важная вещь. Тут на самом деле еще подсвечу такой момент. Ставя в контур, клиент думает, вот мне достаточно одной инсталляции, даже если это огромная компания, и забывает совершенно про масштабирование, про отключение условно оборудования, забывает, что нужно как-то перекатывать. И вот это тоже образовательная часть, с которой мы сталкиваемся практически каждый раз, когда у нас идет проект вон-прям. Люди просто не задумываются, думают, поставлю, и оно будет работать вечно. Несмотря на сбои у меня в ДЦ и так далее, забывают про какие-то базовые моменты, про которые мы думаем в облако. И это всегда какой-то компромисс, выбор. Ходить в облако, ставить у себя, потому что ставя у себя, так или иначе вам все равно экспертизу придется растить. Да. Ну что ж, давайте сделаем первую промежуточную подведение результатов. Мы говорили про агентов, как агентские подходы, где у нас в основе LLM, и она дергает инструменты. Просто для примера, для разнообразия, давайте попробуем посмотреть, как в ноу-код интерфейсе можно такого примерного агента собрать. Можно я чуть-чуть перебью, просто чтобы для всех сказать. Мы говорили про агентов, о агентском подходе, и вот все, что мы перечисляли про RAC, Fintune, про нарезание документов, подбор решений под безопасность и так далее, это условия, при котором ангельский подход в целом становится возможным. Все производство МСП и так далее. То есть, условно, я понимаю, откуда была критика того, что мы утром обсуждали, что вы так обсуждаете легко, не учитывая того, что вот это все надо сделать, чтобы то, что вы обсуждали, работало устойчиво, в контуре и так далее. Это наоборот. То есть, смотри, идеальная схема там, хотелось бы туда. Но есть вот куча технологических вопросов, чтобы мы туда в целом могли бы хотя бы чуть-чуть посмотреть. Отсюда я, на самом деле, в моей позиции всегда говорю, лучше начинать с экспериментов, а не с инстанса, раскатанного. Потому что эксперимент позволяет тебе хотя бы чуть-чуть понять, на что ты претендовать можешь. А то ты в фантазиях претендовал себе уже на то, что у тебя безлюдная компания, зарабатывающая миллиард. И говоришь, ну тогда я куплю себе все, что тут. Яндекс, давайте мы поставим полный инстанс. И будем, значит, всю iStudio у себя разместим. А потом она не работает. Абсолютно согласен. Этот комментарий, что почему важно то, что ты говоришь, относительно того, что мы утром разбирали. Да. Да, что здесь хочется показать? Упс. Не получилось показать. Давайте еще раз посмотрим. Да, мы хотим… Давайте я сделаю паузу, наверное. Да что ж такое. собрать агента, который будет условно, вот я тоже сегодня приводил пример, по ИНН будет проводить проверку контрагента. История. У нас есть набор готовых шаблонов, и первый такой шаблон, который у нас появился, сложилось, была компания ContrFocus, у них есть API-шка как раз для проверки контрагента. Мы ее добавили как шаблон, и можем подключить к агенту по MCP. Давайте посмотрим примерно, как я хочу собрать такого агента, из каких шагов будет состоять мой процесс. Начинаем, конечно, с модельки. Имя, выбираем модель и задаем инструкцию. Дальше у нас есть блок, это инструменты, которые мы можем добавить. Мы сегодня про них говорили. MCP, веб-поиск, обычный рак. Мы подгружаем набор наших документов, и наш агент будет работать с несколькими инструментами. Он может сходить в вот эту документацию и подшать информацию там. И вот здесь у нас чистый рак. Мы говорим, вот тебе инструкция, вот количество чанков, которые можешь находить. Ищи там информацию. Дальше мы подключаем веб-поиск. Например, мы хотим собрать информацию в интернете, что говорят о компании. Она может сходить в интернет. Дальше мы идем в MCP. И давайте я тут все-таки сделаю паузу. Быстро оно идет. Мы идем в MCP. Это MCP-хаб. Это как раз то отдельное хранилище MCP серверов, про которое я говорил. Что мы здесь видим? Мы подключаем по URL MCP контрфокуса. И у этого контрфокуса целая куча разных тулов. Их можно пролистать, их, по-моему, штук 50. Проверить паспорт, проверить компанию, проверить афилиации и так далее. Понятное дело, мы можем подключить все в модельку, все 50. Но все эти 50 инструментов попадут в контекст модели. И дальше модель такая. вот у меня 50 инструментов, нужно выбрать правильные и заполнить правильные инструменты. Скорее всего, даже большие модельки могут начать говорить глупости. Поэтому первый шаг, что мы делаем, мы выбираем только те инструменты. А как это устроено? К вам надо регистрироваться, чтобы у вас появлялся контекст различных слузов? Или вы сами выбираете какого-то провайдера или сами пишете все эти? У нас есть набор готовых шаблонов. Мы заключаем партнерство, например, там сам АСРМ, скоро Битрикс добавим. Это готовые шаблоны, которые уже хостятся, и каждый клиент может взять себе по клику добавить его. То есть они написали все эти... Они написали все обвязки, сами сделали, и мы пошарили на всех пользователей. Это большие компании. Вы точно так же можете поднять MCP к своим сервисам, к своему аппе. Да, но это просто способ упростить для самых частых сценариев. Но при этом тут можно принести абсолютно любой MCP-сервер. То есть если у вас уже где-то не развернут и хостится, точно так же указывайте на него адрес, указывайте токен. С ним можно работать. Можно создать MCP-сервер прямо здесь внутри, если есть своя API, но не в формате MCP. Все это тоже можно обернуть. Здесь мы уже исходим из того, что этот сервер создан, подключен. Я показывал эту диаграмму сложную. Моделька обращается, берет список всех инструментов, мы выбираем те, что нам нужны, и дальше их подключаем к агенту. То есть мы выбрали скоринг, поиск по, еще 4 инструмента в этом примере мы выбираем. Нажимаем добавить и, соответственно, сейчас вернемся к нашему агенту. Пока Дима демонстрирует, я хотела сказать, что здесь не только опасность засорить контекст лишним мусором и свести модель с ума. Здесь еще не забывайте, что при использовании модели вы платите именно за токены. То есть таская каждый раз какую-то лишнюю информацию в промте, вы, соответственно, потребляете больше, чем могли бы. Да. И вот, по сути, на этом шаге у нас появился тот агент, которого мы сегодня все утро пытались собрать. Моделька, инструкция, три инструмента. Инструмент поиска по документам, инструмент веб-поиска для поиска информации в интернете, инструмент MCP сервера, который ходит к контрфокусу, чтобы получать информацию. Давайте посмотрим, например, попробуем разобрать, что происходит в момент... Какие документы подгружены? Подгружены отчеты по топ-50 компаниям. Просто инвестиционные документы, как бы отчет Яндекса за 2024 год. Это не связано? Или какие-то требования, регламенты? Нет, там могут быть любые документы совершенно. А при чем здесь проверки? Проверка имеется в виду, что, например, я хочу моего агента спросить, какая была выручка у Яндекса за 2024 год. Откуда он может взять эту информацию? Он может пойти в интернет, в контур-фокусе он этого не найдет. Получается, если я спрашиваю, какая там была выручка, он понимает, что эта информация лежит в моем раге, в моем поиске, делает вызов рага, ищет нужную информацию, отвечает по ней, опираясь на документы. Если мы спрашиваем, это агент, который может взаимодействовать именно в чатовом режиме. И поэтому для разных сценариев, для разных вопросов у него есть разные инструменты. И в этом суть демонстрации, что моделька сама в зависимости от запроса может выбрать инструмент и пойти туда, куда и нужно. Прости, пожалуйста. Раньше разбирали. Вот смотри, для чего мы в первом модуле рисовали идеологические схемы процесса. не вот эту штуку рисовали, а рисовали реально процесс. Потому что логическая схема процессов и артефакты, про которые Дима говорил, и операции с процессами, будут потом основанием для ответа на вопрос, какие инструменты тебе надо в агента положить. Ты потом говоришь, если у меня юрист ходит в контрфокус, значит надо контрфокус сделать инструментом. Если у меня юрист ходит в интернет и ищет цены на каких-то сайтах, сделаем их, прохождение в интернет через этим одним из инструментов, которые он сделает. Если он у меня данные из почты забирает или из какого-то другого канала, значит сделаем это тоже кусочком, который будет подтягиваться. И есть правило, как он туда ходит. Он же туда не сразу ходит вообще. Он сначала делает одно, потом делает другое, потом третье, потом на основании этого генерирует какой-то вывод. А правило записано вон там по выводу. Да, да, да, да. Именно там. Ну я еще проведу аналогию с новым сотрудником. Вы нового человека инструктируете текстом. Вы не говорите ему пайп, сделай обязательно такое. У тебя какие-то обязанности, и ты можешь для решения задач пользоваться тремя программами. Можешь сходить в интернет, можешь посмотреть документации, можешь дернуть ручки API каким-то образом. И вот ровно так строится взаимодействие, когда оркестрация идет через агента. Можно я дополню тебя? Там даже есть, сотрудникам богатая такая метафора, там есть примерно такое. Как обычно это происходит? Сделай вот это. Он говорит, а как? Ты говоришь, ну вот надо, значит, вот такой документ создать. Он говорит, а где я эти данные буду брать? А эти данные ты берешь у нас в HR-системе. А где вот эти данные ты берешь? А это ты в интернет сходи. А где вот эти? Ну, то есть ты, когда ты человеку объясняешь, что совершенно правильно, ты ему так по жизни и рассказываешь, где что брать. Вот у нас обычно, когда новый сотрудник, он бордится, ему и говорят, где смотреть, заказывать кофебрейки, где, значит, с профессорами расчет ведется, где ведется, где материалы, значит, программы лежат. То есть ему фактически это все рассказывают. мы должны сделать это же самое только через тулы и через промп как правило обхождение. Чтобы аналогия полная сложилась. Да, абсолютно верно. И заканчивая уже этот пример, например, мы... Кремниевого парня, девушки. Персонажа. Его онбординг. Что у меня тут случилось? Ну да, он просто, это следующий шаг, на который мы пока так далеко заходить не будем. Но как бы условно, мы задаем какой-то вопрос, моделька сходила, проверила в контур-фокус, сходила, достала информацию в интернете из документов и сформировала какой-то ответ. То, как этот ответ будет выглядеть, зависит полностью от промта, который я ей написал. Если я не написал ей, что я хочу, например, структурированный вывод тремя предложениями, Моделька вольна писать так, как она хочет. Уже это дальше регулируется промтом, как мы хотим получать от нее вывод. Еще хочу важное замечание сказать, что у нее в момент доступна не только одна ручка, она может дернуть все три и на основании контекста, полученного отовсюду, сформулировать уже окончательный результат. Да, а может на самом деле зациклиться. То есть модель может сгаллюцинировать, решить, что что-то не хватает информации и начать ходить в ручку поиска 10 раз, 20 раз, 30 раз, пока не найдет ответ, ну и пока не закончится контекст. Поэтому тут, опять же, много технических нюансов, которые важно учитывать. Но для базового примера, тут собрана основная основа того, что представляет собой агент. И чуть-чуть забегая вперед, мы об этом сегодня немного поговорим, но давайте представим, что вот таким вот инструментом может быть другой агент. То есть мы собрали одного такого агента для проверки документов, еще одного агента и подключаем другого агента как инструмент. Получается, что агент может понять, ага, мне нужен агент, который будет обрабатывать входные сообщения. Я это не умею делать, я передам управление другому агенту. Происходит вызов другого агента, дальше он начинает делать обработку, когда он там все выполнил, возвращает результат обратно. И здесь у нас уже такой первый шаг к мультиагенным системам, где у нас агент является один, например, условно-оркестратором, который выбирает других агентов, и подмножество агентов, которые могут выполнять специализированные задачи. А вот как уже их сгруппировать и что будет работать хорошо и плохо, это такой открытый вопрос, который будет зависеть очень сильно от задачи, данных и так далее. Итого, что мы увидели? мы увидели агентский паттерн архитектуры, такой условно идеальный агент, где саму агентскую логику, вот эту оркестрацию тулов, выполняет LLM. То есть у нас есть LLM, у нас есть инструменты, у нас есть prompt. И, конечно, этот подход выглядит очень привлекательно, потому что, по сути, что нам нужно сделать? Нам сделать интеграцию с тулами, написать prompt, и все работает. Но если вы посмотрите, пообщаетесь с компаниями, в которых уже есть агенты, Вот прям таких агентов, к сожалению, не очень много сейчас. Почему? Потому что вся оркестрация, вот эти все вызовы инструментов, они зависят от решения модельки. То есть на модельку кладется очень большая ответственность понять, какой инструмент, как заполнить, как вызвать и так далее. Мы помним про галлюцинации, мы помним про контекст, про стоимость. И компании просто боятся доверять вот эту вот всю цепочку вызовов чисто агентам. Потому что есть риски ошибок и есть возможные неточности. Модель может передать не на того агента, не туда уйти. И у такого подхода есть ограничения. Лично мое мнение, что со временем, с эволюцией лэмок, этот подход будет становиться условно доминирующим. Лэмки будут более надежными, будет появляться много инструментов, как это все визуализировать, просматривать, мониторить. Но пока компании этому не доверяют. И есть альтернатива. Это альтернатива, сейчас мы о ней подробнее поговорим. Это то, что тоже называют агентами, но на самом деле это все-таки подход, больше основанный на цепочках вызовов. Предположим, я бы хотел моего проверки контрагентов, агента, реализовать немного иначе. Я не хочу, чтобы ЛМК решала, а я говорю заранее. Если тебя спрашивают про что-то, вызывай вот эту LLM. Если тебя спрашивают про Яндекс, вызывай вот эту LLM. И уже выстраивается какой-то процесс, набор шагов из кубиков. Это то, что делает, например, NATN и другие решения. То есть мы прописываем логику нашего агента не чистой LLM, а прям набором шагов явным, который мы можем контролировать. То есть оркестрация вынесена наружу в код или в workflow-движок какой-то, и каждый шаг явно прописан. Почему это проще? Потому что больше контроля и прозрачности. Мы не отдаем все на откуп LLM, LLM является частью шага или нескольких шагов, но мы ее можем локализировать и лучше контролировать, проверять. И если вдруг что-то, наверное, не то, откатиться и пойти по другому пути. Еще вопросик. ЛЛМ может, собственно, собирать такого оргенстратора с учетом, то есть, грубо говоря, написать агента, который будет собирать вот такие вот workflow решения? Да, да, конечно. Это тоже сейчас популярный подход. Вот, например, NNN сделали это прямо внутри. У тебя есть кнопочка, ты можешь там, генерирование потока, писал задачу, пишется полностью, например, YAML файл или какой там формат, он вставляется и дальше сразу появляется в визуальном редакторе весь этот поток, и дальше его можно модифицировать. Это нормальный подход, он используется. И в целом сейчас, это тренд последнего полугодия по общению, когда вообще говоришь про агентов, очень часто под агентом понимается не то, что я вначале с ним рассказывал, а вот такая цепочка на low-code инструменте, что условно это агент. Какие тут есть инструменты? Самые популярные low-code это NN, FlowWise, LongFlow, DeFi, Вот Яндексовый Workflows, мы тоже его сейчас делаем. И OpenAI тоже недавно запустили. И, кстати, Google, по-моему, на позапрошлой неделе тоже сделали новую код решения. Они чем хороши? Они очень простые и понятные, большая часть для прототипирования. То есть не надо быть программистом. Если говорить про Nathan, у него очень большое комьюнити, большое количество готовых цепочек. Взяли, выбрали подходящую автоматизацию, чуть поправили, либо вообще попросили LLEN сгенерировать поток. И можно использовать. Если вы работаете с разработчиками, и, например, разработчики пишут таких агентов, то они скорее предпочитают решения на базе кода. Это лангчейн для цепочек, ланграф, для таких более сложных мультиагентских систем. Ну, условно, такой стандарт. Есть, на самом деле, Lama Index, есть OpenAI, Agent Development Kit у Google свой, у Amazon свой. В общем, есть много очень опенсорсных решений на основе кода. Есть кто-то, кто уже обучает ребят этим подходом, чтобы можно было 100 человек обучить? Да, на самом деле много обучения есть. И открытые курсы, и мы в Яндексе проводим, и платные, и бесплатные курсы. Дадите контакт? Да, конечно. Соответственно, какой паттерн я, кстати, чаще всего вижу сейчас в компаниях. Обычно есть NLTN или что-то похожее, развернутое в компании, и что предоставляется сотрудникам в разных подразделениях. Они собирают прототипы, но в силу ряда ограничений Нейтана его не используют для продакшна агентов. Из-за масштабируемости и так далее. То есть он хорош для прототипирования, и это отличный способ проверить гипотезы. Дали финансистам Нейтан, они там собрали себе из кубиков какое-то решение, потестили, да, классно, там не хватает качества. Дальше происходит приоритизация вот этих вот агентов на базе кубиков. Выбираются самые такие перспективные. И дальше уже команда непосредственно IT переписывает это решение на лонгчейн, лонграф или что-то подобное. И уже выводит в прод. То есть пока скорее лоу-код это такие прототипы. Код используется для продакшн-решений из-за гибкости, надежности и так далее. Еще, кстати, например, важный момент. Сейчас на это не обращать внимания. Но, например, могут быть проблемы с лицензией. Тот же Нейтан, у него очень хитрая лицензия. И по-хорошему коммерческое использование ни в облаке, ни внутри она не допускает. На это многие не обращают внимания, но такой нюанс есть. Я думаю, что многие здесь видели такие решения. Я так понимаю, вы раньше на прошлых сессиях смотрели. То есть это набор кубиков. И по сути, если мы провалимся в кубик агента, вы увидите, что это ровно то, про что мы сегодня так долго говорили. У нас есть агент, у него есть набор тулов, у него есть тул поиска в интернете, тул вызова другого агента, тул с памятью, то есть взаимодействие с долгосрочной памятью и тул. Но это не всем тул, это именно здесь отображается моделька сама, но как бы глобально под капотом вот этого кубика. Это моделька, которая ходит вот в эти все инструменты тулы, вот как мы показывали сегодня в примере ранее. Ну и давай дальше. Похожий же интерфейс есть OpenAI запустили. Точно такие же кубики. То есть есть кубик агента, кубик поиска файлов, кубик MCP. Guardrails – это отдельный кубик этики у них. И управляющие кубики, типа условия, циклы и вот такие вещи. Но все это строится на основе, на той базе, про которую говорили. Модель, инструменты, память. Дальше. Если сделать суммаризацию всех этих решений, какие тут есть наборы интеграции, упрощая кубиков. Это, конечно, кубики взаимодействия с самими моделями. Как правило, такие Open Source решения поддерживают разных провайдеров. Это управляющие кубики, циклы, ветвления и так далее. Это кубики интеграции с поиском, с другими инструментами, с MCP и вот этим всем. И кубики интеграции просто с системами, вызов REST, ручки, поход в базу и так далее. Это все интеграции, которые доступны и из коробки. И вот одна из таких интересных тем, какие компетенции вообще нужны для того, чтобы работать с лоукод инструментами. Я, наверное, скорее затрудняюсь ответить на этот вопрос. Почему? С одной стороны, очень много примеров, когда люди без вообще какого-либо технического бэкграунда собирают прототипы, посмотрели видео, сделали прототип, работает классно, здорово. С другой стороны, очень редко такие прототипы доходят до прода и не используются в реальных сценариях. И на самом деле простота Нейтана тоже как бы простота на первый взгляд. Когда мы хотим получать какую-то гибкость и его адаптировать под задачи именно компании, Там появляется необходимость где-нибудь еще код пописать, что-нибудь добавить. И, скажем так, применимость лоу-кода достаточно ограничена. Поэтому глобально для прототипов специфических знаний не нужно. Для какого-то более продового использования все-таки технический бэкграунд и готовность изучать, погружаться именно в технические детали, она нужна и важна. Давай дальше. Да, сегодня в практике мы будем в том числе чуть-чуть работать с Workflows. Workflows это наш индексовый аналог NN. Ну, вернее, не совсем все-таки аналог. Он более сложный, более технический. Опять же, потому что аудитория нашей платформы, это скорее технические специалисты, которые… Ну, мы все-таки делаем код first, то есть все-таки API, SDK и так далее. Low-code это скорее, опять же, для прототипирования. Но просто для примера чуть-чуть его сегодня потрогаем. И думаю, что здесь пора бы сделать еще одну паузу и поговорить немножко, подискутировать. Мы сегодня на лекции рассказали про два подхода. Первый подход, условно назовем его агентским паттерном, это модель плюс инструменты. Второй это цепочка вызовов. То есть мы прописали цепочку вызовов и дальше мы ходим по этой цепочке и выполняем такие действия. Вот тут хочется, мы сейчас поделимся QR-кодом. Хотелось бы, чтобы вы написали, какой сценарий, как бы вы свой сценарий, о котором вы говорили, реализовывали. Вот что вам кажется, он лучше реализуется через агентский паттерн или цепочку вызовов. И давайте посмотрим на эти сценарии и попробуем прокомментировать, насколько оно действительно хорошо ложится на эти сценарии. Так, сейчас. Но вообще идея понятна, чем отличается цепочка вызовов от того, когда LM является оркестратором? Или нужно еще примеров? Я предупреждал, что будет технически сложно. А может гибрид быть какой-то? Что что? Гибрид какой-то. Конечно, гибрид. Чаще всего гибрид как раз и бывает. Я вижу, уже пошли голосование за цепочку вызовов. Я немножко неправильно сказал задачу. Давайте голосом сначала обсудим. Может быть, кто-то готов поделиться своим сценарием. И давайте, например, ваших сценариев попробуем подискутировать, как его лучше реализовать через агентский паттерн или через цепочку вызовов. Кто-нибудь готов поделиться сценарием? И заодно, как пример, давайте разберем. Стандартный чат. Для чего? Ну вот, давайте простой пример. Стандартный чат поддержки. Вот у меня есть, ну скажем так, я хочу автоматизировать первую линию поддержки. Не важно где. Все это начинается, как правило, с рага. То есть у меня есть база знаний, в которой лежат какие-то документы. Я бы, если бы пытался реализовать такую задачу, я бы начал с агентского подхода, с агентского паттерна. Кажется, что он должен здесь хорошо работать. Условно, что это значит? Берем модельку, делаем рядом базу знаний, загружаем туда мои данные, подключаем эту базу знаний как инструмент к моей модельке. Дальше описываем модель и инструкцию. Ты помощник того-то, того-то. Ты должен отвечать на вопросы, только опираясь на базу знаний. Вот у тебя такая есть база знаний. Ходи в нее, собирай информацию. Соответственно, мы делаем обращение в модель. Модель понимает, ага, вопрос, как настроить Linux. Происходит обращение в базу, достаем информацию, возвращаем ответ. Как бы мы эту же штуку пытались реализовать через цепочку вызовов? Скорее всего, мы бы пошли по теме классификаторов. Мы отправляем запрос, дальше говорим модель, классифицируя этот запрос на тематику. Это базы знаний или базы данных, операционные системы или настройка почты. В зависимости от того, какой это класс, мы бы, например, сделали обращение в одну базу. Если бы это был вопрос про операционную систему, мы бы вообще, например, не пошли в рак, а пошли в какое-нибудь дерево решений. В третьем сценарии мы бы еще, не знаю, свой бы рак сделали, третий. То есть у нас был бы уже набор кубиков. Вот у нас какой-то классификатор, он ведет по разным путям, и дальше это надо как-то поддерживать. Ну это вот, не знаю, стало понятнее или не сильно? Да. Вот это же однозначно про разные задачи. Цепочка это больше про какие-то конвейерные, на поток конвейерные, а агентские паттерны это более… Это больше чатовые сценарии. Агентские это действительно больше про чатовые сценарии, но и не только. Вот, например, давайте возьмем, вчера был вопрос про перплексити. И вот эти агенты, которые работают в перплексити. То есть, условно, вы по какой-то причине решили реализовать своего перплексити. Что для этого нужно? Для этого нужна LLM, которая формирует план, и нужен инструмент, который эта LLM может использовать. В случае перплексии это инструмент поиска в интернете. Мы про него сегодня говорили. Как бы, ну, сильно упрощая, мог бы выглядеть такой агент, если мы используем агентский паттерн. Мы агенту говорим, составь мне отчет про компанию Danone. Агент формирует, ну, как бы языковая модель, она думает, она формирует план. Окей, чтобы составить отчет, мне нужно пойти на сайт Danone. Делает вызов инструмента поиска, собирает информацию с сайта Danone, передает себе, ну как бы, передаем эту информацию в контекст модели. У модели есть, что у нее есть в контексте в этот момент? У нее есть запрос, у нее есть план, который нужно сделать, и информация о Danone. Она анализирует эту информацию в контексте, видит, ага, мне не хватает, например, информации о Danone, как он соотносится по отношению с конкурентами. Ей нужно ставить табличку соотношения с конкурентами. Она делает еще один вызов. Теперь уже с другим. Модель сама формирует поисковый запрос. Найди информацию о конкурентах Danone. Делает запрос в интернет. Находит информацию про вот этих конкурентов. Теперь у нее в контексте, в модель, передается вот эта вся информация. Запрос, план, информация о Danone, информация о конкурентах. У нее огромный контекст. Модель все это анализирует и дальше смотрит, хватает мне информации или не хватает. Если не хватает, она опять делает поиск с новым запросом. И так вот этот цикл, он выполняется моделью, и модель как бы сама решает, хватает, не хватает. И она эту информацию собирает до тех пор, пока она по какой-то причине не решила. Все, вот тут мне информации достаточно. И дает ответ пользователю. Это вот такой полноценный агентский паттерн. Агент, который много раз вызывает тулы, получает информацию и собирает, это все воедино. Цепочкой вызовов это было бы достаточно сложно реализовать, потому что нам надо было посидеть, сначала сходи туда, потом если это, то это. Это не очень хороший пример. Действительно, если у нас что-то потоковое, какой-нибудь ИТЛ и так далее, то тут цепочка вызовов работает лучше. А где лучше применяется про цепочку вызовов? Цепочка вызовов применяется в тех случаях, когда, во-первых, мы не очень хотим допускать ошибки, когда у нас сценарий, который… Цена ошибки высокая. Да, цена ошибки высокая. То есть мы понимаем процесс, мы понимаем шаги, мы понимаем, как их можно описать, и мы хотим иметь максимальный контроль. То есть если вот, опять же, возвращались к проекту контрагентов. Мне достаточно будет сходить в контур один раз, сходить в отчет и собрать все это воедино. Я могу это описать цепочкой. Типа сначала сходи в контур, получи информацию, потом после этого сходи в RAC, забери отчет, теперь сделай summary вот этих двух документов и верни результат. Здесь у меня жестко шаги прописаны. Сначала контур, потом документ, потом результат. Если бы это был агент, то агент бы сам решал, а пойду ли я в контур или пойду еще куда-то. Когда не требуются дополнительные запросы, которые обрабатывают. Да, да, да. То есть когда нет дополнительной обратной связи, и модели не нужно эту обратную связь учитывать. Когда процесс хорошо описывается конечным набором шагов. Да. Вот смотрите, мне кажется, про цепочку, чем она более универсальная, чем агентский подход. Во-первых, можно любую даже сложную штуку все-таки декомпозировать до каких-то элементарных действий. Эти элементарные действия как будто, вот тут, может, вы скажете, что не так, проще, опять-таки, на уровне компании закрыть какими-то простыми премис-лмками. Не здоровенную, там, GPT-ху, там, с кучей контекста, а вот набор небольших сеточек, может быть, даже под разные задачи, которые стоят в нашем контуре. И второе, то, что проще, так как они разложены на кубики, с понятным входом-выходом, с структурированным выходом между ними, ну, скорее всего, то там надо будет еще и, ну, проще контролировать результаты. То есть, человека, ставив в любую точку, Он может посмотреть, что куда передается и почему. На самом деле, да, это абсолютно верно. И это, наверное, причина, почему сейчас агенты на основе цепочек вызовов более популярны и гораздо чаще используются, чем вот эти агенты. Но здесь, даже если посмотреть на вот этот утренний пример, как бы в идеале, что бы нам хотелось, чтобы мы и сюда в AI, и сюда в AI, просто написали инструкцию, подключили тулу, и оно само работает. В одну какую-то, да? Что? В одно какое-то тулзу, грубо говоря. Да, да, да. Вот мы это все один раз описали, условно, большой инструкцией. Вот тут даже была вот эта инструкция. Мы это один раз описали инструкцией, и оно само работает, как бы, я вообще не волнуюсь. Это не решается, как раз, оркестратором сверху, который, да, ты туда все запихиваешь, или тебя видится так, что ты как будто в одно место все запихнул, но дальше он уже раскладывает по конвейеру. Да, но этот конвейер, как бы, чуть-чуть процесс поменялся, и по-хорошему конвейер тоже нужно менять. То есть это сложнее поддерживать. Тут это поддерживается инструкцией. Там чуть-чуть заменил инструкцию. Здесь это прямо надо проанализировать кубики. Их может сильно раздуть. Тебе нужно понимать хорошо процесс. Если ты даже это кодом модифицируешь, все равно надо проверить, что он не написал глупость. В общем, в плане поддержки это с одной стороны прозрачнее, с другой стороны сложнее и меньше гибкости, чем по сравнению с агентским паттерном. Но на самом деле как раз следующий слайд про дискуссию. Давайте вот тут немножко подведем итоги. Вообще немножко неожиданно разделились голоса 50 на 50 ровно. Я думал, что все проголосуют за цепочку вызовов. Но тут не совсем честное сравнение, потому что действительно разные подходы подходят под разные задачи. И тут важно выбирать в зависимости от задачи подход. Но плюсы и минусы мы постарались все подсвечу. Я еще свой опыт подсвечу. Мы говорим про процессы и про чаты. Я очень много взаимодействовал с голосовыми ботами, с тем сегментом, который строят роботов. Как же они ждали ЛЛМ-ку? Их тогда стало конструировать каждый раз эти ветки деревьев, вот эту логику продумывать и уточнять у клиента, а вот что должно быть дальше и так далее. И когда стала возможность принести текстовое описание, и чтобы модель это распарсивала и сделала все правильно, это было очень большое облегчение. Цена ошибки не такая высокая, прозвонили или ответили поддержке, но индустрия стала гораздо легче. Сейчас это начинает внедряться, и всем очень-очень нравится. Да, ну давайте такое, подводя немножко итог вот этой первой части, попрошу еще кого-то, может, высказаться. Вот из тех подходов, какой ближе, почему, какие вообще впечатления от того, что рассказали? Так, ну я, наверное, поделюсь своим каким-то субъективным мнением и опытом, что если возможно сделать именно цепочку, то делаем цепочку. Меньше ошибок, естественно, и всех выходящих галлюцинаций и так далее. Но если это сценарий однозадачный, то есть условно приведу пример. Мы нанимали сотрудников, я нанимал, и у них было тестовое задание создать агента, который является помощником для сотрудников отдела продаж и с помощью голоса выполняет определенные действия в СРМ со сделкой, с контактом, с компанией. Условно, поля меняет, название ставит, задачу и так далее. Один из людей, он старый закалки такой программист, он реализовал агента и несколько выходов после него. И они были заточены под одно действие. То есть он не мог сделать многозадачную историю, что я ему сказал поставь задачу, поменяй название и добавь в поле какую-то информацию. Вот агент как раз таки с этим справится. А цепочка нет. Поэтому здесь очень сильно все зависит от задачи. Если это однокипная какая-то задача, то, естественно, лучше в цепочку смотреть. Если это многозначная история, то, конечно, агент с инструментами. Да, спасибо большое. Мы сможем зайти в этот агент снова с этой же задачей и прогнать еще раз, пока не выполним все три. Нет, у тебя в цикле надо все прописать в цепочке, все вот эти выходы. Да, этих вариантов же может быть бесконечное количество, сколько полей в сделке. Несколько интентов, да, получается. Да, да, да. А так, если у него инструмент обновления сделки, условно говоря, там зашиты все поля, через API сделано, то он передает туда то, что ему нужно. И, соответственно, на выходе мы получаем обновленные необходимые нам поля, потом он ставит задачу, сам следующий инструмент пошел, ну и так далее. Да, спасибо большое. Абсолютно согласен. Да, еще кто-то? У меня короткий вопрос. Как может выглядеть до обучения RAC? Как может выглядеть до обучения RAC? Давайте чуть с терминологией. Когда мы говорим про RAC, это все-таки не до обучения, а обновления базы. Глобально, к чему мы стремимся? Чтобы эта база была всегда в самом последнем состоянии. То есть это функции добавления, удаления файлов, данных, информации из этой базы. То есть, как правило, когда мы говорим про RAC, недостаточно просто загрузить один раз базу и положить. Нам нужны отдельные API-инструменты-ручки, чтобы добавлять и удалять файлы. То есть, когда мы удаляем файл, они просто уходят из базы, и модель к ним не может обращаться, она их не видит. Когда мы добавляем новые файлы, они также векторизуются, складываются в этот RAC, и модель всегда работает с последним состоянием тем, что есть в этих файлах. И этим, кстати, RAC ценен. Дообучать, вот именно если бы мы дообучали модель, нам для каждого изменения пришлось бы собирать новую обучающую выборку, на этом дообучаться. В RAC достаточно удалить старый файл, добавить новый, и модель будет всегда работать с актуальным состоянием. Помните, вот была ситуация с COVID, когда мы все как-то проснулись и не могли пойти на работу. Нужно было всем расхватить инструкции. Представьте, что за это отвечала бы модель, обученная на каких-то конкретных инструкциях, которая бы ни слова не знала про COVID. Но тут случилась ситуация, и мы просто добавили этот файл, обновили инструкции буквально за час, и у нас вся система уже обновилась, без дообучения модели, без каких-то сложных действий. И вот в этом преимущество. Вы очень быстро можете накатывать какие-то изменения, не трогая там слои модели. В этом преимущество Rago. Он без обучения работает, он обновляем в документации. Можно? Если у нас такая ситуация, развернутая LLM on-premise, я здесь. и мы хотим еще одну модель подключить, например, по токенам. Как будет оркестрация происходить? В какую модель ходить? Грубо говоря, либо этим управляете вы, то есть тестируете обе модели, посмотрите, какая лучше на конкретном сценарии и выбираете. Чем удобно, практически все модели работают по одному и тому же формату API. То есть у них единый контракт взаимодействия. И, как правило, переключение сводится просто к смене условно, адреса и пароля. Не пароля, а токена авторизации. И получается, что обычно как? Просто тестируются разные модели, выбирается лучшая модель под задачу, и она используется. Но здесь важно помнить, что вот это такая немножко спорная тема, но когда вы завязываетесь на модель, сейчас очень часто есть такое, вот вышла новая модель, нужно сразу переключиться на нее. С другой стороны, если мы реально выводим в прот, то может быть ситуация, что обновление модели может сломать предыдущие промды и предыдущую логику. Поэтому здесь очень важно не бежать за моделью, а все-таки сначала выстроить какой-то замеры качества, то есть какой-то бейзлайн, на который мы опираемся, и дальше уже менять модели процессно, исходя из этого бейзлайна, а не просто быть на самой новой популярной модели, потому что очень часто случается, как было с OpenAI недавно. Они что-то выкатили, что модель стала меньше хвалить. Все начали очень жаловаться. они потом снова перевыкатили, модель стала снова хвалить все рады. То есть, окей, в B2C они себе такое могут позволить. В B2B, если мы завязались на конкретную задачу, а особенно если мы ждем от модели какой-то всегда фиксированный выход во временном формате, то такая быстрая смена моделей может привести к проблемам. Поэтому переключение лучше делать в зависимости, опираясь на метрики. Здесь был вопрос? Да, у меня не вопрос. Вы спрашивали, изначально вопрос был, к чему больше душа лежит. Лично у меня душа лежит больше к агентской истории, нежели к цепочке. По ощущениям, мы используем цепочку из-за того, что у нас агентская система пока не совершенна. Как только агентская система будет совершенна, такое ощущение, как будто эта агентская система должна создавать различные цепочки и все остальное. Агент должен создавать другого агента, который создает других агентов, чтобы выполнить задачу. Да, спасибо большое. Мне тоже кажется, что мы просто пока не дошли до того момента, когда у нас лэнки действительно настолько хороши, чтобы им доверить полноценный агентский подход. У вас еще был вопрос? У меня был вопрос по поводу системы. Вы сказали, что можно выстроить перечень задач, их потом категоризировать. рядом сделать перечень всех этих самых моделей и дальше просто от специализации каждой модели на эти задачи сделать такую разметку. Когда у меня там тысячу задач, то это единственный способ, понимая, что погрешность пока вот эту 70% нужно брать, погрешность 30% и просто вот таким способом прогонять все задачи, которые нужно порешать. Все равно это даст результаты. Главное тогда просто понимать, какие взаимосвязи к каким типам задач, от какой модели, и потом тогда уже система будет самосовершенствоваться, когда начнут практически применять. Да, действительно так. И тут еще есть нюанс, когда мы подбираем именно модели, то есть пока еще, если говорить, есть такая профессия, там, условно, промт-инженер была популярна год назад. Сейчас потому что модели учатся быть агностичны к промту, то есть понимать любую инструкцию, не так важно, в какой формулировке. Но они все еще не идеально справляются, и формулировка играет роль. Но что более важно, кроме промта, это еще навыки моделей, которые в нее закладываются. Мы начинали с того, что есть ряд моделей, куда закладывается навык вызова функций. Это прям отдельный навык, которому модели нужно учить. Это не то, что сходу любая модель умеет хорошо вызывать функции. И в зависимости от задачи, это может быть, если мне нужна summary делать, то, наверное, мне вообще не нужна большая модель, которая умеет вызывать функции. Мне нужна моделька поменьше, которая может сделать хорошую суммаризацию. Если я строю агента, который у меня будет выходить в прот, мне нужна моделька, которая хорошо умеет вызывать функции. Но если это будет модель, которая хорошо вызывает функции, но отвечает в течение трех минут, я не смогу построить для нее чат-бот, потому что пользователи не будут ждать три минуты, Пока я обойду кучу всяких тулов, сформирую результат. Поэтому на самом деле выбор модели – это такой тоже достаточно сложный шаг, потому что у нас есть функциональные требования к качеству самой модели. Есть ряд нефункциональных. Скорость, качество тулов и так далее. Если взять навыки промт-инженера и разложить их на составные части, то принципы – это те компетенции, которые, ну, как минимум, нужно у себя протестировать лично и у того количества людей, которые будут с этим работать, чтобы эти навыки подтвердились. Вопрос, не уверен, что сейчас прозвучала, что есть у прямых пайплайнов еще преимущество, что они иногда быстрее работают, потому что не нужно эти все вызовы делать, множественные, узнай инструменты, а запроси теперь, подай запрос, получи, прочитай, а сразу она идет по четким пайплайну. Абсолютно верно, да. То есть тоже еще один большой блокер. То есть вот для агентского подхода нужны большие модели. Большие модели отвечают долго. Это можно обходить, например, стримингом, когда она как бы печатает кусочками. Но если ей нужно вызвать тулы, она может уйти в цикл вызова тулов. И это действительно может быть сильно дольше, чем решить цепочку. То есть да, это спасибо. Да, и вот если я как-то выбирал, я бы для себя смотрел так. Если вариативность в том, как выполнить задачу, высокая, часто как раз-таки вообще не с пользователями так, потому что Бухов знает, что он хочет, он может в одном запросе дать два запроса сразу. То есть там вариативность огромная, и чтобы не зажимать его, не ухудшить опыт, тогда это скорее агентская схема. А если вариативность низкая, то это какой-то backend процесс, где конкретный вход, конкретный выход. Ну да, его нужно LLM-кой обработать, вот там pipeline как бы лучше всего. Да, да, да. Подскажите, какая модель для функций у вас получилось лучше юзать? У нас именно? Да. Смотрите, хороший вопрос. У нас сейчас из тех моделей, которые доступны именно по токенам, у нас есть наших две модели, Яндекс.Гпт, Lite и Pro. Есть три опенсорсных, три семейства. Это QAN, это GPT-OSS и GEMMA, гугловская модель опенсорсная. Чаще всего пользователи для агентских сценариев выбирают QAN. Он большой, он 235 миллиардов, он работает хорошо. Для RAC сценариев пользователи выбирают Яндекс.Гпт.Про, потому что мы ее очень сильно дообучали по драк сценарию, чтобы она не фантазировала. Но у QN, например, есть проблема, что он очень долго отвечает, потому что он большой. То есть у него вполне может ответ уходить за 3-5 минут. Пользователи не все готовы ждать. Промежуточный вариант это GPT-OSS, open source модель, как OpenAI. Она неплохо вызывает тулы и сильно быстрее работает. У нее в 2-3 раза меньше параметров. Поэтому, опять же, если смотреть по популярности, просто без сценариев популярности. У нас сейчас самая популярная Яндекс.GPT. Она маленькая относительно квена, но быстрая и для таких базовых рак сценариев работает хорошо. Дальше идет квен, дальше идет GPT-SS. Ну и, наверное, подводя небольшой итог дискуссии, здесь тоже видно, что разделились на две части, то есть три цепочки, два агентских. Но мы обсудили, что, опять же, надо смотреть на конкретную задачу, на требования по гибкости, по времени ответа, по скорости ответа. Поэтому кажется, что очень неплохо получилось нам это все суммализировать. Я еще озвучу такую мысль, когда еще есть такой вариант. Например, ко мне пришел клиент, который хочет заполнять по результатам разговора аудио CRM, но в конце он не цепочкой вызовов оперирует, а просто подключает того же оператора, который вел разговор. И, собственно, пользователи валидируют, а все ли поля были заполнены правильно. То есть еще можно добавить человека третьим пунктом. Да, спасибо. На самом деле такая вводная часть про агентов подошла к концу. Следующий блок, о котором мы бы хотели поговорить, это такие более продвинутые темы и более узкие специфические агенты. Но, видимо, у нас сейчас прерыв на обед, да? Нет еще? Ну, отлично, тогда мы успеем говорить про продвинутых агентов сейчас. Давайте, наверное, успеем все... У нас как раз 20 минут, если я не ошибаюсь, да? Да, это 400.00 у нас опять. Ну, давайте, тогда, Настя, тебе слово, давай. Давайте я попробую побыстрее и попроще. Ну, давайте. Давайте поговорим про моих любимых голосовых агентов. Да, они там следующие. Вот, мы преимущественно сейчас рассуждали про текст, и там где-то протекстовое взаимодействие, и в большинстве случаев были допустимые задержки. Мы представляли какой-то чатовый интерфейс. Давайте поговорим про то, как вся агентская архитектура переходит в голос. Наверное, можно следующий слайд. Паттерн агента, еще раз я его повторю, здесь это LLM. Если мы говорим про подход с аркестрацией, есть некий набор тулзов, то есть возможность выполнить те или иные действия. Инструкция, которая сама описывает ТЗ, задает ограничения и верховодит этой ЛЛМкой. Ну и память, которая делает взаимодействие в короткий какой-то период, возможным обеспечивать связанность диалога, либо сохраняет там, запоминает факты в дальнейшем. Итак, если мы говорим про паттерн голосового агента, то помимо технологии у нас, конечно, ЛЛМки, добавляется история с тем, что есть Speech-to-Text, когда голос превращается в текст, чтобы скормить его LLM, и чтобы преобразовать текстовый ответ LLM в речь, в голосовом канале нам, конечно, необходим синтез речи, тексту спич, TTS. Можно следующий слайд. Как это вообще выглядит? Ну, вот как раз-таки один из вариантов той схемы, которую мы так сегодня все обсуждали. Допустим, у нас есть агент голосовой поддержки Яндекс.Клауд, которые мы предоставляем разные API, нам звонит клиент, он что-то нам говорит, мы преобразуем это по окончанию какой-то короткой его фразы в текстовый ввод. Мы все постоянно делаем в потоке и отдаем ЛЛМке максимально завершенный фрагмент его речи, для того чтобы ответить на вопрос. Мы определяем это специальными механизмами. В ЛЛМке пускают доступна инструкция, что ты агент голосовой поддержки, у тебя есть возможность завести задачу в трекер, если понадобится, выполнить веб-поиск по сайту для того, чтобы ответить, как работает Алина Апишка, сходить какие-то внутренние ресурсы. В общем, вся та же самая история, что мы обсуждали ранее. И когда LLM в ответ на реплику пользователя, на какой-то его запрос, получает ответ, ну, генерирует ответ, мы этот ответ сразу начинаем стримить в синтез. Мы не дожидаемся конца, потому что голосовой канал очень важна скорость взаимодействия. Дим, наверное, там дальше есть. Анастасия, вопрос можно по предыдущему? А чем отличается function calling от MCP? Function calling я скорее здесь подчеркиваю как возможность головы взаимодействия. То есть вам не обязательно... То есть, смотрите, как сейчас устроена индустрия, которая предоставляет API для создания агентов. Все LLM поддерживают практически все Function Calling, и на это сверху навешивается упрощение работы с Function Calling, упрощение задания туллов. То есть Function Calling как голое взаимодействие всегда доступно, MCP как вот такая обертка, синтаксический сахар, упрощение, добавление безопасности над MCP. И какие-то встроенные тулы чаще всего предоставляются. Так как мы все в той или иной степени оглядываемся на OpenAI, которые так заложили вот такую структуру устройства тулов, все в индустрии стараются предоставлять плюс-минус то же самое. То есть Function Calling это просто возможность что-то сделать в обход MCP, если вы хотите подключить какую-то свою функцию. Не обязательно городить MCP сервер и ходить именно в него. Просто можете там локальненько реализовать. RAC здесь это по файловой поиске? RAC, он здесь инкапсулирован, спрятан за веб-поиском, по файловым поискам. Смотрите, RAC это вообще, повторюсь, retrieval augmented generation. Когда генерация, она происходит не только на основе того, что есть в собственной голове модели, что она обучилась, когда мы ее подкармливаем что-то сверху, даем какой-то кусок информации, который с какой-то долей вероятности содержит данные, необходимые для ответа. Если так рассуждать, рак это даже вызов функции, когда мы чем-то подкормили, и модель, уже обладая всеми возможными на этот момент информацией, формулирует ответ. То есть, еще раз, вот LLM. Вот он, агент, где у нас роботик торчит LLM. Она следует инструкции, которая описывает. Ты агент поддержки, у тебя такие татулы, ты вообще молодец. Отвечай коротко по делу и так далее. И можно дергать function calling, mcp, веб-поиск, по файловый поиск. Причем по файловый и веб-поиск – это максимально встроенные инструменты, где прописать, по сути, пару параметров. Требования голосовым агентам. Когда вы строите агента в голосе, то здесь, скажем так, становятся жестче требования по скорости взаимодействия. С чем это связано? Потому что голосовой интерфейс более привычен. Мы все с ним знакомы с детства, и все привыкли общаться голосом и ожидаем некой реакции ответа, возможности перебивания и так далее. Поэтому, когда строим голосовых агентов, очень важна именно скорость, важна живость диалога, важна, так скажем, крадация агентов, наверное, что умеет агент, чтобы он не уходил в тот бесконечный цикл, про который рассказывал Дима до этого. Дима, давай, наверное, дальше. То есть у нас паттерн голосового агента, я не буду там уходить в какую-то сильно большую теорию, он практически то же самое, что было вот в тексте, только добавляются такие технологии, как СТТ и ТТС, чтобы процессить голосовой вход и вывод. И выкручиваются требования к скорости ответа и оживленности системы. Скорость ответа, она достигается с тем, что мы идем на некоторые жертвы, обычно прикручивая туда определенные модели, которые, может быть, менее умные, но более быстро и хорошо умеет функцион холлинг. Ну а оживленность диалога там компенсируется какими-нибудь способами API, которые, например, в частности предоставляет Яндекс.АйСтудия, ну и, конечно, предоставляет OpenAI как законодатель мод. А вы можете пояснить, что это просто? Что? Что это такое? То есть это с помощью API, ну а что такое оживленность диалога? Что сделать? Ну, оживленность диалога это что на выходе? Ну, смотрите, вот вы разговариваете, я что-то говорю, вы можете меня перебить, например. Вот это возможность живости диалога. Также живленность диалога, если вот вообще хорошо про это рассказывать, она еще должна быть, что модель, ну как бы скорость ответа. То есть вам задали вопрос, вот если вы звоните какую-то линию поддержки, оператор же не просто думает, ушел там в себя, чтобы отвечать, он такой минуточку, секундочку, хрю-хрю, вот это вот все, он говорит, что я здесь, он всячески показывает. И вот такого рода поведения и ожидается в том числе от голосовых перебить, максимально быстро, четко дать ответ, понять, что пользователь закончил фразу, то есть быть максимально, ну не то что эмпатичным, но эмулировать живого человека. Конечно же, все это стало возможно с голосом именно сейчас, потому что технологии синтеза как раз доходят до того, что звучит реально шикарно. Уходит на максимум, вот это вот, минимизируется роботизация, мы можем быстро оперировать, Можно играть интонация, промтить текстовыми промтами. Вот тут сделай более такой продажный тон или еще что-то. Сейчас все технологии к этому идут. И вот благодаря LLM, чат-боты, которые раньше там строились, ветка, ходи сюда, не ходи сюда, пойдешь сюда, попадешь на ошибку, тебя вернут в предыдущий пункт меню, оно все уходит. И поэтому мои коллеги, которые умеют в ботов, в голос, они, конечно, все бесконечно рады появлению LLM и возможности задания вот этих голосовых ботов с помощью вот такого текстовой инструкции, текстового теза. Короче, если подводить итоги, есть агенты текстовые, есть агенты голосовые. Когда вы делаете голосового агента, вступают в силу еще дополнительные технологии, с которыми нужно подружиться, синтез и распознавание. И также необходимо забывать про их взаимодействие, потому что если вы делаете, например, простите, господи, холодные обзвоны, вам нужно всячески удержать клиента на линии. Если вам клиент звонит, вам тоже было бы, наверное, неплохо сделать это взаимодействие максимально живым и приятным конечному человеку. Особенно если вы какой-нибудь там стараетесь выглядеть элитно или разделяете пользователя по каким-то градациям. Как-то так. У меня, наверное, есть демо голосового агента. Но достаточно... В общем, тут есть нюанс, что у нас звук не выводится. Здравствуйте. Меня зовут Владислав. Вы зашли в чат специалистов поддержки. Ответственные по направлениям. Будут рады помочь вам. Чем могу помочь? Павел, поставь. Я хотел бы... В общем, суть такая. Здесь голосовой агент, который действует ровно... А, голосовой... Прошу прощения. Голосовой агент, который действует ровно по тем блокам, которые мы обсудили ранее. И в чем интересная фишка? Мой коллега, он заварил себе голос, свой собственный, и общается фактически сам с собой. Вот если вот там очень похоже, как звучит. То есть, ну, конечно, тут не идеально, но сама идея, что вы можете сделать свой голос, не будучи профессиональным диктором, она стала, возможно, сейчас. И мы на этом демо это демонстрируем. Алло. Здравствуйте. Меня зовут Владислав. Вы зашли в чат специалистов поддержки, ответственные по направлениям, будут рады помочь вам. Чем могу помочь? Я хотел бы сменить свое место в рейсе. Вот это настоящий вот я сам слышал. Здравствуйте, меня зовут Кирилл. Я отвечаю за бронирование мест в рейсе. Пожалуйста, сообщите мне номер вашего бронирования. Коллеги идею поняли, но суть в том, что сейчас технологии дозрели как раз до той степени, когда именно живые интерфейсы голоса стали, ну, стало возможным сделать. Поэтому и появляются голосовые агенты, пример из которого мы только что вам показали. На самом деле, да, тут достаточно просто. Тут все будет упираться в распознавание и синтез. Если распознавание технология поддерживает, соответственно, вы все сможете. Вообще сейчас такого рода штуки делаются из трех компонентов, это ТЛМ, ТТС, но в целом индустрия крутит разные модельки, которые будут себя сочетать некоторые из этих компонентов, а то и объединять все три. Я думаю, что в скором времени, вот сценарий, о котором я мечтала, что ты звонишь в поддержку любого, скажем так, провайдера, и даже если ты не говоришь на языке, тебя поймут и ответят тебе ровно так, чтобы ты понял, я хочу, чтобы это реализовалось жизнь. Или, например, когда вы общаетесь с иностранцем, чтобы вот этот подсрочный перевод, он происходил вот здесь и сейчас. Такие примеры сценариев тоже возможно уже реализовывать. Извините вопрос. Ваш агент это аналог реалтайма от GPT? Это он и есть. Это он и есть, да? Ну, как бы наш реалтайм, ай-студия. Смотрите, OpenAI задает вектор, задает тренды и так далее. И, конечно, местами хочется придумать какой-то слой API и прочее, но не только… Тренды в плане API, то есть формата взаимодействия построения. Да, да, да. Но мы реализуем на своих технологиях и, конечно же, стараемся всячески дополнять. И, на самом деле, это не только вот мы так действуем, но еще такие гиганты. там DeepSync реализуют отчасти, OpenAE совместимая API. То есть индустрия к этому идет. Все так быстро развивается, что городить собственный API, в который нужно еще погрузить разработчика, становится не то чтобы выгодным и хорошим. А по цене как у вас? Дешевле чем? Зависит от модельки. Да? Окей. С фуншн коллингом нормально справляется. С фуншн коллингом нормально справляется. Ну, фуншн коллинг. Фуншн коллинг, ну, да. Ну вот, кстати, про фуншн коллинг. Как раз хотел здесь заметить. вот тут вверху появилась такая штучка хенд-офф, мало, наверное, кому что говорит, но глобально это function calling. То есть этот пример, он собран на трех агентах. Вот если мы говорили, что сущность модели это моделька с инструментами, то здесь их три. У нас есть общий ангет-оргестратор, который просто там приветствует и описывает логику, и дальше, это называется triage agent, и он дальше передает, когда Влад говорит, я хотел бы сменить свое место в рейсе, моделька понимает, что для обработки смены рейса мне нужен другой агент, через механизм хенд-оффа, то есть через тул, она передает управление сид-букин-эджент, и он уже дальше, это другая модель с другими тулами, с другой инструкцией, обрабатывает этот процесс бронирования. Ну да, мы тут такой пример мультиагентности. На самом деле в рамках нашего разговора не то чтобы важен, но можно в голосе реализовывать мультиагентные сценарии. А у вас интерфейс тоже как реал-тайм используется? Да, реалтайм один в один совместимый. Слушайте, приходите. Как дела у вас? Мы сейчас, потому что как раз тестируем реалтайм, буквально неделю-две назад запустили у себя, и он плохо справляется с функциями. Прям вообще. Рот теряет, женский, мужской. Иногда забываешь, что это говорит женщина, говорит, я вам напомню. Ну, давайте мы с вами в кулар поговорим. Очень интересно. Да, здорово. Подскажите, пожалуйста, с казахским языком, как у вас? С казахским как раз хорошо. Это вот положение. У лекторов не очень, а у моделей, наверное, с нормой. Смотрите, в общем, короче, казахский у нас хорошо, у нас есть синтез и распознавание, и в API пока это не задать, но я недавно тестировала на своем коллеге, и ему понравилось. Он спрашивал, где деньги, что с этим делать. В общем, очень мило пообщались, сказал, что ответил хорошо. Кажется, что скоро дадим возможность использовать для казахского флоу. И модель хорошо с казахским общается. Она делала рак на русском языке, перевела и ответила. А вот еще один момент. У нас еще в Казахстане миксуют русский и казахский. Это суржик, да. Это учитывали, да? Это наша головная боль, это еще с распознавания. В общем, мы стараемся, но учитываем по мере возможности. Модель, кстати, когда появилась ЛМ, на самом деле стало прикольно накладывать ее поверх результатов распознавания. Она круто начинает вытягивать смысл. Поэтому LLM круто вытягивает то, что в распознавании раньше, казалось бы, вытянуть невозможно. Что-то там получилось, и уже из этого, кажется, ничего хорошего не сделаешь. LLM забустила эту историю. В общем, казахский будет поддержан, приходите. Еще в дополнение к этому мы как раз используем ваш спич-кит, который распознает этот. У нас есть отдельное распознавание именно аудиозаписей русско-казахских. Справляется намного лучше, мы перешли на вас, потому что GPT-шна творила дичь какую-то. Их Whisper вообще казахский не понимал нормально. А ваш спичкит прям хорошо залетел. Слушайте, мы стойко вкладывались. Это очень приятно передам коллегам. Да. Еще можно спросить, планируется ли релиз более такой эмоционально живой модельки, которая озвучивает голос? И если да, то когда? Я тоже вопрос этот задаю Рэнди команде. Как только будет понятно, мы сразу расскажем. Но вообще мы понимаем тренды, понимаем, что сейчас, если OpenAI это все про агентов, то Eleven Labs это вот та сотая, которая сей идет. Конечно, мы смотрим на Eleven Labs и экспериментируем. Хотелось бы, чтобы завтра, но как уже будет получится. В следующем году, я думаю, все сойдется. Спасибо. Да, это то, что касается голосовых агентов, но на этом наши продвинутые темы не закончились. Глобально, что мы сегодня делаем, мы постепенно закрашиваем все круги агентской инфраструктуры, и мы не поговорили еще про очень важный компонент. Мы его старались подсвечивать, но детально не говорили. То есть языковая модель, инструкция, инструменты, память, это все покрыли. Но когда мы переходим в продакшн, большую роль начинает играть среда выполнения. Где это все крутится, какой интерфейс взаимодействия, как происходит логирование, как происходит мониторинг. Это большие, сложные технические темы. Наверное, сегодня не совсем тот формат, где мы можем вдаваться в детали. Но это просто те вещи, которые хотелось бы подсветить, что о них важно помнить. Вот тут на слайде пример агентской архитектуры условно end-to-end. То есть, если раньше мы начинали с того, что у нас просто была LLM, которая ходит в тулы, то вокруг есть еще больше всего. Что здесь есть, давайте без детали пройдемся. Responses API, я уже говорил, это тот шаг, который, я все время говорю, не LLM вызывает тулы, а обертка. И вот эта обертка, это как раз Responses API. То есть здесь кодом реализована логика, что модель понимает, какой инструмент нужно вызвать. Дальше код в Responsys API делает вызов MCP, VectorStore или LLM, возвращает результат и обрабатывает его до того момента, когда сама языковая модель не вернет финальный ответ. Но если мы, это по сути один агент, одна LLM с одним агентом. А если мы хотим сделать так, вот как у нас в голосовом было, когда мы передаем управление другому агенту, нам нужна еще одна обертка сверху. Ее можно реализовать разными способами, лангчейн, лангграф, нтн и так далее. Но вот конкретно в этом примере у нас респонсов API совместимы с OpenAI. И для простоты мы взяли OpenAI Agents SDK. Очень хороший SDK, очень простой для использования, очень понятные концепции. И в нем есть как раз механизм вот этих хендоффов, то есть передачи управления между агентами. Мы берем респонс из API и оборачиваем его в SDK. Дальше стоит вопрос, а как там выдать этого агента вообще наружу? И здесь есть два направления. Есть направление встроиться в UI, то есть есть какой-то конечный фронтенд, и мы с этого фронтенда ходим в агента, который, например, может быть задеплоен в serverless контейнерах и взаимодействует с агентом. Либо же мы можем выдать наружу взаимодействие с агентом через A2A протокол. Это еще одно такое, ну, сон в базворд, о котором даже много говорят. Еще один open-source протокол. Он уже был предложен Google. И он скорее нужен для взаимодействия агента к агенту. То есть хочется прийти к унифицированному стандарту не только вызова инструментов, но и еще и взаимодействия с агентами. И Google предложил вот A2A. Не могу сказать, что он прям стал уже сейчас стандартом. все на него перешли, но, наверное, лучшего пока в плане межагентского взаимодействия ничего нет. А как такое, ну вот, чуть-чуть поясните, пожалуйста, вот A2A, агент, как я понимаю, он выдает все-таки частенько, да, структурированную, но может быть иногда разную информацию. И второй, кто его принимает, как-то ее должен по этому протоколу понимать. Да, у A2A протокола есть такая штука, как Agent Card, то есть первым делом, когда мы хотим взаимодействовать с агентом по A2A, мы делаем к нему вызов этой карточки. В карточке в определенном формате описывается способность этого агента, что агент может делать, какую сдачу он решает, какие инструменты он может создавать и так далее. И уже на основании этой карточки дальше по этому протоколу могут проходить обращения к агенту. А, это типа два агента между собой договариваются про контракт? Примерно так, да. Как вы понимаете, мы говорили условно про кучу сложностей на этом уровне, а если мы переходим еще и на этот уровень, то количество сложностей еще сильнее возрастает. И когда мы говорим, что у нас агентов много, это прям вообще уровень того, что может пойти не так, возрастает супер сильно. Ну и дополнительно у нас могут быть отдельные базы, где мы храним треды, таски, что угодно. Но тут не столько важны сами компоненты, сколько важно помнить, что полноценный агент, вот мы начинали с того, что там в чате 5 такого спрятано, это и LLM, это и инструменты, Это API, которая сверху. Это реализация мультиагентских систем. Это интерфейс и взаимодействие с агентами. И все эти компоненты должны мониториться, оцениваться и контролироваться. Если вдруг кто-то еще не почувствовал сложности, какие здесь могут возникать, просто один из примеров, он такой глуповатый, но это то, с чем мы реально столкнулись на практике. Ну вот кажется, взять модель, развернуть, ну что здесь сложного? На Hacking Face куча обнасорсных моделей. Скачали, подняли, может любой студент МФТИ первого курса, даже не МФТИ, любой студент технический может поднять модельку. Это вообще не проблема. И это правда так. То есть в самом поднятии модельки нет никакой rocket science, никакой сложности. Но когда начинаем мы предоставить модельку не просто как модельку, а как сервис, то понятно, что есть какие-то SLA и какие-то требования, которые мы должны выполнять по скорости, по отказоустойчивости и вот этим всем вещам. И вот мы взяли одну модельку какую-то из опансористных, я не помню, мы разворачивали, и мы хотели проверить ее, насколько она хорошо справляется на вызове функций. Вот тот самый function calling, о котором мы сегодня очень-очень много говорим. И мы решили взять бенчмарк публичный, опенсорсный, BFSL. Это бенчмарк, в котором есть условно примеры вызова и информация о том, что модель должна выбрать правильный инструмент. И мы оценивали качество. Модель опенсорсная, у нее эти метрики все публичные. Мы запускаем модель в продакшене, ожидаем получить точно такую же информацию, как и в публикации, 70%. Но почему-то тест только 60%. В итоге 3 дня синий разработчик в Яндексе сидит, дебажит этот код и тесты. Он мог бы быстрее, если бы чуть повнимательнее. Ну да. В итоге вся проблема состоится в том, что в имени модели в публичном апе было лишнее двоеточие. Все хорошо, но из-за этого двоеточие тесты не проходят. И кажется, что оно работает. То есть 60% дает, но просто не то, что должно было быть. И вот таких вот, это понятно, что такой немножко вычурный пример, но на самом деле вот таких нюансов очень много. И так происходит с любой моделькой, которую мы берем. И в итоге, если мы говорим про развертывание, то вещи, которые важно здесь учитывать, их очень много. Это и тулы, как они обрабатываются, баги, которые есть в опенсорсе. Например, еще один пример. Есть такой популярный движок для инференса моделей VLLM. Это как бы инференс сервер, поверх которого деплоятся модели. У PNI в августе выкладывают модель в open source, вот эти GPT-OSS. И чтобы ее задеплоить, на VLLM оно деплоится, но оно не работает с тулами. И моделька вроде хорошая, но ее никто не использует, потому что она не может вызывать тулы вообще. То есть VLLM как движок open source не поддерживает тулы в openI. Потребовалось 3 месяца, чтобы сделать доработку, чтобы они начали поддерживать тулы. И опять же, в зависимости от видеокарт, от других вещей, может быть большое количество нюансов. А что такое типы квантования моделей? Да, что такое типы квантования модели? Модель – это математическая функция, в которой… Примите, кто спрашивал? Вот, математическая функция, у которой много-много параметров, миллиарды. Параметр в памяти представлен каким-то числом. В самом идеальном виде это двоичное число с 16 битами. Если всю эту модель загружать на карту, она требует очень много памяти. И поэтому есть такой подход квантования, когда мы эти параметры сжимаем с 16 до 8 битов. Но здесь важно, что когда мы сделаем сжимание, то мы можем потерять часть информации в этих параметрах. И поэтому есть разные подходы к квантованию, так, чтобы модель занимала меньше памяти, но при этом качество проседало совсем немного. И таких квантований есть много разных. Мы можем нажимать до целочисленного, можем до float 8. И это тоже играет... Нет, зависит на какой видюхе лучше она заведется. И на какой видюхе заведется, да. На какой видюхе в принципе она сможет завестись. Если просто по памяти не влезет, то она не заведется. Да, это все. Грубо говоря, округляем. Да, округляем. Тут еще такой важный момент. Сейчас я поняла, что я хочу сказать. Смотрите, вот то, что сейчас рассказывает Дима, это при развертывании модели самостоятельно, а не при потреблении ее через какие-то сервисы. То есть если вы занимаетесь всем непосредственно у себя контуре, вы с этим сталкиваетесь. Просто мы до этого исходили из того, что модель развернута, и все с ней хорошо. Все эти сложности преодолены, у нее есть какой-то аппи, вызываются функции и так далее. Здесь же ситуация описывает тот кейс, когда вы скачали модель и самостоятельно ей занимаетесь. И, возможно, у вас еще нет необходимых скиллов, специалистов, вот то, с чем вы можете столкнуться. Потому что модели зоопарк, там авторы, обычно люди скромные, на карточке указывают минимум информации, и для того, чтобы запустить и понять, а запустилась ли она в оптимальном режиме, или это все-таки, что где-то я тут накосячил, нужно, ну, скажем так, немножко посидеть, подумать, поэкспериментировать. И для этого нужны соответствующие скиллы. Да, у меня буквально еще два слайда, и пойдем обедать. Безопасность. О ней можно говорить бесконечно. Мы очень много говорим с разными командами ИБ. Если очень упрощать, какие тут есть способы безопасности? Самые надежные – это все поставить в контур. Об этом мы чуть позже поговорим, уже после обеда. Но все любят контур безопасники. Это самый идеальный вариант, но он долгий, дорогой и сложный. Следующий шаг – это облачные провайдеры. российские и зарубежные. Здесь есть много вещей, которые могут делать вызовы моделей в облаке безопаснее. Это могут быть приватные сети, это может быть отдельный кабель, который проводится из сода клиентов, сод облака. Это ключи авторизации, ролевая модель, подтверждение вызова. В общем, вещей вокруг безопасности, которые можно сделать очень много. И я бы все-таки рекомендовал, мы чуть-чуть поговорим про экономику дальше, но закупка и развертывание всего в анпремии это дорогой сложный капекс, который не всегда оправдывается. Лучше все-таки начинать с пилотов в облаке и общаться с облачным провайдером. Как правило, есть много инструментов, которые позволяют сделать вызовы безопаснее и пройти преграды ИБ. Здесь просто важно общаться с нужными людьми и подобрать правильные инструменты, которые обоснуют, почему вызов безопасен. Очень часто самый простой способ безопасности сказать, что нет, это не безопасно, потому что данные уходят из контура. Понятный аргумент, но надо взвешивать риски. То есть о каких рисках и издержках мы говорим. Как правило, если, например, речь идет просто про вызов модели в облаке, то, опять же, многие уверены почти 100%, что в модель сразу обучится, все залагируется и так далее. Это технически не так. И есть юридические подтверждения, разные способы, почему это не так. Тоже касается и хранения данных, и передачи и так далее. Тут, наверное, идея такая, что не стоит сразу бежать в контур, лучше все-таки начинать с публичных облачных провайдеров, просто потому что это дешевле и быстрее. Правда, сильно быстрее. Мультиагенные системы. Что там дальше идет? Давайте на мультиагенных системах закончим. Мультиагенные системы, это уже совсем и два тема. То есть мы видели, мы собирали простого агента, мы собирали A2A. Мультиагенные системы, как правило, используются, когда мы хотим декомпозировать задачу на несколько агентов, и агенты начинают взаимодействовать между собой, чтобы решать задачу. Ну, например, в самом простом виде у меня есть агент, который возвращает ответы, и второй агент, который делает проверку ответов. Этичные они, еще что-то, не знаю, отвечают на мой вопрос или нет. Это такой самый базовый пример. Его можно расширять, то есть агент может отвечать за какую-то конкретную область, как мы видели с авиалиниями и так далее. Это большая тема. Туда тоже сейчас много исследуется, много говорят. Но, опять же, мультиагенты очень сложные в поддержке, очень дорогие, они едят очень много токенов. И, как правило, редко используются прямо в продакшене из-за сложности и дороговизны. Мы тоже будем подходить к мультиагентам в будущем. Я думаю, что без этого никуда. Но здесь должно случиться ряд вещей и в качестве моделей, и в стоимости инференса, и в целом в качестве всех платформ, которые вот этих агентов позволяют делать. А в чем принципиальная разница, например, NL7M, где у тебя в каждом кубике какой-то, собственно, может быть агент. И вот мультиагентная система. Ну, все это глобально мультиагентные системы. Что я здесь имею в виду про сложность, это когда, условно, вызов другого агента идет через вызов функции. То есть, когда LM-модель сама понимает, что нужно вызвать другого агента, передает управление, этот другой агент делает какую-то работу, дальше возвращает управление обратно. И тут какая сложность? Все эти агенты должны видеть одно и то же состояние. То есть, состояние в широком смысле это и контекст тот же самый, и какие-то определенные параметры и так далее. И когда мы начинаем деплоить всех этих агентов с одним состоянием, которым они обмениваются, И так, чтобы агенты не переписали это состояние или еще что-то, начинается много сложных нюансов, которые делают разработку таких агенных систем достаточно сложной. Через час или сколько у нас времени у нас будет еще один небольшой теоретический кусочек. Вчера был вопрос, хотелось его тоже сегодня немного раскрыть. как сопоставить онпрем и облако, когда какой подход выбирать. И после этого мы уже после обеда тоже попробуем практику, попробуем пособирать агентов, как показывали. Но это будет через час. Так, уважаемые коллеги, прерываемся на обед. Давайте, чтобы было ровно. 15.10. Собираемся здесь и продолжаем. Спасибо. Да, те, кто не сделал портреты еще свои, пожалуйста, сделайте. У нас фотограф ждет 20 человек еще не сделали портреты Это очень много Причешитесь Замажьте мешки под глазами И вперед Спасибо.