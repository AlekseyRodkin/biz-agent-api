Отличительная характеристика нашей программы, то что у нас звонок аналоговый по самой неволусе. У нас это называется чугунный колокол, понимаешь? Точно, мы прямо вот в эту сторону. Все, только аналог там, типа, поезд отправляется. Еще раз колокол нам, извините. Давайте, уважаемые участники, если меня там слышно снаружи, пожалуйста, проходите. У нас сейчас Кульминация Того что мы с вами обсуждали Все эти дни Поговорим про Чтобы понимали уважаемые эксперты И не дезакцитирование Словосочетание агенты И агенты у нас зашкаливает Куда-то в космос Но теперь наша задача Чтобы это из слов превратилось в какой-то образ За этим смысл появился За этим словосочетанием Мы очень сильно надеемся, что коллеги и в вашу сторону тоже было уже много заочных вопросов, но и вчера ты, наверное, слышал, так сказать, когда с Дмитрием Сушниковым был диалог, про это много чего говорили. В этом смысле аудитория подготовлена и готова разбираться, а мы с вами, дорогая аудитория, поговорим с нашими коллегами из Яндекс Клауд, с которыми мы проектировали вместе и разбирались с этим, про то, как автоматизируются бизнес-процессы с агентами, как это работает, что под этим лежит с точки зрения технологии, что под этим лежит с точки зрения логики. И, я надеюсь, вашими вопросами, как это вообще применить к себе. Это, собственно, главная часть. Мы не зря выбрали, так сказать, в качестве технологического партнера и докладчиков коллег из Яндекс.Клауда, потому что нам кажется, что коллеги идут по этому пути систематически. Это их главный, так сказать, вектор развития. И они в этом уже не просто понимают, но и делают, правильно? Уже много уже есть с посчитанными эффектами проектов, на которые тоже можно сослаться. Раз, а два, есть, мы вчера про это говорили, платформа сама по себе. Мы, конечно, показываем всякие разные платформы, которые есть в мире, но свою платформу надо знать, свои платформы надо знать точно хорошо, потому что они соответствуют законодательствам всяким, они безопасные и здесь адаптированы под местные реалии. Поэтому платформа, которая есть у коллег, всячески нами рекомендуется для использования, ну или во всяком случае к ней точно надо присмотреться и не надо пропустить. И потренироваться, если уж тренироваться, то тренироваться на этой платформе и понимать, как что работает. Вот, я не знаю, как это называется, слышали, по-моему, ты был, когда мы эту всю штуку обсуждали. Мне кажется, я просто помню доклад, мне кажется, доклад это оживление тех схем, которые мы здесь рисуем, так сказать, на дощечке. Здесь прямо оживление с точки зрения решений, логик, соответственно, подходов и вообще, так сказать, бизнес-практики, в которой это работает. Дмитрий, предоставляю тебе слово, с Дмитрием Анастасия, Александр у нас позже подойдет. Это хорошо. Да, мы целый день отвели на это, потому что тема, с которой надо серьезно разобраться, поэтому ничего другого не будет, будем только весь день с коллегами в эту сторону заходить. Все, Дмитрий, придавайте слово. Доброе утро. Наверное, такая небольшая преамбула. Я вот сегодня часик посидел, послушал то, что вы здесь обсуждали, и почувствовал себя немножко в роли того неприятного технаря, который приходит и говорит, что нет, это все не полетит, это очень сложно, и технологии сейчас не на том уровне. На самом деле, сегодняшний весь наш доклад, вся лекция, она про то, где сейчас находятся технологии, что они реально позволяют сделать и какие тут есть ограничения. Наверное, что самое важное нужно сказать, это то, что в некоторых местах от языковых моделей и от агентов есть немного завышенное ожидание. И вот, чтобы понимать, где реально, в какие бизнес-процессы этих агентов можно применить, Важно понимать вообще о каких технологиях и на чем они строятся. И сегодня, наверное, будет самый технологический такой доклад из всех, что были до этого. Мы постарались добавить максимально дискуссии, интерактивов, чтобы это не выглядело и не звучало слишком сложно. Но если какие-то моменты нас сильно уносят, мы оба технари, если нас уносят, пожалуйста, останавливайте, поднимайте руки, говорите, что, где, пояснить подробнее. Ну, давайте начнем. Пару слов, кто мы вообще такие. Меня зовут Дима. Я в Яндекс.Клауд отвечаю за продуктовое развитие слоя платформы iStudio, который предназначен как раз для построения агентов. Именно поэтому мы в том числе готовили этот доклад про агентов. Вообще у меня опыт такой, что я почти всю свою рабочую жизнь занимаюсь так или иначе платформами. Начинал я с того, что активно занимался платформой в компании IBM. У них была платформа для классического ML, когда еще не было генеративок. Были популярны такие подходы, как ML Ops. Они и сейчас популярны, просто о них не говорят. Но ML Ops, классические модели, и у IBM была платформа как раз для реализации ML Ops. Потом я перешел в российский интегратор, поработал там какое-то время. И вот сейчас уже третий год работаю в Яндексе, где мы делаем платформу, но уже не для классического ML Ops, а для условно LRM Ops или вот всех этих вещей вокруг генеративных моделей. Настя. Доброе утро. Меня зовут Настя Каримова. Мой путь в ML был более фернист. Начинала я как iOS и Android-разработчик и думала, что это любовь на всю жизнь. В какой-то момент я попала в компанию Яндекс более 10 лет назад. И тогда мы начинали делать голосовые интерфейсы. И на первом этапе это был тот самый интерфейс навигатора, который наверняка знаком многим из вас. Он вначале был довольно простым, можно было оперировать, скажем так, ограниченным вокабуляром. Мы делали под это СДК, развивали направление спички ТА. И в какой-то момент, когда мир перешел на Алису, я буквально влюбилась в эту идею. На самом деле на тот момент Алиса даже близко не была, близка к тому агенту, но она была каким-то живым персонажем. С ней можно было общаться, и это, наверное, было больше на тот момент нашей фантазии, но идея настолько меня увлекла, что я больше не могла заниматься просто разработкой. Мне хотелось перейти в какую-то продуктовую стезю, и, собственно, я это сделала. Появились курсы валют, появились какие-то простейшие интенты, и вот так, шаг за шагом, я попала в этот мир агентов, и периодически меняла роль и была продуктовым менеджером, была техническим менеджером, но так или иначе и очень сильно увлек именно своей персонажностью. Сейчас я работаю в компании Яндекс.Клауд, по-прежнему есть близкие связи с Алисой, по-прежнему люблю этим всем заниматься и надеюсь, что сегодня мы вовлечем вас в эту всю историю И вы, если не пробовали ранее, попробуйте у себя в бизнесе как-то применить искусственный интеллект. Спасибо. Да, спасибо. Слышно, да? Нормально? Пару слов о том, как вообще тут происходит водораздел в Яндексе. Алиса наша – это B2C-сервис, то есть это для конечных пользователей. То, что делает Облако или B2B Tech, AI-студию, про которую мы будем говорить, это наше решение именно для бизнеса. Но сегодня не столько про Яндекс, сколько в целом про агентов, что они могут, что они хотят. И вот сегодня уже звучала утром фраза про чат-гпт как интерфейс именно чат-бота. И, наверное, вся наша первая часть будет о том, чтобы показать, что на самом деле чат-гпт, как он есть, это не про чат-бот, а это полноценный агент, причем очень сложный. И мы хотим показать, чтобы такой агент вообще существовал, какие здесь нужны инфраструктурные и программные шаги, компоненты, чтобы эта вся штука заработала. Но когда мы общаемся с клиентами, последний год мы общаемся очень много, чаще всего что мы слышим? Мы хотим иметь в компании чат-гпт, примерно в том же интерфейсе, как он есть в чат-гпт, но чтобы он работал с моими данными, чтобы он был безопасным, чтобы я мог загружать туда конфиденциальную информацию, чтобы он учитывал роли, которые есть у моих сотрудников. Если мой сотрудник имеет доступ к SAP или к CRM, то эта роль должна передаваться и на агента. Все это должно быть изолировано, безопасно и так далее. И глобально стоит вопрос, а как мне в компании построить такого агента чат ГПТ только полностью безопасного? Понятно, в большинстве компаний России ходить в западные облака или вообще ходить в облака, Даже российские это либо отобуированная тема, либо очень сложно и требует большого согласования с безопасниками. И вот сегодня мы хотим поговорить про те компоненты, которые нам нужны, чтобы такого чат-ГПТ собрать в своей компании. Давайте, наверное, попробуем определить, что вообще из себя представляет этот чат-ГПТ. Во-первых, это в каком-то виде диалоговый интерфейс. Нам нужен интерфейс, куда мы можем сказать, сделай то-то, открой то-то, создай мне тикет, отправь почту, или выполнить какую-то более сложную задачу. Он должен понимать, естественно, язык. Конечно, мы говорим про ЛМК. У него должен быть доступ к базам знаний внутри компании. Это, на самом деле, как правило, самая сложная часть, потому что здесь встает очень много таких вопросов вокруг безопасности. Он должен выполнять внутренние задачи для конкретного сотрудника, и в идеале он должен быть персонализированным. То есть, если мы говорим про финансы, он должен иметь навыки, которые подходят для работы финансиста. Если мы говорим про юриста, то, соответственно, этот юрист должен иметь возможность в простом интерфейсе загрузить свой договор, нажать кнопочку «Составь мне заключение», и это заключение будет готово. То есть это интерфейс плюс набор каких-то навыков, которые у агента могут быть. Он должен быть подключен к опишкам, к документам и может выполнять запросы. И здесь давайте, наверное, сделаем такую первую паузу. И я попрошу вас подумать. Можно первый слайд «Ага»? и накидать через АХА, какие компоненты должны быть у такого чат-гпт, чтобы его можно было реализовать. То есть если попробовать начать технически проектировать, вчера Дима рассказал про LLM, рассказал, из чего они стоят. Но вот что еще кроме LLM нужно, чтобы такого помощника можно было собрать? Давайте мы сейчас пошарим QR-код. Прямо с нуля. Можно включить первый слайд. Какие компоненты? Второй слайд. Пожалуйста, отсканируйте QR-код, и там должно быть поле, в котором вы можете вводить любые слова, условно, компоненты, которые нам нужны для сборки агента. А можете запустить, пожалуйста, презентацию? Кто на пульте показывает сейчас? Или это я показываю? Это хорошо. Так, сейчас. А, это я шарю, да? Нет, сейчас я просто найду его, где он показывается. Давайте попробуем вот так. А вот сейчас что-то изменилось? Я запустил именно презентацию. А, вижу, да. Спасибо. нужны компоненты для реализации такого чат-гпт в компании. Ура, пошло. Да, моя любимая «Много денег». На самом деле про деньги сегодня тоже поговорим. Вчера был вопрос к Дмитрию про сравнение облачного подхода и он премного. Про это есть отдельный блог. Пошло же вечер, я думал. Супер. Давайте посмотрим, что у нас самый центральный блок, который не меняется и не летает никуда, это данные. Вы много говорили про данные, это действительно так. Любое IML это про данные, и это действительно супер важная вещь. Хотя здесь тоже важно сказать, что подход к данным в классическом... О, нужен Дмитрий Рыбалка. Да, замечательно. С удовольствием. Да, регламенты, математики. Давайте из больших базоданных данные. Модель, на самом деле, без нее никуда. Ядро, на основе чего строятся все агенты. Хотя мы сегодня посмотрим на два подхода к агентам, где модели могут использоваться по-разному. Интеграции все верно. role-based access – супер важная вещь. Про MCP мы сегодня как раз в этой связи будем много говорить. База данных. Вот это мне нравится. Там была где-то база данных, плюс тулы, плюс ВБД. Не знаю до конца, что внешняя база данных. Векторная, да, да. Векторная база данных. Все так, айтишники. Вот, кстати, про айтишников интересная дискуссия. Думаю, тоже сегодня немного поговорим, насколько они нужны. Бизнес-наказчик 100%, железоинфраструктура. Вот, не знаю, Настя, есть что-то, что тебя цепляет? Давайте покажем, что нарисовали мы. Когда... На самом деле Дима уже заспойлерил, Но, наверное, стоит думать в ту сторону, что должен уметь такой чат GPT, не только откуда он получает знания, а какие действия вы планируете совершать с ним. Советовал подумать в эту сторону. Да, у нас, к сожалению, у Яндекс.Клауд такая тема, что у нас все презентации с большим количеством голубого цвета. И как мы вчера определили, голубой цвет не отражается на этих экранах, поэтому эта схема выглядит немножко не так, как она у нас выглядела изначально. Представьте, что здесь везде есть голубые красивые кубики. Но если обобщить часть компонентов не столько процессно, сколько технически, то что должен уметь такой чат GPT? Когда вы задаете какой-то вопрос, вы могли замечать, что если этот вопрос про самые актуальные данные, то он пишет «я схожу в интернет». Или вы можете загрузить файлы, модель будет отвечать по файлам. Либо он сам может нарисовать, например, график и вернуть этот файл. Таким образом, что у нас получается? У нас в основе такого агента всегда лежит языковая модель. И дальше эта языковая модель расширяется набором ручек, инструментов, которые она может дергать и выполнять, чтобы решать какие-то задачи. Она может сходить в интернет, она может читать файлы и генерировать файлы, она может строить таблицы, графики и так далее. Она может вызывать внешние API, и это как раз то, где сейчас стандартно становится MCP. И еще важный компонент – это память. Когда мы ведем какой-то персонализированный диалог, может вы замечали, там чат GPT запоминает. Если ты спросишь, а что ты обо мне знаешь? Вот мне он недавно рассказывал, что я там интересуюсь API-шками, интересуюсь респонсом с реалтайм. Вот, кажется, что я айтишник. Вот, чат GPT умеет автоматически выделять ключевую информацию из диалогов, сохранять эту информацию в базе, и потом передавать в других контекстах, в других диалогах, чтобы это общение было более персонализированным. И на самом деле это все не способность языковой модели. Это все набор дополнительных компонентов, которые находятся рядом с этой языковой моделью. И теперь, если мы посмотрим на вот эту сложную картинку с большим количеством слов, у нас получается сложная техническая архитектура, чтобы таких агентов вообще можно было создавать. И LLM это только небольшое маленькое ядро, ключевое, важное, без него никуда. То есть если будет плохая LLM, ничего адекватно работать не будет. Но чтобы построить агентов, которые действительно будут работать в продакшене, которыми можно как бы поделиться на конечных пользователей и не бояться, что он начнет говорить ерунду или вообще перестанет отвечать, нужно все вот эти четыре шарика, про которые мы сегодня будем подробно говорить. То есть в основе LLM. Дальше, если вот посмотреть на AI Agents, это, например, долгосрочная память, декомпозиция шагов, безопасность, рассуждение в несколько шагов. Это история с рагами, история с планированием, обработка ошибок, безопасность, compliance. И вот масштабирование, в общем, такие неинтересные с бизнес точки зрения вещи, но при этом очень важные, если мы говорим про нефункциональные требования к таким агентам. Если говорить вообще чуть более строго и технически, то агент, это знаете такое слово, меня до сих пор немного смущает, потому что все, когда говорят про агентов, вкладывают очень разные технические вещи. Начиная от чистой лэмки, заканчивая сложным потоком, где вообще почти языковой модели нет. И это все называется агентом, и это в целом правда. Поэтому мы, скорее, когда внутри обсуждаем агентов, мы склоняемся говорить про агентскую архитектуру или агентские подходы, или агентский паттерн. И что здесь имеется в виду? Этот паттерн заключается в следующем. У нас есть языковая модель, которая является основой, ядром, которая принимает решение, какие инструменты нужно вызывать. Дальше ей нужно дать набор этих инструментов, которые могут быть вызваны. То есть, условно, процесс выглядит следующим образом. Я говорю модели, как мне оформить командировку. Модель может начать отвечать из своих знаний, но она ничего не знает про то, как в моей конкретной компании, в каких системах оформляется командировка. Мы можем подключить RAC, подключить базу знаний. Тогда модель может описать, ну, для этого перейдите туда, нажмите то-то, напишите тому. Но это тоже не совсем тот уровень автоматизации, который я бы хотел получить в идеале. Хотелось бы, чтобы модель мне сказала, чтобы оформить командировку, тебе нужно скажи, куда ты хочешь поехать, укажи дату, укажи еще что-то. И модель сама вызвала нужные инструменты, которые оформили мне где-то в моем трекере, джире, еще где-то в системе нужный тикет. Все это согласовал, запустило процесс. И дальше я получил подтверждение, что запрос на командировку отправлен. И каждый такой шаг – это вызов конкретной API-шки конкретной системы. То есть модель понимает, что нужно вызвать, какой инструмент, делает вызов этого инструмента, получает результат, обрабатывает его и таким образом интерактивно ведет диалог общения. Понятно, что в зависимости от процесса, где это происходит, вот эта цепочка шагов, она может быть очень сложная. И в этом заключается сложность построения агентов. Как вообще модель понимает, что нужно делать? Это решается инструкцией. Тоже сегодня в первой части немного говорили про инструкции. Инструкция или промт. Это очень важная вещь. И она описывает то поведение, как агент вообще должен выполнять те или иные задачи. Ну и, конечно, память. Память мы тоже будем сегодня говорить. Их бывает очень верхний уровень 2. Это краткосрочная память или сессионная память. То есть, когда я общаюсь, модель помнит мои предыдущие ответы и учитывает их при генерации новых ответов. И долгосрочная память, где модель условно формирует какое-то знание о конкретном пользователе и потом может в других диалогах это знание использовать. Это гораздо более сложная память. И сейчас ее пока только скорее в исследованиях, в таких первых прототипах начинают использовать. Давай дальше. Спасибо. Чуть-чуть в сторону. Если говорить про агентов, ну вот сегодня уже коллеги говорили, что про агентов говорят все. Это видно и по всяким графикам и отчетам, количество статей научных, где говорят про агентов, количество open source репозиториев, вакансии агентов, популярность запросов. Агенты растут, думаю, что тут ничего нового я сильно не скажу. Давай дальше. При этом, если смотреть на проникновение агентов, это опрос PwC, Pricewaterhouse, не про российский рынок. Тут более интересная картина. То есть примерно у 35% широкое внедрение, у 27% ограниченное внедрение, у 17% полное. Ну и остальные там изучают, не планируют еще что-то. И тут как бы дьявол в деталях. Опять же, то, что я говорил в начале, когда говорят про агента, это не совсем, или, как правило, совсем не тот агент идеальный, где языковая модель дергает разные инструменты. Как раз из-за проблем с галлюцинациями, с всем со всем и так далее. Если пообщаться с многими компаниями в реальности, что у них внедряется, то, как правило, это жесткая цепочка шагов, в которой в каком-то моменте используется LLM, что-нибудь достать, ответить, и это уже называется агентом. Поэтому к таким цифрам нужно сейчас подходить очень аккуратно, очень часто. Это пока не полноценные агенты, идеальные, которые автоматически сами принимают решения и что-то решают и делают. Почему? Потому что, как мы увидим, есть очень много инженерных сложностей на всех этих шагах цепочки. От самой модели до вызова инструмента, обработки результатов и так далее. Да, следующий слайд. Если говорить про агентов, которые приносят ценность, согласно тому же исследованию, которое мы здесь приводим, то среди ценностей это рост производительности, снижение затрат, быстрое принятие решений, улучшение клиентского опыта и так далее. В общем-то, тоже достаточно очевидные вещи. При этом, опять же, важно помнить, что под агентом могут приниматься очень разные вещи. Если говорить про вообще сценарий, где агентов начинают применять, то подавляющее большинство внедрений полноценных агентов – это поддержка в широком смысле. Техническая поддержка, клиентская поддержка, клиентский сервис, всякие суфлеры, помощники операторов. Это достаточно понятный кейс, понятно, как его делать. Есть много готовых инструментов, которые его автоматически решают. И это первый способ начать внедрять ИИ. Продажи маркетинга тут, как правило, это не очень сложный сценарий. Какой-то креатив, графика, еще что-то нарисовать. IT и кибербезопасность немного удивительное место. третье место. Если говорить про IT как помощники кода, то понятная история. Это практически в любой компании не используется. В кибербезопасности я знаю много сейчас попыток, подходов, продуктов, но в кибербезопасности очень высокая цена ошибки и полностью доверить кибербезопасности ЛМ пока мало кто решится, просто потому что очень много рисков галлюцинаций. Все остальные сферы там тоже появляются специализированные решения под юристов, под финансы. Эта область тоже развивается, но там еще большой путь до высокого качества. Это такая первая вводная часть. Наверное, хотелось бы чуть больше от вас послушать, чтобы мы ориентировались. Когда мы говорим про и агентов, какие процессы вы планируете внедрять? насколько вот эти данные, которые показал PwC, бьются с тем, что вы видите в своих компаниях. И вот что происходит у вас? Поделитесь, пожалуйста. Продажи. А что именно в продажах? Какие агенты в продажах? Это именно подсказки ему? или прям какая-то полноценная автоматизация замена? А вы какие-то метрики замеряли, типа качество, насколько хорошо им строится? Да. Но в целом уже как бы против, прямо на поясе работает. Может быть, у кого-то есть более простые агенты, которые, может быть, не встроены именно в процесс, но делают какие-то, осуществляют какую-то помощь. Финансы? Финансоаналитика типа телеграм-каналы, новости, отчеты? Нет, в компании. Именно в АС, АПУ, баланс, показания. Это именно на базе ЛМок или скорее классические ML, классические подходы, регрессионные модели? ЛМки именно. Как-то замирали качество? Я только планирую. А, только планируете. Да. Несколько типов. Одни попроще. Это, грубо говоря, аи роль руководителя проектов. Айзировать. То бишь сбор информации по чатам, перепискам и составление тудушек, что ты должен, кому должен, когда должен. Это дело провалить в Task Tracker, в ту же с Jiru, и в проектные планы с рисованием гантов, презентации и все остальное. Это один такой флоу. И второй флоу это цикл discovery гипотез продуктовых, когда мы анализируем данные из различных источников и на продукту, отаранжированный по некоторому, по райсу, еще чему-то, набор гипотез выбери и погнали. Да, интересно. А это вы планируете или уже в каком-то подступили к реализации? Ну, прототипы делаем уже. А и аналитик сделали в некотором смысле. Сколько примерно заняло времени на создание прототипа? Ну, чистого времени, наверное, месяцок на такого простенького ботика, который ищет по заданным метрикам нужную информацию. А какой ОЛМ используете, если не секрет? Сейчас ни в чем себе не отказываем, выбираем самые лучшие, а дальше уже будем смотреть, что из этого ОЛМ можно будет развернуть. Спасибо. Да, кто-то еще хочет поделиться? Вот сейчас, сколько взяли проект, взяли слой данных, мы называем это агентик data engineering. Задача этой мультиагентной системы - это сбор с пользователя запроса, построение, скажем так, Какие данные нужно собрать и сохранить? Потом схематизация, построение, на каких технологиях система должна развернуться. Стащить со всех источников данных в один источник данных, нормализовать, структурировать, проверить полноту данных. Спасибо большое. На самом деле очень интересный сценарий. Мы как раз сегодня будем говорить про эту разницу. Вот если, например, в случае с проектным агентом, то это, как правило, агент, который хорошо может решаться именно таким классическим агентом, где у тебя есть языковая модель, которая ходит в разные инструменты, собирает информацию. Например, в вашем случае архитектурный подход будет немного другой. Он будет заключаться в том, что это будет какая-то цепочка, где языковая модель будет использоваться в разных шагах, скорее как вспомогательный инструмент. Сходили в данные, прочитали схему, сделали описание языковой моделью. Дальше, например, сформировать языковую модель и искать запрос, чтобы собрать нужную информацию. И это будет другой подход, который именно основывается на цепочке вызовов. И мы сегодня как бы цель, глобальная цель научиться различать, какой подход лучше применять и какие у каждого подхода есть плюсы и минусы. По сути, все их называют агентами, но в них есть как бы кардинальная техническая разница. Давайте пройдемся по базовым блокам агентов, из чего они состоят. Настя, слово передаю. Спасибо. Давай тогда на следующий слайд. Дима обрисовал вам целую вселенную того, что можно делать с агентами. И сейчас мы начнем с того, что начнем с ядра, которая сосредоточена вокруг LLM. Мы обозначили некий паттерн агента, обозначили, какие части не уходят, и что там есть LLM, память, инструменты, что есть некая инструкция, которая дает ТЗ LLM. И на самом деле это такой паттерн, в котором мы обозначаем компоненты, но при этом не особенно определяем там какие-то шаги взаимодействия, какие-то четкие правила того, как это все должно быть организовано. Поэтому сейчас мы идем по компонентам, потом мы расскажем вам подходы, как их можно соединять между собой. Первое, с чего мы начнем, это с того, что немножко вернемся назад, Прошлая лекция, которая, как я понимаю, была вчера про LLM. На ней вы узнали, что LLM это большие языковые модели, основная функция которых, из которой лучше всего они справляются, это продолжение текста. То есть вы пишете какой-то текст, которым определяете, грубо говоря, ТЗ модели, может быть, определяете роли, то, что надо делать, какие ограничения могут быть, и на вход получаете текст. На самом деле для модели это все суммарный текст, который она генерит, продолжая ровно то, что вы подали на вход. И вот то, что LLM-ки, они большие, умные, обучены на огромных массивах данных, они умеют решать какие-то задачи, ну то, что в это вкладывали их создатели, сделала порог входа в ИИ, ну скажем так, невероятно низким в сравнении с тем, что было раньше. Раньше, когда вы делали какого-то бота, что-то автоматизировали, вы там расписывали, грубо говоря, максимально подробно граф, и говорили, в каком месте этого графа какая модель за что отвечает, где она что автоматизирует. Модели были максимально специализированы, не могли решать задачи, все подряд не обладали какими-то всеобъемлющими знаниями, и приходилось каждый раз крутить этот элемент графа, настраивать его под свои нужды. И это было довольно специфично, требовало большого количества данных, требовало большого количества усилий и требовало определенных специалистов. ЛЛМ снижает этот порог входа. Основываясь на том, что вы подали текстом, она вам отвечает весьма разнообразно, в своем базовом каком-то варианте. Она может сделать суммаризацию текста, исправить ошибки, ответить на вопросы, то есть креативить, решить проблему чистого листа максимально. И это позволило многим людям, даже достаточно далеким от ИИ, привлечь его в свою жизнь. Вот, например, я сегодня ехала в такси, и там телеграм-канал дамскроллера скролила, и вижу какие-нибудь найден промпт, который помогает похудеть, выучить английский и так далее. То есть люди уже активно как-то внедряют это в свою жизнь. Про представителей ЛЛМ вы, наверное, тоже поговорили, все их прекрасно знают, Это индексовые, open-a-ish, deep-seek, open-source и так далее. Ой, можно вернуться назад, пожалуйста? Как называется та инструкция, которая подается на вход? Она называется prompt. Наверное, это тоже уже всем известное понятие. Промпт может быть хаотично-текстовым. И я на самом деле с драганием смотрю каждый раз, когда люди начинают писать, излагать свои мысли, что-то пытаясь сказать модели, дай мне сделать то-то. Это напоминает, знаете, как пришел новый человек, какой-то сотрудник в компании, у него нет контекста, ему нужно что-то сделать. И вы привыкшие давать максимально короткое указание, такие, ну, вот сделай то-то. Но чтобы он это сделал, ему нужно объяснить контекст, задать. И искусство составления промта, оно одновременно простое и сложное. То есть модель что-то сделает, что-то отдаст, но для того, чтобы она сделала то, что удовлетворит вашим метрикам, устроит вас, необходимо для этого приложить усилия. Также вы узнали наверняка, что prompt можно поделить условно на системные инструкции, пользовательский вот, который можно как-то менять при взаимодействии. Системный промпт дает какие-то более глобальные настройки, которые нельзя переопределять. Ты, допустим, ассистент, который помогает с поддержкой. Ты отвечаешь на какие-то вопросы. Если тебя спросят про темы, которые, скажем так, не касаются твоих прямых обязанностей, говоришь, что ты в этом не понимаешь. И давайте поговорим, допустим, по тематике, за которую я точно отвечаю. В общем, какая основная идея? LLM очень мощный инструмент с минимальным порогом входа. Он предобучен на задачах. За вас заранее подумали. Он из коробки умеет очень-очень много. И общаться с ним достаточно просто. Все строится на тексте. Но, конечно же, если бы это было все без ограничений и не было многих «но», было бы слишком просто. Поэтому переходим на следующий слайд и какие ограничения есть у LLM. Например, ограничение по актуальности знаний. Когда вы обучаете модель, у нее есть некий срез, допустим, всех глобальных знаний на данный момент. Она их и получает. То, что произошло после обучения, ей, соответственно, базе своей недоступно. Также у ОЛМ есть ограничение взаимодействия с внешним миром. Вот этот самый текстовый интерфейс. Он как бы спросили текстом, ответили текстом, какие-то действия по дефолту она делать не умеет. Конечно же, галлюцинация. Не забываем, что... Галлюцинация, давайте, и отсутствие внутреннего понимания. Не забываем, что LLM-ка это прежде всего такая многоходовочная математическая функция, которая может привирать. она не про смысл, она про вероятность слов, которые она генерирует, исходя из тех огромных массивов данных, на которых она была обучена. Поэтому, например, если вы спросили, кто выиграл чемпионат 2026, и она уверенно ответила, что сборная Россия, ну это вот вам как пример галлюцинации. Или там привела, как известная цитата условно Ленина из интернета, это тоже стандартная галлюцинация, которая может встречаться у ЛМ. Еще сейчас у нас проходит серия вебинаров в IE Studio в Яндексе. И, наверное, то ограничение, про которое хочется сказать, это размер контекстного окна. То есть размерность текста, который может оперировать ЛМ. На самом деле, для некоторых кейсов, случаев, пока это может являться ограничением, но это также можно обойти. Вообще всю проблематику, которую мы обозначили здесь. Мы покажем, как ее можно обходить на практике, что с этим можно делать. И это, скажем так, не является блокером. Давай дальше. И мы переходим на следующий слой, где мы расширяем возможности LLM, где мы показываем, как преодолеть все те ограничения, которые мы обозначили OLM на старте. И первое, про что хочется рассказать, что, наверное, интересует многих из вас, как LLM вообще взаимодействует с физически цифровым внешним миром. И есть такой механизм, называется function calling. Как это происходит? Как разработчик, который взаимодействует, строит взаимодействие с моделью? Я даже не знаю, можно его назвать промт-инженером? Или промт-инженер – это тот, кто перебирает текста для того, чтобы модель максимально хорошо отвечала? Ну, допустим, пускай будет тоже промт-инженер, потому что function calling, отчасти, это тоже часть промта. Что происходит? Разработчик говорит модели, смотри, у меня есть такие-то методы, такие-то ручки, которые ты можешь дернуть. Например, пускай будет агент, который де-факто что-то типа Алисы, Алексы или еще что-то, который может включать музыку, который может говорить погоду. Это один из самых частых сценариев, который вообще используется в Алисе, говорю вам, как бывший продукт этого сценария. И мы говорим, вот смотри, дорогая, у тебя есть столько-то ручек, и у этих ручек есть какие-то параметры. Например, getWesa. Самый простой банальный пример, который чаще всего фигурирует при описании function-холлинга. У него какие параметры? Например, дата, локация, для которых нужно выяснить погоду. И когда ты поймешь модель, что тебе нужны актуальные знания про погоду, скажи мне, чтобы я вызвал функцию с такими-то параметрами. И на практике действительно так происходит. Когда пользователь обращается к агенту, построенный поверх ЛЛМки, говорит, скажи мне сегодня погода в Москве. Естественно, у ЛЛМки в базе своих таких данных нет. Но она знает, что у нее предоставлена функция. и она говорит, нам генерирует определенный текстовый вывод, в котором говорит, хочу, чтобы вы мне запроцессили эту функцию. И аргументы такие. Пользователь не уточнит, какая дата, поэтому дата пускай будет сегодня. Пользователь сказал, в каком городе. Город, не знаю, Киото. Вы как пользователь получаете определенный вывод текстовый от модели и вызываете функцию с соответствующими аргументами. Это может быть какой-то код, написанный прямо здесь и сейчас в этом же файле. Можете сходить в какой-то метод API, дернуть ручку, например, погодного сервиса. Сделайте какое-то действие, по результату этого действия отписывайтесь в модели. Вот, смотри, погода такая-то. И модель, обогащенная этим контекстом, генерирует окончательный ответ пользователю. Погода в Киото, плюс 10 сегодня, солнечная. безветренно. Таким образом, можно, например, примеры использования, которые здесь приведены. Курс валют, какая-то актуалочка, запуск каких-то действий. То есть вы уже обогащаете модель за счет того, что она делает своеобразный хук вам какими-то дополнительными возможностями. Но у Function Calling есть много возможностей, но в чистом виде он не то чтобы очень приятен в использовании. Как вы видели, мы должны подать какой-то список абсолютно всех ручек, надо их как-то описать, очень сильно постараться обозначить. Потом, когда модель, соответственно, скажет, что вызывается такая-то функция, вы должны как-то сходить в API. То есть, если вы технически когда-то что-то программировали, реализовывали, перспектива вырисовывается такая, что хочется здесь какого-то рефакторинга, хочется контроля за безопасностью вызова тех или иных И здесь наступает такой момент, что в чистом виде фанкшн-холлингом не то чтобы приятно пользоваться по многим причинам, это сложно масштабировать, сложно следить за безопасностью. Мало ли что модель дернула, удалила запись в банковском приложении и от этого пострадал конечный пользователь. То есть хочется какого-то большего контроля. И индустрия, которая сейчас всецело сосредоточена на агентах, конечно же, нашла выход из этого. А, еще такой момент. Еще заметили определенные паттерны функции, которые хочется дергать с помощью модели. И вот индустрия ответила на это протоколом и встроенными инструментами. Про это расскажет Дима. Да, то есть если чуть-чуть саморизировать, function calling – это как бы способность языковой модели определять, что нужно вызвать внешний инструмент. Когда вам говорят, что LLM вызвала какую-то внешнюю систему, это не совсем корректно технически. LLM сама по себе, если взять веса модели, она не может ничего вызвать. Это просто математическая функция. Функция сама по себе не умеет вызывать. Но у модели есть способность определять, что нужен вызов инструментов и говорить, как его вызвать. То есть какие параметры для этого нужны. На самом деле даже есть специальные модели, которые обучаются именно под это. Например, можно услышать, что кто-то говорит, вот эта модель конкретная, она очень хорошо умеет болтать, красиво отвечает, но вообще не работает в Function Calling. Или наоборот, вот эта модель заточена под Function Calling, но она не умеет совершенно красиво отвечать на вопросы пользователей. Это способность, под которую модели затачивают, чтобы реализовать агентские сценарии. Дальше появляется вот эта обвязка вокруг языковой модели. Хорошо, модель определила, мне нужно вызвать функцию погоды. Модель определила, пообщалась с пользователем и сказала, что для функции погоды мне нужно заполнить локацию, нужно заполнить дату, еще что-то. Как этот вызов происходит? И вот тут начинается первая сложность. Нам нужно интегрировать языковую модель и код вокруг нее с моими внутренними системами, которые есть в компании. Следующий слайд. То есть мне нужно вот эти внешние сервисы как-то подключить. И для этого компания Anthropic предложила отдельный протокол. Он называется MCP, открытый протокол, Model Context Protocol, который на самом деле по немного загадочным причинам стал условно стандартом. Почему говорю по загадочным причинам? Потому что если вы пообщаетесь с технарями, они вам скажут очень много причин, почему MCP плохой, почему не использовать Open API, почему это неудобно, почему это плохо. Но иногда в AI это развивается немного неожиданными образами. и не всегда качественные вещи с технической стороны становятся условно стандартом. Примерно так произошло с MCP. И что он позволяет сделать? По сути, вы описываете, когда свою систему… Система – это набор API, то есть набор ручек, инструментов, которые можно дернуть, выполнить, чтобы что-то сделать. Давайте возьмем пример CRM. Самая простая CRM. Хотим две информации. Получить информацию о конкретном клиенте, Передаю ИНН по API, в ответ получаю описание клиента. И второе, добавить нового клиента. Отправляю название описания, сохраняю этот результат. У нас есть Bittrex, AMA, CRM, куча разных CRM со своими форматами API. Что мы делаем, чтобы интегрировать вот эту CRM с нашим агентом? Сначала мы делаем условно обертку техническую, которая трансформирует API конкретной системы, Bittrex, AMA и так далее, в формат MCP. Дальше все основные провайдеры OpenAI, Anthropic, Яндекс.Клауд, Cloud.ru, все они поддерживают подключение вот этих внешних ручек в формате MCP. Получается, у нас тут выходит такая стандартизация. Вам не нужно для каждого провайдера переписывать отдельно. Вы один раз обернули в MCP, дальше можете подключать в разных провайдеров. То есть условно MCP часто сравнивают с USB стандартом для агентов. Это общий стандарт, который позволяет сделать эту интеграцию. И если вы внутри у себя выстраиваете вот это общее хранилище инструментов, как правило, оно выстраивается на базе MCP, и получается, что вы можете переиспользовать одни и те же API в этом формате в совершенно разных агентах. Давай дальше. Вот, то есть что он позволяет этот протокол решить? В первую очередь он отделяет логику агента от инфраструктуры и бизнес-систем. То есть это прослойка, как правило, делается вообще отдельное решение в компаниях, не знаю, function storage, tool registry он называется, mcp registry у нас в облаке, он называется mcp hub. Неважно. Условно это какой-то набор ручек отделенных. И дальше, что важно, за счет того, что это отдельный компонент, вы можете туда навешивать специфические для бизнеса, как бы и для безопасности вещи. Контроль доступов, мониторинги, логирование. Все это хранится в отдельном компоненте без привязки к агентам. И что важно, за счет того, что теперь у нас происходит такое разделение обязанностей между языковой моделью и инструментом, у нас появляется больше прозрачности. Мы не доверяем чисто тому, что придумала, ответила модель, а мы всегда можем пойти проверить. Окей, вот мой агент, он вызвал этот инструмент, он получил от этого инструмента вот такую информацию, а дальше почему-то ЛНК ответила не то, что ей передали из инструмента. И это конкретное место, где мы можем пойти, начать раскручивать цепочку и смотреть, где произошла ошибка, исправлять модельку. Таким образом, самые частые сценарии, где MCP используется, мы очень много видим примеров с CRM. То есть вот такая автоматизация как раз продаж, где мы подключаем CRM к нашим моделькам. Это частая очень история. Это всякие трекеры, джиры, вещи для технического ведения проектов и так далее. Это такие самые первые простые шаги, но глобально я думаю, что мы примем к тому, что условно будет MCP под любую систему API и, соответственно, через MCP можно будет подключить агента к любой внешней системе. Хотя это немножко в сторону, скажем так, advanced topic. На прошлой неделе компания Anthropic, все та же самая компания, которая написала MCP, сказала, что все, MCP уже не то и у нас новая парадигма, мы будем генерировать код. Вот вчера, я не помню, у кого был вопрос по интеграции, по-моему, то ли с базами данных или что-то такое. И Anthropic говорят, зачем нам MCP, непонятный отдельный шаг, а давайте моделька будет генерировать код похода в API-системы. Дальше рядом с этой моделькой будет происходить выполнение кода, мы будем ходить с системой, получать результат и его обрабатывать. Нам даже не надо там ничего заводить, никакие MCP настраивать, просто как бы все доверим коду и модельке. Звучит привлекательно, но я лично не могу себе представить того безопасника в компании, который скажет, да, я разрешаю модельки сгенерировать код и выполнить поход мою продуктивную базу. Ну то есть никогда ни за что, ну по крайней мере на текущий этап развития, кто-то разрешит так делать. Поэтому от MCP в ближайшем будущем мы не уйдем. Я думаю, что он будет развиваться, он будет стандартом. К коду мы тоже будем подбираться, к генерации кода, но с очень большим количеством ограничений на ту среду, где этот код может выполняться. Когда мы говорим про MCP, давайте немножечко посмотрим на диаграмму. Диаграмма может быть страшна, но тут скорее идея показать сам процесс. У нас есть юзер. Юзер это конечный пользователь, который хочет получить информацию о каком-то клиенте из CRM. Он пишет, расскажи мне про ООО «Ромашка». Что происходит дальше? Этот запрос прилетает в языковую модель. Языковая модель — это столбик «responses». Почему «responses»? Название нашей API в облаке «responses API» — это и наша API, и OpenAin, это условно стандарт взаимодействия по API с LLM. По сути, можно заменить «responses» на языковую модель. Прилетел запрос. Что модель должна сделать? К модели уже заранее подключен MCP-сервер. который описывает все взаимодействие с этой CRM. Модель, как бы даже не модель, а обвязка вокруг модели, делает запрос и говорит, расскажи мне, какие у тебя есть инструменты, что я могу сделать с этой CRM. И вот этот сервер, он возвращает информацию о инструментах, которые доступны для языковой модели. То есть набор тех ручек, которые модель может дернуть, чтобы выполнить какую-то задачу с CRM. Окей, модель посмотрела на эти ручки, посмотрел на запрос пользователя и говорит, так, пользователь хочет информацию о ромашке. Я вижу, у меня есть ручка get company by NNN. Но чтобы получить, чтобы вызвать эту ручку, мне нужны два параметра. Мне нужно NNN и мне нужно какой-нибудь, я не знаю, что это может быть, не знаю, имя основателя компании, фантазирую. И тогда, или телефон, да. И тогда модель не делает вызов, а она понимает, что ей не хватает одного параметра, чтобы этот вызов сделать. И она возвращает пользователю ответ, а какой телефон должен быть у этой компании. Пользователь вводит эту информацию, и дальше модель уже видит, ага, у меня есть достаточное количество параметров, я могу сделать вызов. Я правильно понимаю, что это мы описываем ситуацию, когда модель заранее не общалась с этим тулзом, с этой системой по этому API, поэтому она действительно не знает вообще, что ей может. И под моделью мы имеем в виду именно LLM модель? Да, мы имеем в виду именно LLM. И на самом деле, даже если LLM общалась, она ничего не помнит. То есть LLM никогда не может запомнить систему, с которой она общалась. Это всегда какая-то обвязка. Когда я говорю обвязка, я имею в виду отдельный код, написанный, который реализует эту логику. То есть LLM сама по себе не имеет памяти. Вот LLM, опять же, как набор параметров. Очень многие, с кем я общаюсь, думают, что… Сейчас немножко про боль свою расскажу. У нас в облаке есть много разных моделей, Яндекс.ДпТ. И мне клиенты говорят, я сейчас отправлю запрос в облако в модели Яндекс.ДпТ, она все узнает про мою компанию, и завтра Алиса будет всему, всей России рассказывать, что у меня в конвенциальных данных моей компании. Если технически посмотреть на этот весь процесс, то LLM никогда не обучается условно в реалтайме. То есть технически такое невозможно, потому что процесс обучения и то, что называется инференса, когда модель работает, они разделены и вообще никак не связаны. Начнем с того, что моделька, которая в Алисе и которая в Облаке, это две разных модельки, вообще никак не связаны, как сущности. Во-вторых, самого обучения не происходит. Модель, это реально, можно ее воспринимать как математическую функцию. Подали на вход, она через параметры прогнала, вернула результат, все. Эти параметры никак не обучаются. Скорректируйте, пожалуйста, насколько я знаю, как раз вызовы и ответы, они очень хорошо логируются всеми GPT-хами, всем остальным, и потом уже на этом происходит цикл обучения. Да, отдельный, то есть в моменте этого обучения не происходит, но на данных, на этих все-таки обучается она. Как это выглядит? логируется, опять же, не самой GPT-хой, а система рядом с GPT-хой. То есть, что делает разработчик? Разработчик смотрит, условно, на лог. Он видит, что модель получила такой запрос, она предложила вызвать такую-то функцию. В реальности эта функция была вызвана, вернула результат, модель посмотрела на результат, предложила еще одну функцию вызвать. У нас получается, называется, trace агента, цепочка того, что произошло. И дальше, конечно, модель может ошибиться и вызвать не ту функцию, не с теми параметрами. И как раз под эту задачу модель дообучается. То есть она дообучается не под конкретно вызывать ООО «Ромашка», она учится навыку правильно определять и вызывать нужные функции. Это действительно, это дообучение происходит, но оно происходит не на самих данных, а на вот этих навыках решать конкретную задачу. Возвращаясь к нашему примеру с MCP, модель поняла, окей, у меня есть такие тулы, я их вызываю, сохраняю результат. И дальше очень важная вещь, которая тоже сильно помогает внедрению MCP. У них есть возможность сделать подтверждение. То есть есть два варианта использования MCP. Когда модель автоматически делает, ну не модель, давайте я буду говорить модель, но всегда подразумеваю, что это не совсем модель, а это код рядом. Что когда модель вызывает внешний инструмент сама, либо пользователь может подтвердить. То есть хороший дизайн будет выглядеть следующим образом. Модель поняла, что ей нужно вызовет какой-то инструмент, и дальше говорит пользователю, хотите, чтобы я вызывала функцию получения NNN с такими-то параметрами. Я их поняла из диалога, и давайте я их вызову с такими-то параметрами. Пользователь кликает «Да, вызови» или говорит «Нет, не тот инструмент, давай другой». Дальше происходит вызов, и получается результат. То есть таким образом происходит базовое взаимодействие с внешними системами. Это, наверное, первый главный инструмент, который нам нужно запомнить, когда мы говорим про агентские системы. Это LLM плюс MCP плюс внешние API, с которыми модель может взаимодействовать. Это первый сценарий, первый инструмент. Можно вопрос? Да. Хотелось бы чуть подробнее понять, в чем отличие MCP от, допустим, тулов, привязанных к агенту. На нашем примере мы работаем с Nathan, N8N. МСПшку из Битрикс. МСПшку мы не делали, потому что Битрикс анонсировал, что у них будет свой МСП. Пока просто ждем. Мы сделали саб-сценарии, которые подвязали к агенту как инструменты. И фактически они описаны универсально для любого агента, которого мы создаем. С точки зрения поиска, создания, сделок, контактов и так далее. И вот в чем здесь будет принципиальное отличие? Это же фактически мы сделали тот же самый МСП, просто отдельными саб-сценариями. Правильно? Абсолютно верно. Это то самое, про что я говорил. Вы описываете как раз тот сценарий MCP, как он есть. Да, то есть у Bittrex есть афишка, вы его обернули в MCP, дальше вы можете… Ну, это все синонимы. Тулы, функции, инструменты – это все синонимы, по сути, для вот этого вызова инструмента моделью. Абсолютно. То есть разницы нет, типа MCP отдельно создать, либо вот инструментами просто с абсценариями описанными. На самом деле, скорее всего, в Nathan'е под капотом используется MCP для этого взаимодействия. На этом это low-no-code обертка вокруг, а если бы вы писали то же самое технически, вы бы пошли делать MCP и дергали бы MCP. То есть это просто уровень абстракции выше для конечного пользователя. Технически это как раз на этой технологии. Спасибо. Соответственно, разобрались с инструментами. Второй популярный сценарий, который тоже является инструментом, это работа чисто с документами. То есть не с API, а с документом. И здесь есть свои нюансы, здесь есть подход RAC, Retrieval Agment Generation. Про него Настя расскажет подробнее. То есть про MCP, который построен на Function Calling и фактически является более стандартизованным Function Calling, мы послушали как способ модели преодолеть какие-то ограничения и выйти во внешний мир. Теперь мы поговорим про то, как бороться с галлюцинациями, как актуализировать знания компании, и на настоящий момент, и по тем вещам, которые она в принципе не могла подчеркнуть из общедоступных источников. И здесь появляется такой подход, как RAC. Он актуален уже последние года два, а может быть и больше. Это Retrieval Augmented Generation, как это работает. Модели помогают, дают на вход определенные знания. Ой, плохо видно. определенные знания, основываясь на которых она генерирует ответ. Реализация наивного рага выглядит следующим образом. Вы берете свои документы, по которым хотите, чтобы модель могла отвечать. Это могут быть какие-то внутренние инструкции, это могут быть вики какие-то. Допустим, вы строите, представим, лучше какую-то задачу. Вы делаете агента, который работает и помогает сотрудникам ориентироваться по вашему огромному, допустим, Вики конфлиенсу. У вас есть компания, и вы помогаете сотруднику искать ответы на вопросы. Что можно сделать? Можно перевести всю документацию, всю ту текстовую информацию, и не только текстовую на самом деле, которая у вас есть, векторное представление. Есть такие специальные модели, эмбейдеры, которые позволяют преобразовать текст в векторное представление, чтобы в последствии сравнивать вектора через расстояние, определять, насколько разные тексты близки между собой. И таким образом можно построить индекс, который будет описывать, что вот такой-то кусок текста, векторное его представление такое, это может содержать какие-то метаданные. Когда вам поступает какой-то запрос, вы его преобразуете вектор на основе этой же модели, которая участвовала в формировании индекса, и потом с помощью векторного сравнения, с помощью определения расстояний ищите, что максимально похоже на запрос. Вытаскиваете это, преобразуете к тексту и даете модели в качестве контекста, который она будет оперировать при ответе на вопрос. Допустим, повторюсь, это могут быть те знания, которые никогда не могли попасть в открытый доступ в интернет, но навигироваться по ним, как-то с ними взаимодействовать нужно. Это могут быть те знания, которые, собственно, в интернет попали, но после того, как модель на этом добучилась. Ну и в принципе для того, чтобы снизить риск галлюцинации, замеса какой-то информации, которую вы бы не хотели получать, вы можете подкормить модель наверняка теми данными, по которым вам нужно строить ответ. Едем дальше. Короткую добавку. Я просто смотрю, вот сижу на этот слайд, и тут написано в синем блоке, в большом database. Вот тут важно помнить, что когда мы говорим про RAC, мы говорим про не структурированные данные, а именно про текст. То есть пробовать натягивать RAC на сценарии, где у нас SQL-базы, то есть структурированные таблички, как правило, это работает плохо. То есть если мы под database понимаем условное, не знаю, постгря или что-то, где лежат цифры, строки и так далее, то лучше использовать подход с MCP, то есть когда мы делаем поверх этой базы какую-то опишку и взаимодействуем с ней, либо через генерацию кода, если как-то получится такое вдруг согласовать с безопасниками. Датабейс именно текстовая, не любая. Это важно помнить. Да, мы там храним представление. Можно на самом деле вернуться даже на шаг назад. Была какая-то умная мысль, сейчас я попытаюсь ее вспомнить. То есть на самом деле это может быть не только текст, исходно, что было в текстовых документах, но и это может быть, например, аудио, видео, которые вы каким-то образом расшифровали, преобразовали в текстовое представление, и в том числе поэтому можете искать. Дальше. Теперь поговорим про то, что люди часто путают RAC и Funtunic. То есть рак не предполагает обучение, фонтюнинг предполагает. Рак достаточно быстр и хорош в скорости реализации. Вы можете быстро подкладывать разные документы и тем самым обеспечивать себе актуальные ответы на интересующие вопросы. Фонтюнинг предполагает обучение, и тут есть определенного рода нюансы. Потому что разные люди вкладывают в фонтюнинг. Есть как минимум история с Лорой, когда мы учим модель на определенном пласте, определенном датасете отвечать определенным образом. И при этом никак не вмешиваемся в сферу ее знаний. Если говорить про обучение более глобально, чтобы знания проросли именно в подкорку модели, есть такой термин как предобучение. И обычно под этим фонтюнгом предобучения это совершенно разные вещи. В широком смысле файтюнинг – это коррекция модели, чтобы она отвечала на такие запросы определенным образом. Это больше про подачу, нежели какие-либо знания. То есть при файтюнинге, как это понимает большинство из Data Science, вы не добавляете каких-то новых знаний. Вы просто корректируете то, как модель работает с определенными запросами. формат, подачу, то, что из этих запросов нужно вытаскивать. Дим, тебя, наверное, больше всех спрашивают про это. Может быть, у тебя есть что добавить? Да, я, наверное, попрошу. Просто там подсвечивали до лекции, что такая важная тема, которую стоит подсветить. Кому глобально понятно, в чем технические различия между рагом и файн-тюнингом? Поднимите просто руки. Так, не всем. Давайте попробую еще раз коротко объяснить. Когда мы говорим про дообучение, я уже несколько раз повторял, что модель — это функция. Это математическая функция. Что такое функция? Это какое-то что-то умножается на что-то, прибавляется что-то. Вот так много-много-много раз происходит. Когда мы говорим про дообучение, это значит, что вот эти сами параметры функции, вот эти коэффициенты, они меняются. То есть, по сути, меняется модель сама по себе. Дальше можно говорить о том, меняется ли вся модель или маленький кусочек. Это уже разные подходы к обучению. Но глобальный смысл в том, что меняется модель. Это такой подход. Он был популярен, скажем так, год, полтора делать такие до обучения. И по сути такое делали скорее крупные компании, когда они хотели добиться максимальной оптимизации. То есть что делали? Брали маленькую модельку, небольшую, огромную квен, маленькую модельку, дообучали ее точечно под задачу, и она как быстро и хорошо решала конкретную задачу. Но, чтобы сделать дообучение, нужно данных. Данных много, и что самое важное, данных в определенном формате. То есть недостаточно в модель закинуть просто какой-то текст и сказать «дообучайся». Дообучение происходит не так. Дообучение происходит на парах. Вот такой вопрос, вот такой эталонный ответ. И вот представьте себе, чтобы собрать вот такие вот вопросы, эталонные ответы, это надо прям хорошенько вложиться. Нужен человек, который это будет делать, кто это разметит, кто проверит. И таких эталонных вопросов ответа должно быть сотни тысячи, чтобы дообучение было успешным. И поэтому понятно, что многие компании по этому пути не хотят идти, не идут. И говорят, мы сейчас не будем смотреть настолько сильно на эффективность. Мы возьмем просто модельку побольше, но которая будет сразу из коробки мне хорошо справляться без этого дообучения. И используют подход RAC. В чем смысл RAC? RAC это что-то, что стоит рядом с моделью, какая-то база с текстами, по которой происходит поиск. Мы поискали, нашли и дальше мы передаем вот этот найденный текст в модель в качестве промта. То есть мы просто говорим, вот модель, ты помощник, вот отвечай по этим кусочкам текста, по этим документам. Сама модель никак не меняется. Это просто штука, такой же инструмент, как мы говорили до этого. Инструмент, который стоит рядом, что-то поискали, нашли какую-то информацию, передали в модель, и модель по этой информации отвечает. То есть нам не нужно иметь модель, это сильно дешевле. Да, встает вопрос, окей, нам нужна какая-нибудь векторная база рядом, нам нужно сделать поиск, но это, скажем так, в простом случае задача быстрая и легко решаемая, все умеют собирать наивные раги. Опять же, начинаются проблемы, когда мы говорим не про наивные раги, а про качество. А как сделать этот раг хорошим? Про это Настя дальше подробнее поговорит. Можно вернуться обратно? Я все-таки хочу сказать, почему сначала был на сухую файн-тюнинг, потом пришел рак и остался на самом деле. Давайте так. Отрасль очень интенсивно, быстро развивается. И вначале у нас было какое-то ограниченное число моделей. И не было такой возможности перескакивать с одной на другую. И приходилось работать с тем, что есть. Приходилось файн-тюнить, пытаться как-то довести до ума то, что дают. Сейчас же, когда мы говорим, если у нас модель одна не устраивает, если не устраивают ее данные, мы можем подкормить его рагом, как минимум, попытаться попромтить, и модели все больше развиваются, они становятся все более умными, получают при обучении все больше примеров задач, лучше справляются, узнают все больше текстов. И, по сути, вот то, что раньше обходилось файн-тюнингом, и есть какая-то модель, она плохо отвечает, надо с ней что-то сделать, давайте файн-тюнить. А сейчас это больше удел того, чтобы подкрутить промт и прикрутить правильно рак. На самом деле, когда мы говорим рак, это вот первый слайд, наивный подход и так далее, это очень наивный подход. Индустрия, повторюсь, только формируется и сейчас для того, чтобы рак получился максимально хорошим, чтобы ответы были классными, необходимо приложить усилия. Изменится ли это в дальнейшем в Дим-переходе? Ну, конечно, изменится, но на настоящий момент придется поднапрячься. Можно вопрос. Если рак прикручиваем, для примера, какой объем в терабайтах будет этот рак, если их несколько, тогда сколько надо инфраструктуры, чтобы держать? На самом деле, отвечая конкретно за E-Studio, у нас получается следующая история, что у нас есть лимит на 10 тысяч документов при построении индекса, и каждый файл у нас по 128 мегабайт. Но это расширяемо. А если он премис? На самом деле, скажем так, в самом технологическом подходе жесткого ограничения на размер документов нет. Почему? Потому что эти документы лежат в самой базе, и они лежат в виде вот таких кусочков. То есть огромные документы, там, не знаю, 10 тысяч, они нарезаются кусочки. Допустим, полтора миллиона чанок мы нарезали. И дальше полтора миллиона, три миллиона. На качество поиска это может влиять, но не драматично. То есть происходит сначала поиск по этим полтора миллионам, находятся нужные фрагменты текста, они передаются в модель. Может быть сложность с тем, насколько поиск вообще хорошо работает. То есть это уже скорее вопрос не к самой модели, а вот как этот поиск отрабатывает хорошо, быстро, находит ли он нужные фрагменты. И вот для этого как раз есть слайд про измерение качества поиска. Мы не будем вдаваться в эти метрики, это просто показать, что этих метрик может быть разных. Но когда строится рак, строится эта векторная база, мы учимся по ней искать, и дальше обязательно замеряем разные метрики, насколько вообще хорошо мы ищем, чтобы потом уже смотреть, насколько хорошо модель отвечает. И вот отвечая на вопрос, как это будет выглядеть в реальности, ну, предположим, вот у вас есть вся база, вы ее загрузили, померили метрики. Метрики хорошие? Замечательно. Метрики плохие? Дальше надо пытаться с этой базой работать. Проблем может быть много. Сложные документы плохо распарсились, плохо легли. Либо просто все документы из разных тематик, давайте попробуем их разбить на несколько баз с меньшими тематиками. Поэтому это зависит очень сильно от конкретного сценария данных и сложности. Вот тут я еще хочу предостеречь, чтобы не пихать всю имеющуюся документацию, все подряд в один индекс, потому что когда garbage in, garbage out, поэтому, строя какой-то индекс, старайтесь, чтобы там были плюс-минус относящиеся друг к другу данные, чтобы это не превращалось в свалку. Понятно, что если требуются терабайты того, чтобы индекс поместился, но главное, чтобы это были релевантные терабайты, скажем так, а не просто свалили все в кучу. Да, еще, наверное, такая немножко байка в сторону. Сама история с поиском сейчас вокруг агентов кажется, что-то такое новое, раги, раги, раги. Но на самом деле, если так честно взять и посмотреть историю, Поиск — это та задача, которую почти все компании решают уже много десятков лет. По сути, бизнес Яндекса основной вырос на поиске. Яндекс делает поисковик с самого своего начала, и по сути это тоже самый рак. Но только поменялись подходы. Раньше сделать качественный поиск до LLM, это была очень сложная инженерная техническая задача, потому что нельзя было взять модельку, закинуть в нее текст, получить вектор и сразу по нему искать. Поэтому было очень много разных ухищрений. Сейчас за счет того, что это произошла условно демократизация поиска, теперь достаточно, там вот появились готовые инструменты типа Нейтана, появились имбеды, кажется, что можно закинуть документы, поставить векторов, все, поиск готов, в общем-то, Яндекс на свалку. Ну, глобальный поиск. Но, но, но, но, за этим, как бы, кажется сейчас сложной, простотой, есть много нюансов, когда мы начинаем мерить метрики, смотреть на качество. Наивные подходы, ну вот, у меня, сейчас, одну секунду, Когда все это только начиналось, я общаюсь с клиентами, все собирают рак, как я показывал выше, и почему-то с кем не говоришь, померили качество, 65-70% отвечает отлично. Вот 30% вообще никак не получатся добиться, 65-70% какая-то была такая загадочная цифра, 70%. И вот это доведение от 70% до 90% условно 5-8% это то, куда тратится больше всего времени, где начинаются самые большие сложности. И это уже дальше условно-управленческое решение продуктовое. А готов ли я выводить на клиента решение, где у меня точно 70% или нет? Если я говорю про какого-нибудь помощника по общей информации, почему бы нет? Если я говорю про то, что от этого повлияет моя какая-то репутация, не позволит никто выводить помощника 70%. На самом деле тут еще хочу сказать… Вопрос. А, да, извини. Когда ты внедряешь RAC или вот эту штуку, как тебе оценить? Сколько ты потратишь на это времени, на одно или на другое? Потому что там сесть и, не знаю, 100 человек померится, клиентский сервис, например, 2 миллиона обращений в месяц или в неделю, и разметить какие-то самые популярные, ты 100 человек быстро решишь. А вот RAC, это сколько времени? Да, если говорить про внедрение, часто RAC строится с нуля, но часто RAC используется скорее как замена чат-ботов, где есть какое-то диалоговое дерево. То есть, условно, есть оператор, у него есть ответ, который дается диалоговым деревом. И что компании делают? Они пробуют заменить вот эти части вот этого всего дерева диалогов на раги. Ну, то есть, как бы маленький кусочек, конкретную тематику. И дальше этот раг выдается не на конечного пользователя, а он отдается оператору. То есть, например, мы сделали оператору подсказку, и он, оператор, выступает как оценщик. То есть, он может кликнуть хорошо-плохо, он может доразметить, он может исправить. Таким образом, он дает обратный сигнал, насколько этот раг хороший. И дальше уже команда инженеров анализирует, где была проблема, поиск не отработал, ЛМК не отработал, еще что-то не случилось. И дальше идет вот этот анализ. Опять же, сказать точно, что это месяц или три месяца достаточно сложно. Простые кейсы действительно можно собрать условно до хорошего качества за месяц. Истории по отчетикам каким-нибудь инвестиционным, они сейчас понятно легко решаются. Какие-то сложные корпоративные поиски с кучами пас, это могут быть проекты на полгода, на год и дальше. Тут будет сильно зависеть от задачи. Вы столкнетесь с доступами, необходимо будет приложить какие-то усилия на самом деле. Вы же будете ориентироваться не только на текстовое правдоподобие. Я буду рассказывать немножко дальше, но суть в том, что вы, наверное, захотите опираться на то, когда была обновлена эта информация. Может быть, она обновлена 20 лет назад, и туда лучше не лезть. Или она обновлена вчера, наверное, это что-то более актуально. Если это актуально для вашей компании. Но существует еще много разных причин. Поэтому пока качество поиска, оно больше ложится на клиента, к сожалению, сейчас. Придем ли мы к чему-то более чернокоробочному, придем, но постепенно. Виктор? Да. Мы говорим, что РАК – это поиск по текстам документов компании. а документы могут быть связаны очень тесно с помощью гипертекста, внутренних ссылок. Учитывает ли рак наивный, вот эти все связи внутренние, или для этого используются какие-то другие технологии? Замечательный вопрос. Это как раз подводка к переходу от наивного рака к сложному раку. Потому что, когда мы говорим про рак, большая часть времени и сложности при его сборке падает как раз на подготовку документов. Если взять наивный рак, наивный подход, загуглите, пьяных себе пещете, что такое рак, он вам скажет. Берете текст, бьете на фрагменты, и по нему ищите. Понятно, что в таком подходе никакой перекрестной ссылки, ничего не учитывается. И вот буквально в прошлой неделе со мной компанией общался, они потратили, группа ребят R&D, то есть те, которые занимаются NLP, они потратили 4 месяца по промышленной, технической документации, чтобы так подготовить вот эти чанки, чтобы они учитывали перекрестные ссылки. То есть идет текст какой-то технический, там есть в квадратных скобочках какая-то ссылка, и эта ссылка в самом конце документа расшифровывается. А она критически важна, чтобы ответить нормально. Они писали код, который берет эту ссылку, добавляет ее в чанг, и только после того, как они сделали предобработку, у них получилось значительно повысить качество для одного конкретного формата документов. Представьте в промке, сколько может быть разных форматов с разной сложностью, И вот вся сложность, она именно падает в эту предобработку, чтобы эти данные хорошо положить. Мы начинали с того, что данные были в центре, и это все еще действительно остается в центре. Если мы говорим про раги, это такая большая сложность. Давай немножко, смотря что с чем сравнивать, что долго, что недолго. Допустим, если у вас есть штат операторов, вы в банк, и у вас там сидят операторы на аутсорсе, они, соответственно, требуют какого-то контроля, обучения и так далее. И, конечно, вот те затраты, которые вы потратили на реализацию рага, окупятся вам, скорее всего, гораздо лучше, чем то, что вы будете обучать каждого оператора поиску какими-то технологиями, понимание структуры хранения ваших документов. То есть автоматизация, она может быть сложна на первых этапах, но в дальнейшем она, конечно, окупается. То есть мы все равно должны сравнивать сложность с тем, что мы складываем в те же процессы, когда строим их на людях. То есть если теперь, а, вот можно на предыдущий слайд вернуться, я хочу там вот маленькую прокомментировать метрику, это впечатление пользователя. Ну то есть вот есть какие-то релевантности, точности, полнота и так далее, все про это плюс-минус какое-то представление имеют, но это характеристики поиска, могут там интуитивно ответить. Но играет еще роль впечатление пользователя. Допустим, вот как Дима говорил, там выводить с качеством 70% или не выводить. Качество это тоже вещи относительно. Возможно, ваш сервис такой, что там документация построена так, что самые релевантные и хорошие документы будут максимально запутаны и непонятны. А может быть, что-то относительно дурацкое лучше будет считываться людьми и в конечном итоге лучше даст ответы на их вопросы. Конечно, есть метрики, и они, наверное, основополагающие, и в большинстве случаев хорошо работают, но кейсы, повторюсь, индивидуальны. Нужно смотреть, как это все воспринимается. К тому же, например, вы можете это компенсировать как-то UX использование. То есть предыдущие вопросы, что искали похожие пользователи и так далее. То есть ваша конечная цель-то не поиск сделать идеальным, а ваша конечная цель помочь пользователю. Это очень похоже на то, как работать, знаете, голосовые боты. Вот вера у вас никогда не будет идеальной, но услышать своего пользователя, помочь вы ему все-таки сможете. А как замеряете впечатление пользователя? Каким методом, какими метриками? Ну, тут скорее мы отдаем эту науку пользователю. Это индивидуально. Это как раз то волевое решение, про которое говорил Дима. То есть внутренние какие-то показатели, перевод, не знаю, в Е2Е метрики, которые у клиента есть. Дим, у тебя есть хорошие примеры? Ну, наверное, самый частый, самый простой способ, типа лайк-дизлайк. Просто в интерфейсе показывается, я не знаю, может, робота Макс в госуслугах пользовались, Когда робот Макс дает ответ, он говорит лайк или дизлайк. Это в моменте, да, ответ? В моменте. А вот такие долгосрочные, когда вот прошел нас квартал. Господа пользователи нашего вот этого всего. Да, ну давайте немножко расскажу, как это выглядит, например, в Алисе. Приоткроем кухню Алисы. По сути, есть прям отдельная выделенная команда асессоров. Это внешние люди, которым пишется инструкция, как оценивать качество. То есть, как это выглядит? Прогоняется, летает запрос в модельку, моделька возвращает один ответ, ну и какая-нибудь другая моделька или еще что-то возвращает другой ответ. Им можно, например, попарно сравнить и сказать, какой ответ лучше. Это один способ оценки. Side-by-side, да. Две модельки. Реальный человек кликает, это лучше или это хуже. И ему за это платят деньги. Прям отдельные люди платят деньги за то, чтобы говорить, это лучше, это хуже. Либо же, например, просто, если мы говорим про дообучение, моделька дает какой-то ответ, и дальше есть человек, который делает факт-чекинг, проверяет, что ответ верный, который, например, смотрит на стилистику. То есть у Алисы, как у B2C решения, к ней есть требования. Вы знаете, сейчас все любят, чтобы LN-ки были эмпатичные, уважительные, вот такие вещи. И это прямо закладывается в стиль, который затем человеком исправляется, на котором модель потом будет дообучаться. Если мы говорим не про Алису, а Алиса такая общая болталка, но если мы говорим про условно те же госуслуги, у них есть более сложная задача. Для примера, оказывается, если вы оформляете паспорт Калининградской области, за гранд паспорт, то у него там другая цена, чем если бы вы оформляли в Москве, что-то такое. В общем, какой-то есть нюанс, касающийся Калининградской области. Общий ассессор, который есть в Алисе, не сможет это ответить, потому что он просто этого не знает. То есть нужно нанимать или внутри выделять отдельного человека, специалиста, который понимает специфику предменной области, может сесть и оценить эти ответы. Экспертно. Экспертно, да. То есть, опять же, в зависимости от сценария, тут может потребоваться очень разная сложность погружения именно оценки качества. Если это просто помощник по внутренней вики, то, наверное, ошибется. Сотрудник придет, пожалуется, скажет, он мне наврал, хорошо, мы заберем и справим. Если это условные госуслуги, то требования другие. Еще хотел сказать парадельно про лайк-дизлайк. Это все равно немножко мусорная метрика, потому что люди скорее вам поставят дизлайк, если им что-то не понравится, чем лайк, если все хорошо. Это похоже на врачей, когда все хорошо, к ним не ходят, не обращаются, никак не сигнализируют. А вот когда что-то плохо, вот такой черепикинг будет происходить, у вас будет немножко смещенная выборка, но это хорошие сигналы. У нас остается 5 минут до первого кофебрейка. Давай, наверное, поговорим. А вот еще вопрос был? Да, я хотел спросить, если RAC это продуманная система, то, скорее всего, что уже скопились какие-то знания по разновидностям вот этих систем. Вы для себя делали какой-то каталог, что вот для вот этого такая система, складывание данных для этого вот такая, такая? То есть есть ли уже у вас вот такой опыт? Тут, наверное, не системы, а условно архитектурные подходы, которые нужны. Для меня это одно и то же. Да, есть действительно архитектурные подходы. Если говорить конкретно про нас, у нас есть отдельная команда, которая занимается внедрениями и в зависимости от сценария формирует эту экспертизу. Там лучше строить рак и строить его таким способом. А тут лучше строить 5 агентов, у которого по 3 инструмента и делать это другим образом. То есть это отдельная команда, которая делает эту экспертизу. И дальше уже самые такие частые паттерны архитектуры мы закладываем в продукт, чтобы это было проще. То есть если по-простому идем, покупаем каталог сценариев, если так цивилизованно, и мы понимаем, что мы экономим время, потому что сами эти сценарии будем 10 лет изобретать. Мы больше про кубики, нежели… Ну давайте про более сложный рак. Как это все реализуется, если переходить от наивного к реальным реализациям? Запрос пользователя может не попадать сразу в поиск в том виде, в котором он был задан. Например, с теми же госуслугами вы общались, и в конце вам приходит запрос, у меня паспорт в Калининграде и так далее, а как это сделать? А как это сделать, вы искать не будете. Вам необходимо просумморизировать всю историю общения, понять, к какому куску относится последний запрос, и искать уже именно его. Возможно, вам придется еще поработать с трансформацией запроса. Например, у вас используется одна терминология при общении агента, а в базе знаний она другая, и может быть не очевидны ЛЛМки, может быть вы переживаете, что там что-то может поплыть. Сделать какие-то автозамены, то есть преобразовать запрос пользователя, чтобы он был более понятен. Может быть, нагенерить каких-то дополнительных вопросов. Это тоже такая технология, которая позволяет улучшить качество конечного результата. Потом вы переходите к query, понимаете, а в какой индекс-то вам, собственно, обращаться. Это может быть продиктовано разными причинами. Например, метаинформацию по пользователю, какой-то предварительной классификации, чтобы не обходить все возможные индексы, которых у вас может быть очень много. Вы большая компания, и у вас там индекс поддержки, индекс с кодом, еще какой-то индекс. Нет, вы себе сразу сужаете задачу и что-то пытаетесь оптимизировать на этих этапах. Например, еще можете при построении индексов не переводить абсолютно все в текстовое представление. Возможно, вам нужна некая суммаризация, потому что огромные талмуды текста, даже переданы в ЛМК, вам не то чтобы сильно улучшить положение. То есть, проектируя конкретную архитектуру, вы идете по пути оптимизации, вставляете себе вспомогательные кубики, особенно если вам хочется ускорить процесс получения. Потом, когда получаете результаты условно со всех индексов, полученные, например, где-то полнотекстовым поиском. Полнотекст тоже, кстати, важная история, потому что если речь идет про какие-то конкретные названия, артикулы, то, что плохо ищется в близком векторном пространстве, вам придется делать некие гипридные подходы. И потом это все джойнить с помощью Fusion Retrieval, какой-то логикой, делать реранкинг результатов, допустим, по метапризнакам. Вот с этого индекса у нас получились какие-то результаты, но он обновлен, не знаю, два года назад. А это что-то более актуальное. То есть придется, скажем так, с этим работать, если бы вы работали с продуктом, который делаете для себя. И в конце концов только тогда вы подаете это в контекст. Перед этим может быть подвергнуть постпроцессингу, убрав дубликаты, сделать какие-то преобразования, может быть выполнить даже суммаризацию. То есть на практике рак может превратиться в такое. Часто ли это случается? Ну, нет. Используются ли все подходы, которые я только что писала? Ну, тоже нет. Каждый раз вы выбираете тот кирпичик, который актуален и необходим только вам. Да, я просто хотел добавить, что, наверное, фьюжн, ретривол, реранкинг, это все звучит немножко страшно. Вот, как бы основная идея вот этого слайда в том, что подходов архитектур в публикациях везде к рагу очень много. Нету эталонной идеальной архитектуры сложного рага. Если вы почитаете именно научные публикации, то там есть вообще космолеты, которые по метрикам одной узкой конкретной задачи решаются идеально. Потом выводите в прот, и оказывается, нужно такое количество ресурсов под три базы, под действием обращивания Лэмбанг, что никакая экономика под этим рагом не сходится. То, что заводится суперсложное в публикациях, далеко не всегда доходит до прода. Какая основная идея? То, что это всегда баланс. Баланс между качеством и между сложностью, стоимостью. Не надо бежать строить сразу сложные архитектуры. Лучше начинать с наивного рага с тех 70% и дальше итеративно, постепенно улучшать опираясь на метрики, показатели точности и так далее, а не сразу городить, вот не знаю, какое-то время были суперповлярные графраги. Все-таки вот графраг, графраг, мы сейчас делаем графраг. Потом оказалось, что этот графраг столько контекста сжирает у модели, так дорого на один запрос, что проще человека нанять, который будет отвечать, чем использовать графраг. Ну и не забывайте, что вы можете в принципе не использовать пайпрага, а скормить какую-то, если у вас небольшая статья, прямо в контекст модели, по которому она ответит. токенов съестся, может быть, чуть больше, но, может быть, это будет более эффективно у вас на вашем пайпе. Да, у нас сегодня будет такая практическая часть, где можно тот же рак попробовать без кода, без всего, собрать самостоятельно на базе наших сервисов. И так как у нас через уже 3 минуты назад началась кофе-пауза, перед тем, как мы на нее уйдем, хотели показать просто небольшое демо, что вы… сейчас не сильно… а работает, да. Что оно будет… как оно будет выглядеть сегодня на практике. Просто в каком месте iStudio, если вы это будете собирать, можно попробовать сделать рак. То есть у нас в сентябре появился отдельный агентский слой, где можно собирать агентов. И по сути тут, скажем так, реализован не наивный рак, но не очень далекий от наивного. Мы видим, что самая большая сложность у клиентов, когда они работают с раком, это форматы данных. Это как раз был вопрос, крестные ссылки и так далее. И мы очень сильно вкладываемся в качество парсинга. То есть, например, вот такие вот схемы, таблицы в документах, еще что-то, это нельзя просто так порезать чанг и сложить. Это надо специально предобработать. И на самом деле, когда вы подгружаете в iStudio вот такой документ, он проходит через несколько моделек. Он сначала проходит через SCR, чтобы достать информацию. Дальше через классификатор, чтобы найти картинки. Дальше эти картинки улетают в визуальную LLM модельку, чтобы их обработать, достать информацию. Дальше это все нарезается на фрагменты, дальше эти фрагменты обогащаются языковой моделью, и, наконец, все это складывается в векторную базу данных. То есть там, чтобы обработать один такой документ, какой-нибудь инвестиционный отчет, это на самом деле большой сложный пайплайн, в котором куча моделек. А теперь представьте, один документ на 30 страниц, в нем может быть, давайте возьмем, для примера, 20 изображений. То есть нам одновременно нужно отправить в языковую модель 20 изображений. Мне кажется, с одной стороны, не очень много. С другой стороны, мы предоставляем сервис для всех пользователей. Пользователей много параллельных. И в пике может быть такое, что загрузили 10 документов. Это уже 20x10, то есть 200. А можно вопрос? Да. А был опыт использования RAC для кодовой базы именно? Хороший вопрос. Именно в AI-студию мы не очень сейчас пока идем в сторону кода. То есть у нас есть отдельное решение Sourcecraft для работы именно с кодовыми сценариями. Эксперименты мы с кодом проводили, но, скажем так, мы пока под них не затачиваем специально RAC. У нас скорее такой общий, ну, для скорее, ну, не кодовых сценариев. Нет специализации, так скажем. Да, без специализации на коде, спасибо. Но глобально это выглядит следующим образом. Для пользователя это просто загрузка документов в интерфейсе, и дальше, когда пользователь их подгрузил, он уже создает агента. И вот это самое интересное, про что мы говорили, да? То есть вот тут видны хорошо те компоненты, про которые мы говорили на агенте. Модель. Мы выбираем модель, которая является основой этого агента. Дальше мы выбираем… Температуру вчера говорили, как бы не так важно. Дальше инструкция. Это тот самый промт, та инструкция, где мы описываем, что модель должна делать. И здесь у нас RAC выступает инструментом. То есть мы должны модели в инструкции сказать, что для того, чтобы найти информацию, отвечай не по своим знаниям, а сделай вызов внешнего инструмента, сделай вызов инструмента поиска и отвечай только по той информации, которую ты найдешь. Это как раз прописывается в инструкции языковой модели. У нас очень часто вопросы приходят, люди заходят в интерфейс, начинают пользоваться, просто выбрали модельку, подгрузили документы, модель ничего не отвечает. Ваш поиск плохо работает. Он работает плохо потому, что у нас, во-первых, инструмент для разработчиков, то есть специально, чтобы можно было собирать вот эти сложные агентские системы. И дальше мы прописываем, что она доступна через инструмент поиска. вот как раз то, что был вопрос про, по сути, MCP. Вот тут точно так же происходит запрос, то есть модель понимает, что ей нужно вызвать раг, делает вызов рага, в раге находит вот эту информацию, и дальше, ну там, не знаю, какой тариф за перевод цифровых рублей. Просто в примере показывал, что вот эта информация, она в табличке, то есть табличка распарсилась, сложилась, и дальше вот он отвечает 15 рублей за один перевод. Это такой супер простой базовый сценарий, болталки, но таких индексов может быть много. Кроме поискового индекса, здесь может быть еще поход в MCP собрать другую информацию. Все это ложится в контекст и дальше по всему этому модель отвечает. Это такой первый шаг подготовки вызова инструмента. О других инструментах и о дальнейших сложностях мы уже поговорим после кофе-паузы. Да, я хотела дополнить, что в AI-студии, конечно же, не только вот такой UI-интерфейс есть, все то же самое, вы можете делать из кода, то есть построили индекс, он у вас не жестко фиксирован, вы можете его обновлять. Допустим, что-то случилось, какая-то ситуация, нужно добавить срочную инструкцию, которую должны сотрудники получить, вы точно так же легко обновляете индекс, все максимально динамично, ну скажем так, обновляется все максимально живое. А это скорее для прототипа, для визуализации, для того, чтобы понять, как что работает и немножко отладится. Да, после паузы поговорим про еще один рак, но теперь у нас уже индексом будет не просто поиск, а целый интернет. Это вот такая приманка, чтобы было желание вернуться. Давайте в 12.35 тогда, да? 12.35 собираемся здесь. Ой, в 20, нет, не 25, в 12.25, извините. 12.25. Прошу прощения. 12.25 собираемся здесь, продолжаем.