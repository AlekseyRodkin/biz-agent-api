ご清聴ありがとうございました Так, уважаемые участники, давайте продолжать. У нас, и мы так немножко даже уже задержались. Смотрите, мы с вами от общего к частному, ну то есть, или так, от абстрактного к конкретному, значит, будем двигаться, потому что следующая часть, Дмитрий Гуреев, он у нас был на прошлом модуле, если вы помните, у нас был прошлый модуль. И на чем мы остановились, кстати? Да, ты сейчас спрашиваешь, на чем мы... Или ты меня спрашиваешь, на чем мы... Я тебя спрашиваю, да. А ты себя спрашиваешь, на чем мы... Я и себя, и тебя спрашиваю. Да, значит, смотрите, вы помните, мы с ним как раз брали аннотацию когнитивных операций, которые нужно преобразовывать и так далее. Мы делаем сейчас вторую часть, она будет связана с той, как сказать, теоретическим блоком, который был у Дмитрия Сушникова, но мы уйдем сейчас здесь совсем-совсем в конкретику с точки зрения применения. И еще все это дело руками в упражнениях попробуем поприменять и посравнивать. Самое важное, хочется, чтобы у вас в опыте появилось сравнение нескольких типов решения задач с помощью разных алгоритмов. Вот в этом будет наша задачка. И мы сегодня, надеюсь, добежим и какой-то прототип тоже попробуем сделать. Будем приближаться. Я думаю, что у нас временно полтора часа. Ну, давай посмотрим. Может быть, мы его просто на завтра перенесем. На что успеем, да. Да, поехали. Да, всем добрый вечер. Уже конец дня. Игру же мы точно сделаем. Игру мы точно успеем. Первую мы сделаем. Первую игру точно успеем. А по поводу HR посмотрим. Давайте так, вспомним, на чем мы остановились. Или, наверное, что даже осталось в голове по поводу истории цифровых артефактов. Для чего нам все это надо? Давайте вспомним. Для чего мы это делаем вообще? Это топливо для еды. Хорошо. Прикольно. Так, а действительно, для чего мы вообще делаем вот это упражнение на разбивку на эти артефакты? Просто вспомним, обновим, чтобы могли пойти дальше. А, еще раз вопрос. Сначала вопрос. Хорошо. У нас было два шага. В первом модуле я рассказал про то, что у нас есть понятие когнитивных операций. Помните, такое было? И у нас даже были вот такие кадры. Мы там с вами вместе разбирали. Кто-то плохо делает. Вот такие операции, помните? Потом у нас был с вами шаг, мы его даже в самом начале обозначили, сейчас часто его говорим, что за каждым процессом, в который вы входите, есть набор цифровых артефактов, он начинает, с которым он заканчивается. Правильно, да? Для чего мы вообще делаем такой шаг? Почему мы смотрим именно через артефакты? Чтобы определить объект, которому применить действие. Правильно. Алексей, вы там добавить хотите? Примерно то же самое. Плюс посчитать. Посчитать, хорошо. Когда мы с вами говорим про то, чтобы добавить действия, безусловно, мы говорим про действия, которые находятся в нашей технологической плоскости. Напомните мне, мы говорили с вами такое понятие, как структурированные и не структурированные данные, и заходили ли мы, какой из этих способов мышления, которые здесь на экране, соответствуют разным инструментам. То есть за каждым этим мышлением есть какой-то инструментарий. Мы это с вами проходили или лучше напомнить, как вы считаете? Напомнить, да? Мы частично проходили, да. Частично проходили, да. Хорошо, смотрите, давайте я немножко напомню, в чем история. Первое, мы можем эти артефакты преобразовывать друг друга, исходя из какого-то правила. У нас есть правило, делай то, делай то, если то, сделай то. Второе, что Дмитрий рассказывал до меня, это история, что мы используем некие прогностические модели или генеративные модели для того, чтобы преобразовывать артефакт один на другой. Нам, для того, чтобы понять, какой есть инструментарий, вообще мы это все делаем для того, чтобы пощупать какой-то инструментарий. Нам нужно еще одно ввести понятие, давайте мы обновим это понятие, что же мне так плохо. Это понятие структурированных и не структурированных данных. Потому что этот артефакт по своей природе может иметь разные атрибуты. В зависимости от этого атрибута у нас меняется и способ его обработки. И на что я бы хотел, чтобы вы обращали внимание, какой у вас артефакт. Является ли этот артефакт структурированным или не структурированным. Это важный такой квалификатор. Давайте просто для проверки возьмем. У нас будет в качестве схемы условный процесс приема заявок от клиента. Эти заявки должны как-то обрабатываться, подать в калькулятор. У нас с правой стороны есть требование к входящему артефакту поля калькулятора, что если мы записываем код артикула, если мы записываем объем заказа и место доставки, там процесс может идти дальше. Соответственно, с левой стороны, если к нам приходит заявка в нашу систему, кстати, как вы думаете, если к нам приходит заявка от клиента в таком виде, это у нас какой, структурированный или не структурированный вид ее? А почему, как вы думаете? Вот это важно, то, что Николай сейчас говорит. Николай говорит следующее, что у нас каждый атрибут, который здесь написан, имеет собственное положение. А раз он имеет собственное положение, и более того, если вы встречаетесь, что если исходящий атрибут является структурированным, и входящий атрибут тоже является структурированным, а в нашем случае поле этой калькулятора тоже структурированное, то тогда процесс соединения одного с другим, он часто называется в IT-мэппинг, или, а? Ну, join это скорее про таблицы, я бы скорее тут мэппинг названию, что артикул товара это то же самое, что код артикула, количество единиц это то же самое, что объем заказа, А адрес доставки то же самое, что место доставки. Мы, конечно, у нас может находиться человек вот в этой позиции, который смотрит на один артефакт, копипейстит другой. Но если вы встречаетесь с таким процессом, что человек занимается копипейстом, то это просто у вас руки не дошли для автоматизации его. Если это операция по правилам, с левой стороны артикул скопируя в код заказа. Поэтому смотрите, пожалуйста, если у вас вход и выход структурированы, скорее всего, ИШК вам тут не нужна. Здесь все хорошо Поэтому если бы мы так один раз соединили бы И мы могли бы генерить разные заказы Мы бы получали В калькуляторе тоже переброску Гораздо более интересно Если мы с таким случаем встречаемся Когда на входе у нас От клиента артефакт Не структурированный Мы же согласны, что он не структурированный Да? Но при этом входящий артефакт, который нам требуется конкретно, остается прежним. В нем по-прежнему нужно указать код артикула, объем заказа и место доставки. И традиционно, когда мы при проектировании процессов сталкивались с такой ситуацией, когда у нас на входе получался не структурированный, а на выходе должен быть структурированный, в этот момент мы что делали обычно? Ну это совсем очень хорошо, если партнер, даже если слово такое знаем. А если клиенты пишут даже менеджерам и пишут, слушай, Вась, привет, я тут такую штукенцию, не знаю, как она называется, но она длинненькая, железная, с резьбой еще 2 мм. Ты там сколько ни парся, из этого болт М2 не получишь. Хотя формально это болт М2. Или чем болт отличается от винта. Клиенты могут тоже попутать. Поэтому обычно, когда мы сталкивались в проектировании таких артефактов, ставили человека вот в середину этого процесса, на это действие, для того, чтобы он смотрел на него и перебрасывал. Ну, либо парсил, либо сделал какую-то сложную автоматизацию. Это было непросто. Когда мы с вами говорим про текущий технологический уклад, оказывается, вот эту функцию по пересборке неструктурированных структурированных можно дать машине. Ну, вот в моем примере Прямо в реальном времени, когда мы говорим про сгенерированное письмо, вот оно реально сейчас генерится, это не архивное письмо, не из базы данных, у меня действительно существуют технологии, позволяющие перебросить, то есть превратить вот такой текст в достаточно структурированную форму. Как вы думаете, какую технологию мы использовали для того, чтобы автоматизировать процесс вот такого преобразования из неструктурированного в структурированный? Что? LLM, да. А как бы звучал бы prompt, кстати? Как вы думаете? Ну, кто у нас там? У нас много людей поднимали руки, что курсором пользуется. Найди в письме код артикула, объем заказа, место доставки. Ну, да, да, да. И причем, смотрите, что очень важно. Так как машине без разницы, в каком стиле отвечать, кроме того, что вы сказали, там еще будет дополнено, ответь, пожалуйста, в JSON формате, в структурированном. У меня там написано приблизительно вот так вот заполнить, вернуться вот так вот. Там ты помощник такой, ты найди, ну все, что Николай проговорил, ну и выдавай в таком-то стиле. И вот это нам, то есть для чего мы это делаем? Для того, чтобы понимая, какой у нас артефакт на входе, какой на выходе, мы можем правильно подбирать инструментарии. И мы приходим к очень интересной истории о том, что вообще существует 4 типа. Мы про 4 типа говорили с вами, нет? Напомните. Точно говорили? Да, просто освежу памяти. Мы про населительные операции говорили, но про типы структурированные не говорили. По-моему, мы не говорили. Давай еще раз про них скажем, что это важная штука. Да. Идея в том, что если мы возьмем следующую по оси Х, что у нас входит? Структурированные или не структурированные данные. А по оси Y мы выберем, каким образом мы осуществляем действия преобразования этого артефакта. С помощью правил. Или с помощью подобия. В целом есть два подхода к переработке артефактов. Ничего человечество больше не выдумало. Первый подход. Вот так вот мы подлистаем назад немножко. Первый подход, он появился как раз в 40-х годах. когда у нас машинка Enigma немцы шифровали, там был производственный процесс на входе производственного процесса был зашифрованное сообщение, на выходе из процесса была расшифровка. Производительность этого процесса была крайне низкая требовалась несколько недель а на самом деле бизнес требования было в том, чтобы расшифровка происходила в течение дня иначе она устаревала информация, то что ее расшифровал ну и хорошо через 2 недели Проблема была в том, что настолько было много комбинаторики и вариаций, что руками это не сделать. Поэтому сидели операторы, условно 200 девушек с арифмометрами, и занимались тем, что они вычисляли по некой программе заданной, которую математики продумали, скидывали программу, они вычисляли. Результаты вычислений передавали друг другу. И в этом процессе было два бутылочных горлышка. Первое – это сама передача информации. Им нужно было записать это, передать кому-то. А второе, математики все время сидели и думали, а какой у нас может быть новый алгоритм? А теперь представим себе, у тебя 200 сотрудников, и тебе нужно рассказать им, чтобы они работали совершенно по-новому. По-новому считали. Потому что математики посидели, сказали, о, у нас должен быть другой алгоритм расчета. Мы же все с вами понимаем, как можно быстро, насколько тяжело меняется процесс изменения работы 200 человек. И, конечно, в этот момент возникла интересная идея. Все, что делают атомарные операции этих операторов, они вычисляемые, просто вычисления. Они очень алгоритмические. Почему бы нам не создать машину, которая выполняет все эти операции? Не просто читает, но еще хранит, выполняет программу. И эта машина должна уметь менять свою программу в зависимости от нашего пожелания. Ну, не фиксированно, а менять. Поэтому давайте мы сделаем вычислитель или компьютер. И вот с этого момента пошла идея, что мы можем сделать вычислительные машины, которые умеют вычислять, хранить, следовать по программе. И работают они по заранее заданному алгоритму. Это как бы база. Одновременно с этой же историей возникла другая подхода. А у нас же существует бизнес-процессы, у нас существуют те артефакты, преобразование которых нам сложно сделать алгоритмически. Например, распознавание образов. Там хороший пример был у Дмитрия. Можно, конечно, смотреть на картинку с кошкой и сказать, что кошка – это усы, нос и шерсть. А на самом деле она может быть совершенно по другим углам. Поэтому очень-очень непонятно. Поэтому была вторая идея. Давайте попробуем сформулировать функции человека когнитивные, который умеет, находя подобие, делать какое-то вычисление. Поэтому у нас получилось две развилки. Вторая развилка как раз про искусственный интеллект. она началась уже об этом точно говорили, в 40-х годах. И вот это два разных подхода составляет уже по оси Y. Как мы занимаемся преобразованием? Занимаемся мы по правилам, либо занимаемся с помощью подобия. Это сформировало четыре подхода работы с данными. Еще, может быть, был у нас программирование 1.0, 2.0, 3.0. Помните, был такой? Это здесь, она очень легко видна. Программирование 1.0. Наша традиционная. Это когда вы пытаетесь автоматизировать артефакты, которые всегда на входе являются структурированными. На выходе они тоже всегда структурированы. Ну или, по крайней мере, могут быть структурированы. На входе они структурированы. Да, это правильно. На выходе могут быть разные. Правила преобразования заранее известны и записаны в виде какой-то программы. И это наша прям классическая. Как вы с этим сталкиваетесь? Когда вы пишете техническое задание айтишникам, что они вас просят? Первым? ТЗ. А что они в этом ТЗ хотят от вас? А? Алгоритм, да. Причем не просто алгоритм. Они что еще хотят, кроме этого алгоритма? Ну, таблица не всегда. Если BI, то да. Это да. Как они любят говорить. У нас эти данные почистить. Правильно? Сначала. Был такой там? Да, Василий. Как без чистых данных. Абсолютно верно. Почему это происходит? Потому что у нас не существует классический способ, никакого другого способа автоматизации преобразования этих артефактов, кроме как алгоритмически обратиться к какому-то конкретному месту, к какой-то конкретной системе. И, к сожалению, это сразу нам, как бизнесу, затягивает куча ограничений. Например, мы работаем с клиентом. Нам нужно клиента как-то описать в нашем справочнике. Те поля, которые есть в битриксе, в CRM, в DNS, нам это точно недостаточно. И мы начинаем, и у нас бизнес начинает формулировать все больше, больше, больше, больше, больше, больше полей. Проблема в том, что это не, это задача, это процесс. Он бесконечный. То есть если его не остановить, у нас будет каждый объект описываться все большими и большими полями. Исходит из позиции того, что мы не в состоянии описать конечным числом атрибутов все возможные варианты. Даже как мы не можем описать с помощью алгоритмов все возможные развития бизнес-процесса. Всегда у нас существует какой-то особый случай. И вот проблематика в том, что никаким другим способом, как ограничить количество параметров и описать их преобразование алгоритмически у программистов традиционным способом нету, а бизнес понимает, что это не охватывает все возможные варианты. Вот в этом заключается весь конфликт, найти компромисс. Вторая история, когда у нас на входе все еще структурированные данные, но у нас даже не существует теоретической возможности придумать алгоритм. Один из красивых историй – это история с ритейлом. Я рассказывал в прошлый раз про Волмарта беременную девушку, нет? Точно рассказывал, да? А, ну хорошо, ну вот эта история была про это же. Соответственно, напоминаю тогда, что все, что связано с регрессией, с кластеризацией, с предсказанием временных рядов, что говорил у нас Дмитрий, это история при образовании все еще структурированных данных, но при этом основанной на примерах из прошлого. Следующий шаг, который мы с вами делаем, это… Вот мы жили вот в этой парадигме, и вдруг у нас появляется пару лет назад история с генеративными моделями, которая удивительным образом дает нам еще один инструмент. Инструмент по переработке неструктурированных данных. Вот инструмент по неструктурированным данным – это в чистом виде генеративка. И если вы понимаете, как вы хотите преобразовать эти данные, то вот, пожалуйста, все очень хорошо. И самый последний шаг, это шаг, связанный с, я бы сказал так, когда у вас на входе не структурированные данные, а способы переработки этих артефактов. Если вы взяли эти артефакты и поняли, что у вас есть на вход, на вход у вас не структурированный, как из них сделать другой артефакт, вообще непонятно. Ему можно полагаться только на собственный опыт. То здесь, скорее всего, вы сталкиваетесь с задачей, либо предпринимательской, либо задачей проектирования. когда вы понимаете цель, но как достигнуть эту цель, вообще неизвестно, может быть много разными путями. Вот в этот момент у нас возникает такое понятие, я тут зачеркнул красненьким, писал раньше без автоматизации, сейчас здесь появляется понятие и агентов, которые чуть-чуть начинают заходить на эту сторону, которые пытаются, исходя из имеющихся инструментов и контекстов, каким-то образом выстроить тактику, при образовании одного артефакта в другой, при отсутствии правил, основываясь на опытах, примерах и так далее. Поэтому E-агенты занимают такую очень специфическую роль. В моем понимании, кстати, E-агент это не история, когда мы подключили какой-нибудь рак, а история, когда у тебя система действительно делает план действия, по нему работает и делается с некой обратной связью, с саморефлексией. Для того, чтобы вам почувствовать все вот эти три типа преобразования. У нас будет небольшая игра. Анна, напомните, у нас эта игра есть в Telegram, правильно? Ага, сейчас мы вышли. В чем заключается игра? Игра, скажу пока вводная. Сейчас мы возьмем. Она называется «Три способы разметки отзывов». Игра номер один. Заходим все в Telegram. У нас в канале есть? Она появилась уже? Все, значит, заходим, пожалуйста, на компьютере. Вообще, это история. Я думаю, что имеет смысл разделиться хотя бы по двойкам. Это как бы вместе, чтобы работать. Это очень простая игра. Здесь не обязательно прям индивидуально. Потом, если захотите, поиграйте. Ну, так вот можно Андрей Алексей, Мурат Александров. Можете как-то вместе объединиться. Тут она простая, не обязательно прям индивидуальная. Итак, у всех поднимите руку, кто еще не зашел. Кто не зашел. А, ну, значит, все остальные зашли. Замечательно. Тогда нажимаем с вами на игру номер один. Три способы разметки отзывов. И я даю вводную. Вот на следующее. на негативный, отдельно на позитивный. Давайте так подумаем вместе. Какой входящий артефакт в рамках этого процесса? Сообщение, скорее. Да, даже не набор. Согласны? Нет? Так все молчат. Вроде не сложный кейс. Вроде отзыв. Отзыв. Ну, отзыв, отзыв. Ну, на входе отзыв-то идет, конечно. Сообщение, отзыв, на входе артефакт. А на выходе из нашего процесса по ответ. А теперь давайте с вами сделаем на очень хитрый шаг. Мы с вами понимаем, я вотно сказал, что на позитивный и негативный отзыв мы отвечаем немножко по-разному. Потому что на позитивный отзыв можно ответить спасибо, что воспользовались нашим услугами промоалгоритмически, а на негативный хотелось бы ответить как-то более-менее развернуто, ну хотя бы в какую-то дискуссию. Поэтому, как вы думаете, что предшествует? Мы, кстати, во всей вот этой технологии артефактов крайне важно идти с конца. Если мы понимаем, что наши конечные артефакты – это отзыв, и мы с вами поговорили, что они бывают позитивно-негативно, что предшествует этому артефакту? Абсолютно верно. До этого существует так называемая просто классификация, либо позитивно-негативно, потому что в зависимости от этого класса у нас идут разные цепочки. Поэтому мы с вами сейчас будем заниматься автоматизацией только самого первого шага. Как из входящего отзыва из него сделать позитивно-негативный. В этом примере, если вы сейчас нажмете, ну у вас в помолчании открылся алгоритмический подход. Если вы нажмете кнопочку запустить классификацию, нажмите, да, сколько у нас процентов получилось? Это 7, правильно. Что у нас сейчас произошло? У нас отзыв, который пришел в нашу систему с помощью ключевых слов, которые написаны с правой стороны. Позитивные ключевые слова «хорошие», «отличные», «замещательные», «провосходные» через запятую и «негативные» тоже через запятую. Соответственно, алгоритмическим способом было высчитано, если у нас совпадает, Если содержится ключевое слово в отзыве, то значит оно либо позитивно, либо негативно. Если одно и то же, то мы выбираем приоритет. Я заранее в учебной истории разметил отзывы реальными. И ваша задача за 5 минут вместе, поработав, попробовать подобрать другие слова для того, чтобы получить классификацию больше, чем 37%. Задача понятна? Больше, чем 37%. Сейчас по умолчанию 37. Сразу скажу, что можно до 60 дойти точно. А там один как будто неверно размечен. Там есть неверный, нормально. Не моделью изначально. Граунд трус неверный. Несмотря на сожаление, действительно отличный продукт. Там было достаточно быстро, но все же разочарован. А написано позитивный, граунд трус. Но есть ошибки сознательно, чтобы не было 100%. Это ход такой. Хорошо. Потому что кто скажет 100%, то я вам скажу, что что-то вы не то сделали. А задача сколько позитивных, сколько негативных? Задача алгоритмическим путем подобрать такие ключевые слова, которые будут лучше размечать, чем текущие. А я вот сейчас просто в GPT закинул, он мне говорит, 41 позитивный, 59 негативных. У нас сейчас нет задачи сейчас засовывать это в чат G5. Наша задача сейчас сделать преобразование одного артефакта в другой с помощью алгоритмического способа. Алгоритмический способ это через ключевые слова. Время у нас идет. До G5 мы доберемся. У меня просьба вывести, если есть возможность, минуты на экран. Если такое, конечно, есть. Второе мы сейчас будем... У нас по рядам первый ряд, второй, третий, четвертый, пятый. И будем записывать там 100 отзывов так у нас у нас 4 команды разве а там а там 5 же 5 стол или мы все считаем 4 так сколько у вас Нет, подождите, сейчас мы вытащим. Задача вроде несложная. Если что, там Миша поможет. Давайте так, у кого получилось больше 40 уже? Так, ну давайте, сколько у вас? Так, первая команда 52. Пока мы потом будем увеличивать. Первая команда 50. Первый ряд 52. Так, на второй ряд. 61, второй ряд. Хорошо уже было. Есть у вас в третьем какое-нибудь значение у вас? 55 в третьем. 60 в четвертом получилось у нас. Мы так глобально считаем. Так. Пока. Ну, мы не понижаем. Да, мы у нас, если... Миш, там у Кирилла был вопрос. Так, давайте так. Может быть, первый ряд у вас получилось больше, чем 52. У кого-то есть 50 больше, чем 52? 66 у Василия. 56. 56. А, нет. Мы не уменьшаем. А, не было, да? Да ладно, хорошо. Так, хорошо. Во втором ряде есть что-то больше, чем 61? Получилось у кого-то? 68. 68. Второй. Ага, так. Так, больше, чем 55 получилось? 65. 65. Так, 65, третий. Так, у вас больше, чем 60 получилось у кого-нибудь? 66, да. 66 получилось, отлично. Ага, так. Так, 70 у кого? Ух ты, 73. Так, а у вас больше, чем 56 получилось у кого-нибудь? О, 63. Значит, 63. Первый ряд записываем. 65, второй. На понижение не играем. У нас уже 68 было у кого-то. Да, уже было 68. О, 70. Круто. Четвертая команда. 70. 72, третья. 72, третья. Так. Ну, давайте так. У нас еще немножко времени. 74? Круто. Вторая 74. Анна, вторая 74. 75. Ну все, давайте завершаем тогда. 75, вторая, третья. Финализируем. 74, первая. Отлично, все. Финиш. Финиш первого раунда. Теперь смотрите. Мы с вами переходим на второй этап. Второй этап называется обучение через примеры. Это то, что называл у нас, помните, Дмитрий, если не ошибаюсь, кластеризация, можно сказать так, кластеризация или классификация. Это один из тоже классических email подходов. мы занимаемся преобразованием того же самого артефакта в те же самые выходные, но совершенно другим способом. Потому что для чего нам это нужно? У нас есть разные способы. Как мы это занимаемся? У нас с правой стороны, если вы видите так называемые отзывы для разметки 50 штук, там, заметьте, листочки по 10 штук, там 5 листочков по 10 штук. Если мы выберем не менее четырех, допустим, позитивный, позитивный, негативный, негативный, у нас активируется кнопочка запустить обучение. Что в этот момент? В этот момент система попытается понять, а что общего между позитивными и негативными, которые вы отметили, и как только она поймет общий смысл, паттерн, у вас активируется кнопка запустить классификацию. И она этот паттерн применит в отношении всех отзывов, которые с левой стороны. Смотрите, как нужно запустить. Вы пишете, вы размечаете отзывы, вы размечаете, потом пишете запустить классификацию, потом классифицируете. И можете повторять этот цикл столько раз, сколько нужно. Поехали. Пять минут, то же самое. Пытаемся. Решайте сами. Там, безусловно, есть спорные отзывы, сделаны сознательно. Может, у кого-то есть первые результаты уже? Тринадцать уже. Мы бьем 14,8. Ну давайте, ладно. 19. Так, ну хорошо. Заметьте, у вас там 50 отзывов. Так странички перелистывайте. Перелистывайте странички. У вас там 50 отзывов. Не 10 отзывов к разметке, а 50. Не забудьте. Так, есть какие-нибудь первые результаты? Так, 51,2 у четвертой. Так, 57 у третий. Прям как-то мы... Ну, конечно, надо запустить очень... А, 67,3. 67,3. Так, а первое... О, 90, наконец-то. У нас появились первые близкие результаты. 94 у команды номер 3. Да. 93 у команды номер 4. Так, так, давайте. А в первой команде-то у нас есть что-нибудь? 94 первая. 94 первая. Так, ну у нас вторая осталась. Давайте вторая. Пока 64,6. Давайте до 90 добьем. Нужно до 90 добить. Нет, вообще должен показываться. У кого 85? А у вас уже было 94 у кого-то. Да. Хорошо 85. Вообще, конечно, чтобы каждый из вас довел хотя бы больше 80. 95, 2. 95 вторая, наконец. А давайте мы… А, товар ли в целом отношения компании. Вы задаете правильно. Если мы говорим именно про учебную задачу, я бы сказал так. С совокупностью позитивных и негативных. То есть если у тебя позитивных отзывов больше, чем позитивного отношения к товару, включающий себя... Ну хорошо, и какая там стоит, и что у вас было? Но. А он какой был? Он там позитивный должен быть. Вы поднимаете? Давайте так. 95, ну вот отлично У кого 98? А, ну все, мы максимум достигли Больше 98 лет, потому что там есть Специально два отзыва Сложных, которые Специально сделаны так, чтобы их невозможно было Отгадать Это такая проверка На 100% Хорошо, смотрите, мы с вами сделали Еще один шаг, мы преобразовали Один артефакт другой С помощью ML А теперь мы сделаем опять же все то же самое, такое же самое действие, но с помощью третьего инструментария. А третий инструментарий называется генеративная модель. Если вы нажмете на третью вкладку, у вас появятся такие поля. Это промпт в каком-то смысле. Первая роль, а второе, что такое позитивное, что такое негативное. негативный. Если вы опишите своими словами что такое позитивное, что такое негативное и нажмете кнопочку запустить классификацию в течение полутора минут, она должна отклассифицировать и показать результат. Отличие от первого подхода, если в первом подходе он строго выполнял поиск по тем ключевым словам, то в третьем подходе он будет ассоциативно, с помощью ассоциации. Это будет использоваться генеративной модели. Поехали, снова 5 минут, даже не 5, давайте 4 сделаем. Вы раньше, у вас был фальстарт, у вас был фальстарт, 97. Ну ладно, давайте подождем. Да. Не помню сейчас. Я не помню, как я размечал. Точно работает, когда больше позитивного, чем негативного. В основном товар был. Вопросы вы очень правильно задаете, кстати. С точки зрения, мы сейчас это разберем, но с точки зрения подхода у вас 100% справедливый вопрос. Она на основании чего-то. Это очень правильно. Он начал по команде, скоро должен получить первый результат. А тут везде 98 максимум. Там больше 98 не бывает, потому что там два отзыва специально сделаны неправильно. Ну, чтобы отследить какую-нибудь ошибку. Не, ну можно, на самом деле можно. Обходили у нас. Он долго обрабатывает, да. У нас был случай, когда получилось 100%. было, но мы поняли, что был глюк системы после этого ввели более строгие правила тоже отрефлексировали 97, первое, так, надо включить надо включить но время еще есть, да, можете еще один запуск сделать Дым есть. 96 вторая, но у нас уже был у кого-то 97. Как у вас? 96, да? Хорошо. 96, 2, 96. О, 97, 3. Супер. Так, а команда номер 4? Тоже 97? Тоже 97. Что у нас, досрочная сдача? Получается, да? Ну ладно, слушайте, если у нас получилось, давайте так, кому-то надо еще время докрутить свои результаты? Кого еще крутится? Ну давайте сейчас додумаем, да, пару секунд, чтобы у вас было определенное завершение этого этапа. Анна, можно попросить вас среднее вывести? А, есть, извиняюсь, простите. 97. Ну да, у всех, кстати, у нас, по-моему, еще не было. Так что, у всех было 97. Мы обычно делаем с большим количеством групп, там разнообразие еще больше. 98? У кого 98? Четвертое 98. Догнали, догнали. Все, максимум вывели, максимум. Супер. Хорошо, давайте попробуем поразбирать эту историю. Что у нас сейчас произошло в историческом плане? Или вообще… Если мы посмотрим на историю классификации отзывов, мы де-факто сейчас прошли с вами 15 лет. То есть, когда впервые появилась задача классификации, она начиналась с парсинга по ключевым словам. Прям классика. Потом следующий был заход, где-то начиная условно с 15-16 года, когда как только Google научился хорошо делать векторизацию, там Берфовские модели появились. У нас в течение от 2014 года до 2024 много работали через второй. А генеративные модели для использования классификации стало одной из самых… Кстати, команда номер два у нас победила по общему зачету. Вы молодцы, да, молодцы. Кто неправильно? Где неправильно? Ну ладно, хорошо. Но нас все равно не предусмотрено, поэтому это не так страшно. Хорошо. Теперь у меня вопрос к вам. У нас с вами три способа создания этого артефакта выходного. Какой из этих способов для вас был самый понятный и прогнозируемый? Где вы понимали, что происходит? Первый, второй, третий. Для кого первый, что я просто понимал по рукам? Самый понятный, что происходит. Первый. А для кого второй? А третий? А остальные чего? Остальные, что здесь было? Что мы играем? Ладно, хорошо. Мне, кстати, очень интересно, кто поднял руку третий, расскажите, третий, почему для вас был самый такой понятный? Вы же подняли руку, да? Или вы давайте? Почему? Текстовым простым языком ты можешь создать описание. Это другой вопрос. У вас был легкий вход. Но вы понимали, как это произошло? Почему то, что вы написали, прекрасно разметилось? Вы понимали технологию разметки? В принципе, да. Через связи, через синонимы, через таксономию, которую он построил на этой модели, он делает с пустойными рейтингами. Если мы заходим на таксономию и синонимы, Это история, скорее, больше первого-второго рода, и это, к сожалению, не так не работает. Потому что это была большая языковая модель, но не работает под таксономией. Я могу свою версию. Я поднял руку два раза на первый и на третий. Да, когда интересно, почему? Третий. Ну, из первого стало ясно, что позитивное – это где пользователю либо ему понравилось, либо он рекомендует, либо он будет покупать. Во всех примерах было только это, где была позитивная. А из позиции, смотрите, вы сейчас объясняете из позиции бизнеса, мы же сами говорим про цифровые артефакты и про программирование. Вы же де-факто сейчас занимаетесь программированием. Так я же и рассказываю. А, понятно. Я в критерии позитивного вбил ровно эти три смысловые момента. И дальше, с точки зрения программирования, я понимаю, что ЛЛМ, она этот смысл распознает. Гораздо лучше, чем просто ключевые слова. Она распознает конкретно эти смыслы. Она их видит там. Да. И поэтому она размечает. То есть мне в этом смысле все понятно. Ага, так, интересно, хорошо. Вот почему я на третьем этапе поднял руку, потому что там была возможность вообще промпт внести. Вот на втором, что мне было неудобно, вот если у меня тысяча отзывов, мне надо их прочитать и размечать. Негативный, позитивный, негативный, позитивный. Надо их размечать, это очень неудобно. Ладно, когда 50, а когда их там 50 тысяч. А в третьем, там есть окошечко, куда я могу вписать свой промпт. То есть я эти данные могу изучить по отзывам, условно через языковую модель, сделать промт и закинуть промт. Так как сейчас вы продвинутая аудитория, то для вас такое понятие как промт и большая языковая модель является более-менее естественным и понятным. Вы можете в своей голове понять, что происходит дальше. Или, по крайней мере, мы сейчас об этом поговорим. Но если мы посмотрим на большинство людей, для них это какая-то магия. То есть каким образом у тебя слова и описание смысловое, то, что вы сказали, смысловое, вдруг превращается в конкретный артефакт. Для них это черный ящик. Причем, для большинства это черный ящик. Для многих понятие алгоритмического подхода, оно объяснимо. И, кстати, оно объяснимо в том числе для регуляторов. Если вы попробуете, например, взять какое-нибудь медицинское изделие, которое распознает МРТ или что-то еще, и делает диагнозы, Там будет огромное требование от регулятора объяснить, почему это происходит. Объяснение с позиции «я написал промт» и оно получилось, это не объяснение. По формальной логике. Что оно сделало? Формальная логика – это когда мы 1, 2, 3, 4, если так, то это так. Самое простое объяснимое – это первая история. Но при этом обратите внимание, а вот когнитивно вам проще всего программировать было, какой, первую, вторую или третью? Третью. А почему? Смысла? И заметьте, кстати, какая скорость была выхода на результат. Она была очень быстрая. И у нас получается очень интересная, ну, фактически, картинка. Либо мы можем получить абсолютно понятный, объяснимый алгоритм, но с точностью 74, либо мы можем сэкономить огромное количество когнитивного своего времени, получить быстрый результат. Честно, для большинства непонятный каким образом, ну, потому что это магия как-то получилась, но 97. Вот объяснимо четко, быстро 74, либо необъяснимо, магическим способом 97. Ну, по центру стоит вот эта история с разметкой. Для многих, кстати, кто поднял руку, второе, чтобы понятно, второе было. А почему? Там была очень хитрость. С точки зрения когнитивных затрат, на самом деле, это было точно проще, чем первое. Я не знаю, проще ли когнитивное второе или третье, кстати, я не знаю. А, кстати, как вы думаете, когнитивно проще вторая или третья? Какая первая? Первая точно не проще. Это надо логическое мышление иметь. Тут смотря на сколько глубоко. А представьте, тысячи у нас будет. Вторую можно достаточно простые умственные усилия, но их много. А третья, умственные усилия, они сложнее, но их гораздо короче и быстрее. Вот, Алексей, вот вы прям в точку смотрите. Потому что до недавнего времени история по разметке различных вот таких датасетов. Это была самая низкооплачиваемая когнитивная работа в IT. Реально индусы, вьетнамцы сидели и за 3 копейки размещали вот эти тысячи отзывов. Потому что в массе это очень простое, либо да, либо нет. К ним приходили задачи, это еще Яндекс.Толука была, и есть такие сервисы, когда они регистрировались, люди, к ним прилетала фотография, допустим, для разметки автопилотов. И написано, покажи, пожалуйста, где здесь знак, где здесь человек, где машинка. Они размечали, получали свои полцента за это и переходили к следующей задаче. Только не полцента, а полцента за 100 фотографий. То есть там совсем была такая патагонная рабская работа. Слушайте, с капчей интереснее. Капчей это история, когда вы становитесь этими разметчиками. Кстати, вы замечали, что у нас в разное время капчей бывают такие, видно, что они сгенерированные буквки написать, а бывают фотографии. От самых примитивных «покажи велосипед, лестницу» на выборе до в свое время было «напиши, какая цифра изображена на фотографии, и фотография размытая, какой-нибудь номер дома». В этот момент очень происходила интересная история. Вот эти большие все гиганты, особенно Google, вот они ездили, машинка у них фотографировала дороги. Им нужно было делать какое-то автоматическое сличение, нанесение номеров домов на карты. Самый лучший способ – это взять, вытащить из всех этих фотографий огромное количество плашек и отдать людям на разметку. То есть вы думали, что это капча проверяет, что вы человек, а на самом деле вы делали разметку. Конечно. То есть в этом был смысл. И еще помните, у нас были такие истории, когда запрещены сейчас в социальной сети, был тренд «покажи свою фотографию, как ты выглядел 15 лет назад». Был такой флешмоб. И в этот момент собирались датасеты, как ты выглядишь сейчас и как ты выглядел раньше. И тогда можно было выстраивать определенные цепочки по работе с соображениями. Поэтому вы действительно капчей занимались. Так, а какой-то вопрос? Слушайте, вы подняли хороший вопрос по поводу квалификатора, что является плохим или хорошим. Это вопрос о цифровых артефактах. Безусловно, как только вы описываете цифровой артефакт на выходе, к нему возникает ряд обязательных требований. Если вы не опишете эти требования, вы не сможете раскрутить обратно. Если вы не понимаете, например, это хорошее в отношении продукта или сервиса, как вы правильно заметили, вы не можете правильно составить промт, на что он будет обращать внимание, на сервис или на промт. Поэтому это очень хорошее замечание. Поэтому история со второй, она действительно… Кстати, вы заметили, что чем больше отзывов вы отобрали во втором, тем больше была точность. Заметили? Заметили. Ну, отлично. Таким образом, вы сейчас попробовали на своей практике три разных способа автоматизации. И в каком-то смысле, все, что мы с вами говорили до этого, все процессы, которые расписывали, их можно свести к одному из этих трех решений в конечном итоге. И более того, именно в этом заключается основная задача при автоматизации процессов, так декомпозировать свою работу, чтобы можно было свести к этим примитивным операциям. Если вы способны это сделать, то дальше создание ТЗ является делом таким техническим. Именно из этой позиции, в самом начале я сказал, у нас выходной артефакт является письмо, но им предшествует классификация. Потому что зависимость от того, как я классифицирован, идет по разным сценариям. Зафиксировали. Понимаю, что конец дня уже тяжело. Поэтому попробуем пойти дальше. Давайте так. Мне нужно, чтобы вы сейчас попробовали взглянуть на свои проекты. Мы сейчас не будем это делать с помощью обзоров. Просто прямо с места. Попробуйте теперь посмотреть на какой-нибудь проект, часть вашего процесса, артефакта. Скажите, какой входящий и исходящий артефакт. И приведите пример, где он алгоритмический может быть, а где с помощью генеративных моделей. Ну или третий, предсказательный даже. Давайте, вот, Максим, например, можно микрофончик, да? Проверим заодно понимание. Так, ну у меня проект пока автоматизация коммерческих предложений подготовки. Входной артефакт, это, соответственно, данные, полученные от клиента, в виде конкретных единиц измерения, атрибутов, вернее, да, там, длина, диаметр, там, структурированном виде или нет? Структурированном, это один вариант. Это где мы применяем в дальнейшем алгоритмический подход, потому что у нас это все структурировано, менеджер это записал, условно говоря, ну, он собрал в звонке и записал. А вот это важно, смотрите, у вас на входе, вот вы сейчас, хороший пример, у вас на входе артефакт со стороны клиента, пускай он даже голосом сказал, что артефакт. Он не структурированный. Он не структурированный, а потом менеджер проделал когнитивную операцию и записал это в систему в структурированном виде. Да. То есть у нас есть на входе артефакт в виде, условно, звукового ряда, если бы это было, ну, то есть звуковой, а на выходе структурированный. И вот этот процесс преобразования сейчас делает менеджер. Но мы чуть дальше начинаем наш процесс автоматизации, что у нас есть эти данные. А второй вариант, где применяем генеративный ИИ, это приходит ТЗ. PDF-файл, документы, схемы и так далее. И, соответственно, уже ИИшка это будет распознавать. ОКР и... Первый, это ОКР для того, чтобы привести в текстовый вид, если мы не используем мультимодальные модели. А вторая задача, скорее всего, будет преобразовать параметризированный, структурированный вид, чтобы вы могли дальше по цепочке. Совершенно верно. Хорошо. Так, давайте так, еще пример. Здесь очень хороший пример. Так, давайте кто еще? Вот из четвертой. Вспомните какой-нибудь пример, просто применить это, вспомните какой-нибудь процесс. Два артефакта. Микрофончик лучше поставить. Микрофон будьте добры. Инсии мы когда делали давным-давно, вот там алгоритмический подход в полный рост. Выявление документов, похожих на то-то. Да ладно. Расскажите, какой был артефакт на входе, какой на выходе, и как вы преобразовывали алгоритмически? На входе были названия документов из разных систем, 1С. Названия документов на входе. Их надо было между собой матчить и делать эталонную карточку какого-либо документа или сущности. Нет, у вас на входе было скорее всего не документы, вам нужно было... То есть у вас на входе был список, видимо, правильно? список названий различных документов. А на выходе список сгруппированный. Сгруппированный с золотой карточкой, которые по определенным... И как вы это алгоритмически делали? Находили похожими словами, как раз вот так же со звездочками. А, то есть вы использовали алгоритмический подход? Для того, чтобы находить подобные документы по совпадению. Кстати, кто еще интересовался, раньше был такой алгоритмический подход, называется расстояние Левенштейна. Вот, потом мы его уже сделали. Да, потом Левенштейна. расстояние Ливенштейна, это сколько нужно сделать перестановок букв, чтобы один артефакт получился другой. Если у тебя одинаковые слова, то у тебя получается ноль перестановок. Если у тебя одна буква перепутана между собой, то у тебя получается один. Поэтому чем больше расстояние Ливенштейна, тем дальше друг от друга. Хорошо, это интересно. А генеративно тогда? В бизнесе пока еще, честно сказать, не использовали. Единственное, что классический подход с курсором, который в разработке у нас. Когда ты задаешь ему нечеткий запрос на «сделай мне скрипт такой-то, который делает то-то, то-то». Кстати, по поводу скрипта. Это, кстати, интересная история, потому что де-факто, чем занимается программист? Он берет техническое задание и преобразовывает его тоже в текст, только в виде кода. Поэтому действительно история с генеративными моделями, с вайп-кодингом, когда ты даешь задание, он пишет какую-то страничку, это чисто хороший пример. В вашем случае, кстати, а как бы вы сейчас сделали вот это мейчинг НСИ? Я бы прям загнал все сразу в ЛМ и сказал, пометь мне все похожие как-нибудь. Будет точно лучше, чем... Вот это очень важный момент. Александр, подвесьте вот это сейчас вашу гипотезу о том, что я загнал все. И мы попробуем в течение минут 40. Вы должны подумать, насколько это правильный был подход. Я попробовал. Кстати, а кто пытался засовывать в GPT большое-большое однотипных строк и с попыткой что-то с ними сделать. Лиды, справочники. Кто-то пытался такое сделать? Ну не важно, что. Справочники, лиды, контакты, описание товаров. Что-то повторяющееся, но очень много строк. В курсор я клик стрим. Курсор это отдельный, а курсор это агентская система. Можно микрофон? Аналитику делал по сделкам, ну там создание графиков и так далее. Я сделал это сам в Excel. Там было у нас по-моему порядка 120 тысяч записей. Не знаю, большое или не большое. Это вы в Excel сделали? Он был в Excel файл, я выгрузил с битрикса, закинул с задачей в GPT. В GPT? Да, сделать аналитику, вывести графики и какую-то статистику по определенным данным. И сделал это же самое руками в Excel. А вот теперь хитрый ход. Если кто в чат GPT, у него есть такая функция, называется дата аналитика данных, там прям функция такая красивая возникает. Кстати, как вы думаете, что он делает? Вот де-факто, как у него получается преобразовать 100 тысяч строк в красивой графике, ну в вашем случае графики, видимо, были. Как вы думаете, он это делает? - Питон-код, по-моему. Там же он показывает, что он генерирует питон, потом это применяет. - Вот это очень важный момент. Вот это очень важный момент, потому что, смотрите, по своей природе генеративная модель, у нее нет функции прохода по всем строкам, но она может написать код, который выполнит… Я как раз для вас рассказываю, да. Я говорю о том, что на вашем примере генеративная модель здесь поступила очень хитро. Она не пыталась обработать каждую строку, это бессмысленно, но она написала код алгоритмический, который позволяет отработать каждую строку. Но если вы возьмете, например, какую-нибудь open-source модель, не чат, а какую-нибудь open-source, которая у вас там на АЛАМе или где-то еще, у вас такой фокус не получится. Да, это вот специализирование. У Клода есть такая, у Gmini, то есть специализированные функции, позволяющие работать алгоритмически в отношении большого количества данных. И давайте последний пример, я потом расскажу, почему так происходит. Просто краткий какой-нибудь. Давайте вот где-нибудь. Команда номер три. Любой пример скажите просто. Все-таки хотите лавру первой гадрозикомата забрать, да? Нет, просто интересный пример. Ну давайте, Ланда. Стандарт оснащения медицинского учреждения, да, там все по стандарту, то есть должен быть перечень медицинского оборудования, условно, там, не знаю, стандарт врача-педиатра, да, там есть в кабинете, условно, там, не знаю, 30 позиций. Так, вот под эти 30 позиций нужно автоматически подобрать оборудование из каталога. О, хороший пример. Да. То есть, вот это алгоритмический подход. Оборудование есть разного типа. Есть дороже, есть дешевле. А вот давайте коллеге поможем. У коллеги на входе в качестве требования к оснащению помещения. Помещение. Это может быть 10 кабинетов, 100 кабинетов. Это у нас какой? Структурированный или не структурированный? Слышали или не слышали пример? Может быть не структурированный. А? Ну обычно описано 100. Ну обычно это структурированный, потому что есть конкретные. Ой, да ладно. Ну нет. Ой, стандарт, нет, там, да, стандарт оснащения там. Даже написано, стетоскоп должен быть в кабинете. А какой стетоскоп нет, ты туда можешь любой ставить. Тогда смотрите, вот здесь очень важный момент. Когда мы с вами говорим про артефакт структурированный, так как мы с вами занимаемся цифровизацией, автоматизацией, мы должны говорить из понятия структурированного для машины, не для вас, не для чиновников в Минздраве. не для него структурированные, потому что в их понятии электронный документ это вордовский документ, отсканированный или сфотографированный, отправленный по электронной почте. Для них это электронный документ, на самом деле это не электронный. Так вот, вордовский документ не является структурированным, потому что вы не можете сказать, вы не можете алгоритмически к нему обратиться, сказать, выведи, пожалуйста, всю таблицу, и у каждой есть заход в государстве, каждый товар обозначает через КТРУ, каталог товаров, работы, услуг, да, там есть некая кодификация, Но не всегда. Окей, хорошо. У вас на входе стандарт. Так, а на выходе? На выходе стандарт заполненный нашим оборудованием. Ну давайте так. Как бы вы пошли к автоматизации этого процесса? Давайте, не знаю, четвертая группа. У нас есть на входе, это, кстати, хорошая история, на входе у вас стандарт, а на выходе должно быть коммерческое предложение. Как бы вы, давайте четвертая группа, подумайте, какой может быть подход к этому процессу? Ну, это как вариант, да. А давайте попробуем от обратного пойти. Ну, можно завести шаблон, который соответствует стандарту, и заполнить шаблон согласно вводным... Кстати, хорошая идея, действительно, такой алгоритмический подход, взять шаблон, по нему заполнить. Это прям, видимо, так классически... Ну, и удачные проекты, которые были до, да? Да, это хорошо. А давайте так, если мы пойдем от обратного, у нас с вами есть список оборудования. Для того, чтобы получить список оборудования, нам какой должен быть предварительный артефакт до этого. То есть мы идем с конца. А надо спросить, вам задорого или задешево? Это в самом начале, подождите, задорого или задешево. Это было хорошее. Спецификации. То есть мы должны иметь в структурированном виде продукт 1, продукт 2, 3, 4, 5. Спецификация. Если у нас есть спецификация по продукту, мы можем подобрать либо алгоритмические, либо с помощью генеративки, например, нужные. А как нам получить спецификацию из исходящего? А бумажные, ну, вордовские документы в данном случае. Есть стандарт, да. Либо запросить, либо по какому-нибудь классификации клиента. Типа, если это нефтегазовый, то ему такие турбины. Нет, не стой, прилетает к тебе от клиента вордовский документ. В этом вортовском документе видим какое-нибудь описание стандарта. Я хочу, чтобы в моей больнице было такое оборудование. Это у нас входящие. Исходящие коммерческое предложение. К коммерческому предложению предшествуют спецификации. Структурированное. А спецификации нет в исходном документе? Конечно нет. Тогда нужно попросить механизм поискать спецификацию. Если нет, запросить ее. Смотрите. А вот теперь давайте так. Мы сейчас упрощаем. Мы сейчас сознательно упрощаем. Станислав, мы сейчас сознательно упрощаем, потому что нужно... Смотрите, а вот теперь для того, чтобы из вот этого неструктурированных данных получить спецификацию, просто типов оборудования, в каком будем подход использовать? Алгоритмический или генеративный? Генеративный. А вот теперь восстанавливайте всю цепочку. Мы используем генеративную модель, исходя из входящего текста, формируем сначала структурированную спецификацию. Забываемый. У нас получается уже входящий артефакт структурированный. И дальше мы атомарно в отношении каждой строки этой спецификации применяем либо алгоритмический подход, как у вас было, например, расстояние Ливенштейна. Допустим, если у вас стетоскопы написаны, то там стетоскопы можно было найти. Либо с помощью генеративных моделей найти наиболее близкое. Справедливый был вопрос. Вам подешевле или подороже, то скорее всего к этому будут еще какие-то правила приоритизации. И вот мы с вами сейчас де-факто восстановили цепочку автоматизации процесса, где у нас появляется два шага. Первый шаг – получить спецификацию, а спецификация – сделать уже коммерческое предложение. Мы же можем в генеративном задать сценарии, либо… Как их еще правильно обозвать? Ну, я думаю, да, как сценарий прописать или... Вот слова не могу подобрать. Вот это, смотрите, хороший вопрос. Как раз будет слайд про это по поводу, можем ли сценарий прописать. Вот в случае... Давайте покопаемся немножко в разных подходах. В алгоритмическом подходе, сейчас на экране у нас, есть процесс. Этот процесс доставки пиццы. И здесь уже сценарии прописаны. Или у вас по сценариям это что было? Если кто-то делает... В генеративке, потому что если мы берем алгоритм, там четко прописано, он не может отклониться. То есть если да, он идет вверх, если нет, он идет вниз. Абсолютно верно, да, точно. Да, если что-то непонятно, он возвращается к истоку, к запросу. А в генеративке мы можем оставить окошко, где он импровизирует. То есть либо дообучается, либо импровизирует, либо сценарий. Ну, вы правы, мы действительно можем оставить окошко, нужно ответить себе с точки зрения бизнеса. А мы должны оставлять эти окошки? Допустим, у нас есть какая-нибудь задача по маршрутизации договоров. Там все, что больше 10 миллионов, должно пойти на подпись генеральному директору, а все, что меньше, должно пойти, можно подписывать начальника дела. Вот эту сценарность, ну, можно, конечно, дать генеративке. Скажи, генеративка, определи, пожалуйста, кто должен подписывать. И там можно заставить какой-то люфт. Но де-факто, конечно, в реальной жизни, если вам нужно иметь очень четкое распределение, детерминированный выход 1 или 2, то лучше использовать алгоритмический подход. Во-первых, это дешевле, а во-вторых, это гарантированно. Да, это понятнее. Во-первых, это объяснимо. Почему он принял решение отправить на согласование директору. Во-вторых, это дешевле. Потому что заметьте, как у вас… Так, у нас… Нет, можем вместе обсудить на самом деле. Если есть какая-то интересная идея, можем вместе обсудить. Давайте. Так. Микрофон. Микрофон лучше говорить, да, потому что… Так. То есть обозначен класс оборудования, то есть не обозначены никакие его характеристики. И попытка сделать на руке вот этого запроса что-то на вроде трафарета. То есть, а какие виды оборудования могут соответствовать требованиям стандарта этой конкретной страны или этого региона? Ну, это очень хорошо. И как вы думаете, какой подход-то будет? Логографические или генеративные ближе? Я думаю, что генеративные. Я тоже считаю, что генеративные ближе. Я думаю, что это, например, для конкретного типа оборудования, предположим, только японское оборудование. Вот только японское. Не, я согласен, да. И вот это уже, и только после этого уже приступают к шуму каталогов. Как можно из этого класса оборудования, исходя из контекста клиента, контекста всего, подобрать нужно? Потому что вы правильно сказали, контекст. Это безусловная история про генеративную модель. И для того, чтобы сделать такое преобразование, потребуется вытаскивание вот этой методики. А как мы должны мыслить? У меня сейчас на экране немножко другой подход. У меня сейчас на экране алгоритмический. Что сейчас на экране? На экране сейчас схемка бизнес-процесса доставки пиццы. В нем есть, ну, какие артефакты, я думаю, что вы можете сами представить, они не столь важны, какие артефакты стоят за каждым этих квадратиков. Тут есть два очень типовых элемента именно для алгоритмического подхода. Первый, мы уже, кстати, обсудили его, это история про заранее заданные решения. Вы их видите? Да, сколько у нас заранее заданных решений здесь, кстати? 2, 3, 5, 3, а 3-е-то где? Два, да? Скажите, три какой? Четыре? Пять? Это же интересно. А сколько развилок-то? Две развилки-то? Это в стандарте BPMN. Третий ромбик – это соединение. По стандарту BPMN – это не развилка. Первые два ромбика – да, это два. Но две развилки. Ну как, первая развилка, есть заказ, да или нет, вторая развилка, сколько список заказа? Развилка у нас две всего. Ну как бы развилка, это не означает, что два варианта. Это как, да, ну я просто по терминологии выручу. Ну да, здесь если три, у него нет выбора пешком или машина, у него определился уже выбор. А вот теперь смотрите, а есть еще второй элемент, крайне важный на этой схеме, который присущ алгоритмическому пути. Если вы можете определить конкретный, детерминированный выход, и у вас есть логическое объяснение, как по нему пойти, больше или меньше какой-то, это дешево, быстро и понятно. Это надо так делать. Есть еще второй элемент на этой схеме, который относится исключительно к очень алгоритмическому подходу. Давайте посмотрим. Еще раз вопрос. Здесь на этой схеме я условно отобразил некий процесс. В этом процессе есть два элемента, которые, если вы встречаетесь при проектировании ваших артефактов или процессов, или артефактов связанных с процессами, скорее всего, это надо делать алгоритмически. Первое мы определили. Если у вас артефакт преобразовывается по какому-то процессу с детерминированными, заранее определенными логическими цепочками, если то, то делай то. Налево пойдешь, там коня потеряешь, направо пойдешь, там себя и так далее. Детерминированные. А второе что? Очень важное. Михаил, да. Последовательно? Давайте, тепло. Но ответ не точный, но тепло. Может, наоборот, параллельная операция? Давайте, параллельно, уже близко. Кто-то мне, кажется, сказал все слова такое важное. Одновременно. Так, давайте думаем, смотрим. Конец дня, полоседнего, тяжело, но надо. Смотрите, вопрос. Мы всегда должны, когда автоматизируем, мы всегда должны думать неким процессом, конвейером. Одно перетекает в другое, одно перетекает в другое. И первое, когда на этом конвейере встречается такое понятие, как развилка, мы понимаем, что по этой развилке пойдет в зависимости от какого-то конкретного правила, детерминированного, заранее определенного правила. Он не может пойти здесь на велосипеде. Здесь нет велосипеда, у него просто в инструкции этого нет. Он может либо так, либо так. Здесь местами, например, он должен перво стоять. Это есть заказ или нет заказа? Забыли. У нас есть еще второй элемент, который прямо показан как сущность. Первая – это развилка. Вторая. Цветом выделено даже. Цикл. Кто сказал цикл? Правильно, коллеги, цикл. Когда вам нужно повторить преобразование одного и того же артефакта или сделать циклический? Вот цикл – это абсолютно алгоритмическая история. И она является крайне важной при проектировании автоматизации. Потому что, представим себе, мы хотим сделать разнесение платежек, там финансовая служба, приходит огромное количество платежек. Вам нужно их разнести по разным статьям расхода. Из названия вытащить, из названия понять какая-то статья расхода. Какую бы взяли бы технологию? Алгоритм из названия? Ну, это традиционный, но это мы вспоминаем первый случай. Это понятно объяснимо, но 74%. Можно сделать проще когнитивно. С помощью ЛЛМ-ки загоняем назначение платежа в ЛЛМ-ку, может быть, с каким-то контекстом, с правилами разнесения, а на выходе он тебе будет поплевывать статью расхода или что-то еще. Это ЛЛМка. Понятно. А теперь следующий вопрос. Но нам же это нужно повторить в отношении всех сотен платежек на сегодняшний день. Не в отношении одной. В отношении одной мы это умеем делать. Правильный промпут составили. Ты финансист, твоя задача на основе финансовых правил разнесения платежек. Получаешь назначение платежа, на выходе выдаешь какой-то код, например. Вот смотрите, да, это правильно. Так вот, смотрите, в циклический повторить, это означает, что вы должны сделать некоторую алгоритмическую надстройку в Одинесске, в Битриксе, в вашей системе, которая в цикле будет вызывать эту ЛЛМ. Не то, что вы в генеративную модель кидаете сразу сотни назначений и ждете, что он каждую из них переработает, особенно в случае локальной модели. А вы делаете систему, в которой она, в том интерфейсе, в котором вы работаете, будет последовательно вызывать генеративную модель шаг за шагом. И у вас получается маленький продукт автоматизации. Потому что, де-факто, вы берете первую, классифицируете, кладете, первую классифицируете. Это часто ошибку совершают, потому что как-то звучит часто в беседах. А пусть за меня ИИшка разметит все справочники. пусть она разметит за меня всех клиентов. Когда мы говорим всех клиентов, это означает, что кто-то должен взять каждого клиента по отдельности по одной штуке и его засунуть. И обычно это какая-нибудь, там, где вы работаете, в интерфейсе, может быть, Bittrex, может быть, 1S и так далее. Поэтому, когда вы будете продумывать преобразование артефактов, обращайте внимание, не требуется ли вам повторение циклических работ. Это будет часть вашего, допустим, вашей спецификации у вас по циклу. Мы создали с помощью генеративной модели спецификацию, а я бы сделал следующее, потому что у вас может быть много спецификаций, длинные, может быть сотни этих позиций. Я бы с помощью генеративки сделал бы спецификацию, а потом вот наше не каждого элемента этой спецификации в цикле нашел бы его соответствие. И тогда у нас снижается вообще в целом и требование к колоннамоге. Вот так вот мы должны мыслить. Я бы сказал так, если у вас будет до 30 элементов, 20-30, может быть даже 40, с современной ЛЛМКой они более-менее справятся. Дальше начинает идти уже галлюцинация, может быть справится, может быть нет. Если вам допустимо, что в рамках последовательной обработки что-то пропустится, ну и ладно. Но обычно в таких случаях это не нужно. Более того, у вас тем самым повышается требование к ЛЛМКе. Если вы это делаете на локальной модели какой-нибудь, совсем очень маленькой, она первые три возьмет, а дальше забудет. Поэтому чем маленькую модель использовать, тем больше нужно детерминировать на микрооперации. Вот из этого складывается решение. Так, устали, наверное, да? К концу дня, да? Нет, не устали, да? Хорошо, слушайте, у меня еще такой к вам вопрос. насколько вы поняли математическую составляющую ИИШ. Просто не знаю, или пропускать меня, или не пропускать. Можете мне объяснить, что такое математический вся вот эта искусственный интеллект, все, о чем Дмитрий говорил? Можно про вектора? Про вектора мы расскажу обязательно. Про многовекторное непонятно. Это я расскажу. Мне про вектора, я даже спрашивать не буду. Про вектора расскажу. Мне сейчас хочется до вектора понять. Да, да. Ага, Александр. Ну, если вкратце, это генерируемый на основе вероятности ответ, собранный из кусочков наиболее повторяющихся текстов на тот или иной запрос. Ну, это если мы говорим про языковые модели? Да. А я говорю вообще в целом. Минимизация кост-функции. А? О, это ближе, да, конечно. Это ближе к матрице. Ну, давайте так. Повторить, да, наверное? Да, да. Ладно, хорошо, повторил. Смотрите, в чем основная история? Ну, вот этот всякий регрессик, кластеризация аномалии, вы там слышали? список задач, которые есть. В основе лежит очень интересное понимание, что знать не роняется уметь. Если мы знаем правила движения, это означает, что мы умеем водить машину и наоборот. И теперь возникает вопрос. Вот это понимание или умение, оно возникает у нас, как у людей или даже у собачек. В силу чего? Она просто возникает неожиданно? Или нет? Опыт, правильно. А опыт, это что? Повторение, правильно. Но не просто повторение, сидишь и думаешь об этом. Надо же какие-то, наверное. О, обратная связь. Кто стал обратной связи? О, супер. На самом деле, действительно, дело в обратной связи. То есть, когда делается какое-то определенное действие, нами как людьми, или просто животные, мы получаем обратную связь либо от учителя, либо если кто-то дастеровит собаку, либо ее наказывает, либо какую-нибудь вкусняшку кормит. И в этот момент происходит очень интересное явление, которое заметили ученые. Явление происходит в нашей голове. Если наш мозг порезать на кусочки и посмотреть, из чего он состоит, оказывается, что у него есть такое понятие, как нейрончик. У нейрона три элемента. И сейчас вы заметите, что это и такие же самые артефакты. На входе, только если мы говорим с вами про цифровые артефакты, в нашей биологии это электрохимические сигналы. Просто электрохимический сигнал, который входит в тело нейрона, а дальше с ним происходит преобразование. А преобразование очень простое. Либо этот сигнал усиливается, либо он гасится и передается дальше. И, соответственно, выход одного нейрона – это вход другого. И только это миллион, там сто миллиардов раз между собой переплетено в такую сеточку. Так вот, понятие обучения – это когда у нас меняется свойство этого тела. Когда ребенок в первый раз затрагивается до чего-то горячего и получает ожог, или ему неприятно, у него возникает система обратной связи. Если ты в следующий раз увидишь что-то красное и теплое, то формируется белок, который производит адреналин, и у человека повышенное внимание возникает, чувство опасности. То есть мы за счет вот этого очень маленького дативного механизма, за счет обучения изменения тела нейрона, можем очень хорошо реагировать на разные события. И когда математики спросили, а можно ли то же самое повторить в виде алгоритма, но только понят, что мы будем использовать электрохимический сигнал, будем математику, они сказали, давайте так, у нас есть входящие артефакты в числовом виде, это, кстати, важно, артефакты в числовом виде. Мы эти артефакты в числовом виде будем, так же как тело нейрона преобразовывает электрохимический сигнал, мы будем математически преобразовывать. Например, давайте так, аналогия. Тело нейрона в голове сигнал притупляет. Математически это что может быть? Умножение на ноль. Правильно? Логично, не логично? Да, логично, правильно. А если мы хотим усилить? Ну, какой-то там. А? Давайте усилить. Понизить это на ноль. А если мы хотим усилить? Ну, больше, чем на один умножить. Если мы умножаем меньше, чем на один, на 0,9, на 0,8, на 0,7, на 0,6, на 0,5, мы понижаем сигнал. А если мы умножаем на 1,2, на 2, 3, 4, 5, мы его повышаем. От нуля до двух можно вообще все сделать. Так вот, математики в течение этих 70 лет доказали, что если мы правильно подберем вот эти коэффициенты, на которые нужно умножать, мы де-факто сможем симулировать работу мозга. Если мы будем ей показывать, здесь очень важный момент, отличие. Если в первом варианте, когда вы программировали, вы прописывали процесс преобразования исходящего артефакта в исходящий путем, допустим, ключевых слов. вы прописывали этот путь, то заметьте, что во втором варианте вы не прописывали этот путь, вы показывали правильный вход и выход, вход и выход, а связка между ними оставалась на уровне машины. Так вот, математики доказали, что эту связку можно восстанавливать путем большого количества перебора. В этом и смысл вообще нейронной сети. Нейронная сеть – это файлик, в котором хранятся вот эти всякие веса, на которые нужно преобразовать входящий сигнал. Проблема заключается в том, что тебе этот входящий сигнал нужно представить в математике, в числовом виде. Требование к артефакту – это число. Не буква, не звук, а число. В этом был соосновной большой вызов. Но если мы способны перевести входящий артефакт, исходящий в цифровой вид, у нас появляется техническая возможность найти зависимость между ними. Вот помните пример с кошками? С правой стороны вот эти циферки, это что? Пиксели, цвет. RGB-стой цвет? Ну да, цвет пикселей. Все же понимаем, что такое пиксели, да? 10 мегапикселей в нашем телефоне, это 10 миллионов точек. У каждой точки имеет свой цвет. Это цифровой вид. А теперь возникает вопрос. У нас есть два подхода. Если бы мы подходили к задаче распознавания образов алгоритмическим путем, нам нужно было найти какую-то хитрую зависимость алгоритмическую в этих цифрах, для того, чтобы сказать, кошка есть или кошка нет. Например, кошка есть, значит единица, кошки нет, это нолик. Если бы у нас картинка была статичная, это было бы не проблема найти. У нас картинка-то не статичная, у нас может меняться угол зрения. У нас формула начинает идти, плавать. И в этом было, кстати, и некоторое… Рассказывали на сегодня, AlexNet, это когда был конкурс на качество распознавания образов и объектов на фотографиях. До 2015 года качество алгоритмического распознавания было на уровне 80%. То есть ты мог сколько угодно с кошками сидеть, но разнообразие такое большое, что как бы ты алгоритм не подкручивал, у тебя распознавания больше, чем 80% не будет. Но когда поменяли технологию, сказали, давайте мы будем не алгоритм устраивать, мы попробуем показывать машине, что такое кошка, что такое собака и куча-куча объектов. И таким образом в 2016 году уже удалось преодолеть 4% точности и ошибки само по себе этот конкурс закрыли. Ну потому что проблема какая? Алгоритмически. Если мы говорим про кошку, кошка может быть разного ра... Что у нас тут есть? Вот такие у нас есть варианты, например. Кошка может быть под разным освещением. Кошка может быть сливаться с фоном. И мы понимаем, что с точки зрения математики это практически неотличимо. У нас кошка может быть закрыта. Мы видим только хвостик. Кстати, если бы мы в 2013 году сказали бы, А может ли машина, посмотрев на эту фотографию, определить, что здесь находится? И многие считали, что это недостижимо до 30-го года. Но если вы попробуете взять вот эту фотографию, засунуть в какой-нибудь чат GPT и сказать, что здесь изображено, она напишет, что это и хвост кошки, и маленький белый шарик, лежащий здесь, и сумка, и краешек сумки. Она все опишет. То есть мы проделали большой путь. Ну, как бы кошки – это жидкость, поэтому алгоритмически тоже невозможно понять, что изображено. Ну, или вот это очень интересная история. С одной стороны, это смех смехом, но на практике, например, в гистологии, в медицине, одно и то же изображение гистологического снимка может иметь разные диагнозы в зависимости от того, какой анамнез. И нам иногда может просто не хватать контекста. Это, кстати, тоже одна из больших историй, особенно если вы будете заходить на ИИ-агентов. Процесс преобразования одного артефакта в другой потребует еще восстановления всего контекста. Если, например, в вашем случае мы из артефакта спецификации должны сделать артефакт, каталог оборудования, как минимум контекст должен быть весь список оборудования, как минимум, причем вашего, причем еще, скорее всего, с какими-то нюансами. Поэтому как это еще проявляется? Если мы говорим пусть за меня искусственный интеллект сделает анализ. Можно звучать такое-то З. Для меня это стоп-слово. Потому что за словом анализ скрывается такой огромный 20-летний контекст, который для человека подразумевается естественно. Ну как же так? Он же искусственный интеллект. Почему он не знает? Как делать отчетность по МСФО? Вообще странно. Да, как это он не знает, он же искусственный интеллект. А на самом деле он не знает. Он не знает. Поэтому контент крайне важен. Я был в Китае, мы ездили в клинику, в учреждение, мы ездили с брендом Миндрей, китайский такой, производитель медицинского оборудования. Они сейчас тестируют в Китае на семи клиниках пилотно цифровую реанимацию с ИИ, с искусственным интеллектом. То есть, чтобы понимать, реанимация, большой поток данных пациента, прикроватного оборудования, состояния и так далее. И мы приехали в клинику и общались с заведующим отделения реанимации. Он рассказывал кейсы, которые они применяли, не применяли, что касательно контекста. То есть, был кейс, при котором данные пациента Ишка съела, они для нее спросили и действительно дал такой, так скажем, прогноз лечения и употребление фармы и так далее, которые врачи не увидели, потому что они не считали некоторые нюансы. А был кейс такой, что и съела, но был скрыт контекст, потому что пациентка не сказала, что она больна диабетом. И, соответственно, входящий анамнез, он был неверен. И она была неверная прогноз лечения. И вот что касательно контекста. Да, вообще мы контекст, очень хороший, кстати, пример, по поводу медицины тоже есть интересные байки. Это, знаете, байка такая, это же не байка, это реально было исследование в июле 22-го, когда COVID начался, в 20-м году? В 20-м, да. В июле 20-го года вышло метаобзор 400 видов алгоритмов, позволяющие тебе оценить по ПКТ наличие пневмонии. Так вот, было рассмотрено 400 стартапов, потому что в этот момент огромное количество стартапов начало возникать, которые делают автоматическую постановку пневмонии. Как вы думаете, из этих 400 алгоритмов, сколько реально оказались клинически эффективными или доказанными? Вы читали эту статью, да? Нет? Нет, действительно не один. Там половина просто ушла, потому что это был полный бред с точки зрения доказательства. Но было очень интересно, очень красиво, хорошо работающее с референсами, красиво обоснованного алгоритма, но работало совершенно интересным способом. Это была группа, условная группа, которая связана с каким-то университетом в Штатах. И их модель показывала очень высокую точность. А сделана очень хитрым образом. У них был датасет входящий, где позитивная пневмония приходила из их референсного центра. а негативные из других референсов, из других больниц. То есть они взяли патяну, они работали в этом референсном центре, поэтому понятно, что у них было очень много позитивных, потому что доказанных уже, как датасет, разметка. А негативные они собирали условно со всей страны, из всего штата. И они обучили на этой истории, получилась очень высокая точность. Как вы думаете, в чем был прокол? Был признак, откуда взяли… Вот, вот, вот. А как вы думаете, какой взяла машина признак? Какой взяла? Из какого центра поступила карточка? Правильно. Она взяла в качестве признака штамп на КТ. Название больницы. Потому что это был очень абсолютно явный признак. Если у тебя входящий с названием, условно, институт Томаса какого-нибудь, это означает, что там точно позитивно. потому что оттуда был позитивный дата-сет, и прошли именно из этого референсного центра. То есть она определяла по очевидный класс, именно по названию. Вот это такой очень интересный пример. Он красиво был описан, но вот проблема первичных данных. Поэтому контекст действительно крайне важен. Теперь мы с вами идем дальше. Если мы говорим, что вот в этом случае у нас в качестве входящих циферок были пиксели, Давайте у вас такой контрольный вопрос. А если бы у вас были бы клиенты, и вы бы хотели классифицировать клиентов каким-то классом или что-то определять, вам что нужно было бы сделать? Проверочный вопрос. Вы бы захотели, например, использовать какую-нибудь обычную ML в отношении ваших клиентов. Вам как клиентов нужно было представить? А можно вопрос? А если бы они догадались и этот признак... Кто говорит? И этот признак нивелировали. Она бы тогда не ошибалась? Ну, это уже история умалчивает. Не знаю. Я просто положу. Наверное, она бы... Я не знаю. Она точно была бы лучше. Потому что не использовал бы это. Так, вот давайте с клиентами. Представим себе, что мы хотим... Так, а у нас, напомните, кто у нас тут был кейс безопасности? У кого это было? А, да, у вас. Помните то, что я рассказывал? Нам нужно что сделать? Если мы хотим использовать нейросетку в отношении не картинки, а в отношении какого-то объекта. Первое, близлечье, структурировать, причем каждый структурированный параметр нужно представить в виде числа. Если мы это способны сделать, от самых простых, например, если мы хотим кластрализовать наших клиентов. Но кластрализовать их не по принципу больше, чем на миллион рублей, это класс А, по какому-то базовому признаку, алгоритмическому. А если мы хотим кластеризовать по более сложному, нам, возможно, придется наших клиентов параметризовать, найти какие-то признаки. Есть маркетинговые признаки, частота покупки, средний чек, время. А может быть какие-то классы, например, связанные с его деятельностью. Какую-то байную систему, что-то. То есть если мы способны нашего объекта, с которым мы работаем, наш артефакт, представить в конечном итоге в структурированном виде, делать это достаточно алгоритмично, то у нас открывается куча возможностей, связанных с анализом, с кластеризацией и так далее. И все было бы хорошо, пока не было бы у нас таких задач. Это мы сейчас все пропустим, потому что времени нет и уже подустали, наверное. Пока у нас не возникла бы история. Это как раз включая на ваш вопрос, Михаил, по поводу векторов. Пока мы не столкнулись с проблемой параметризации текста. Потому что параметризировать картинку легко, она по умолчанию цифровая, параметризировать какие-то объекты, ну, понятно, какой-то экспертный взгляд, как мы можем обозначить нашего клиента. У нас была целая большая вагона, маленькая тележка артефактов, текстовых, запросы клиентов, договора, назначения платежей, огромное количество, которые хотелось бы как-то автоматизировать, но мы при этом не понимали, как это сделать. Так вот, работа с текстом. Как вы думаете, что было сделано или как было сделано, чтобы научиться его представлять в числовом виде? Векторы, да. А что такое векторы? Вот в вашем понимании. Сейчас потом в своем понимании скажу. Ну, а и что расположение? Точка, хорошо, точка математического права. Я здесь как математик, у меня подкладная математика в факультете. Я понимаю, что точка пространства. но нам же нужно провести какую-то абстрактную связь между точкой пространства и мамой мыло раму. Вот найдите связь между точкой пространства и какое-нибудь выражение. Давайте мы будем говорить. Выражение лучше через микрофон, потому что не все заслышано. Мы говорим сейчас про запрос мамы мыло раму, либо как это превратить в век. Понимаете, у нас в любом случае при работе с искусственным интеллектом, мы не важно с каким, у нас происходит его преобразование в число. Картинка, понятно, как в число преобразовывается. Если вы там делаете прогностику, вы сами занимаетесь тем, чтобы придумывать эти параметры в число. Это понятный ход. Но когда мы заходим на понятие с текстом, все становится сложнее, потому что в тексте главный смысл. Не только буквки расположены между собой, это вообще не... Клиент может матом тебе сказать, что он хочет. И ты его поймешь, а сделаешь ему КП. Ну, по крайней мере, хороший портамес, но это можно сделать. Хотя мы понимаем, что с точки зрения Мата там у него не такой большой разобразный лексикон. Но мы в целом понимаем. Приводить примеры не буду. Нам крайне важно уметь работать со смыслом. И тут произошла очень интересная история. Естественно, поисковые системы, когда первые поисковые системы в 2000-х годах, они искали по ключевым словам. Это тоже было очень хорошо ключевое слово. Потом возникла история, у нас по таксономии, уже не выговорю к концу дня, потом был следующий шаг, попробовать с учетом морфологии искать. Я еще просто помню время, когда нужно было без учета морфологии вбивать это ключевое слово. Там была альтовиста, яха и все остальное. Потом нужно было писать в любом падеже, и он там искал. А теперь, если мы заходим на Яндекс или на Google, мы пишем вопрос абстрактный, получаем, особенно сейчас с нейропоиском, мы получаем ответ. Это эволюция произошла. Она произошла из-за того, что в 2015-2016 компания Google поставила перед собой задачу. Давайте мы, посмотрев огромное количество текстов, которые есть в интернете, создадим их такое математическое представление, только не руками, а математикой, которое будет обладать очень интересным свойством. Если два предложения являются близкими по смыслу, например, они находились на одной и той же странице, как вариант, при считывании, то они должны быть математически близки. И вот здесь мы как раз переходим к понятию точки. То есть не просто точка в пространстве, а у нас, допустим, Если две точки в пространстве находятся близко математически, мы можем почитать математикой, то значит и фразы, которые соответствуют этим точкам, тоже по смыслу очень близок. Можно небольшой комментарий. Они, чтобы это делать, на самом деле, брали, выдергивались из текста слова, вернее, брали из текста слова, у которых одинаковый контекст, то есть, которые находятся в одинаковом контексте. и дальше требовали от модели, чтобы у этих слов были похожие вектора. Ну да, потому что размечать не надо было, все работало само. Не надо было кому-то... Да, да, да, это, кстати, важный момент, действительно, потому что физически это размечать было невозможно. А теперь я сейчас покажу вам на практике, что это такое. Для того, чтобы у вас возникла первичная картинка, я немножко дополню, помните такая была, может быть, кто помнит, была такая игра Акинатор, где ты должен был, ты заказывал, загадывал персонажа, а он тебе, машинка, задавал 5-6 вопросов, и она отгадывала. Как она это сделала? У них каждый персонаж был параметризован по 4 или 6 параметрам. И когда она спрашивала, вы загадали мужчину или женщину, отвечала мужчина, она там часть просто баз данных убирала фильтром. И вот оказывалось, что достаточно 7 или 8 признаков, каждый из которых имеет всего 3 варианта значения, либо да, нет, либо не знаю, для того, чтобы описать практически всех героев всех сказок. Ну, потому что там 7 в третьей степени, 3 в седьмой степени, это очень много. Там много всего было, да, там не только сказки. Ну, я сейчас образом говорю. Да, вы правда, я был известный человек. А теперь представим себе, что сделал Google. Он сделал следующее. А давайте мы возьмем не 7 параметров, а возьмем условно 4000 параметров, и каждый из которых имеет значение не 3, а 10 в 18 степени, больше, чем звезд на небе. Это открывается такое огромное математическое пространство, которое де-факто может уместить всевозможные смыслы. Что сейчас будет сделано на экране? У меня сейчас на экране такой маленький визуализатор вот этих всех точек. Мы с вами сейчас придумаем 6 предложений. Точнее так, 3 пары по 2 предложения. Причем эта пара должна быть близка по смыслу, по каким-то признакам. Первое, что я сейчас сделаю в качестве примера, это пара про птиц, которые летают вокруг какого-то водного объекта. То есть там будет общий смысл по признаку, что это птица. Там будет общий признак, потому что они летают. И третий признак, что это вокруг водного объекта. Это будет звучать так. Чайка летит над морем. Поклан кружит около озера. Сознательно не использовал одни и те же слова. Придумайте, пожалуйста, еще какую-нибудь пару, которая близко к каким-то параметрам, по смыслу, только не с птицами, не с водным объектами. Может, про криптовалюту, про Динеску, про айтишников, про еду, про все что угодно. Про Сколково. Как вариант, да, поезд едет по рельсам. Так, что? Что? А в чем здесь? Даже я, как человек, не понимаю, в чем... Сапсан мчит в Питер. Ну, хотя бы так. Сапсан мчит в Питер. Сапсан птица, так что интересно будет. Ну да, он ближе будет, это интересно. То есть на Сапсан... Давайте чайка-менеджер готовит КП. Ну, как, знаешь, наш отечественный сирий всегда поломает систему, попробовать. Ну давайте мы хотя бы первый шаг для обучения сделаем контрастно, потом поломаем, так будет интереснее. Давайте так, Сапсан мчит в Питер, ну ой как интересно, я даже не знаю куда он засунет это. Если он знает что такое Сапсан, ну посмотрим, ладно, даже интересно. Так, и давайте какое-нибудь еще продолжение пару. Ну давайте, Чайка-менеджер готовит что-то. А Чайка-то фамилия что ли? Это образное выражение таких категорий. Ну давайте, Чайка-менеджер. Я не понимаю эту фразу, если честно. Чайка-менеджер. Если мы хотим набор слов поломать, давайте что-нибудь тогда другое. Чай остыл, например. Чай остыл, давайте, хорошо. Чай остыл, но давайте что-нибудь с едой тогда, может быть, с готовкой с едой. Борщ пересолил, и соли пересолил. Борщ. Борщ пи... Пересолил, а наверху... Соли переборщил. У каждого вот это предложение, у него появилось его математическое представление. Я использовал модель Коупен Ай. И так как мы четырехтысячмерное пространство представить себе не можем, но это невозможно его представить, оно собрано до двухмерного пространства. Вот если я помещу чайку в какой-нибудь в левый верхний угол, то баклан где будет? Внизу? Там же, да, он будет там же, он будет рядышком. То есть я предполагаю следующее, это мое личное человеческое предположение, что у нас чайка и баклана будут относительно близко. Мне почему-то кажется, что он Сапсан плохо знает, поэтому поезд едет по рельсам Сапсаном через битр будет близко. То есть соль и борщ, скорее всего, будут близки, а вот поезд и Сапсан, я почему-то предлагаю, что они будут подальше друг от друга, чуть-чуть. У нас будет игру. Посмотрим, что сделала машинка. Даже самому интересно. Ну, кстати, смотрите, она прям даже хорошо сделала. Да, да, хорошо. Удивительно. Смотрите, соль переборщил, чайка и сапсан. То есть у нас получилось три группы. Что вы сейчас увидели? Вы сейчас увидели математическое представление, правда схлопнутое в двухмерное пространство, специально, потому что мы не можем четырехтысячмерное представить. И у ней хорошее свойство. Значит, что каждый текст, с которым мы работаем, у нас существует технический механизм превращать его в цифру. Для того, чтобы у вас возникло полноценное понимание, есть такая у меня визуализация. Кстати, расстояние считается очень просто. Чем ближе точка, тем ближе у нее так называемое гифклидовое расстояние. А если они смотрят в одну и ту же сторону, это называется косинусная близость. Смотрят они в одну сторону, значит единица косинусная близость. Если в разные стороны, то там минус один. Если перпендикулярно, то там ноль. Смотрите, возьмем вот эту собаку, которая разговаривает на лугу. Чем ближе мы будем помещать нашу красную точку, тем ближе она будет становиться похожа на собаку. Да, собака разговаривает на лугу. Теперь начинаем идти вправо. Кто-то там разговаривает. Вот разговаривает по этой оси. Да, он разговаривает. Если пойдем вниз, кто-то что-то делает в парке. Пойдем по диагонали, это будет медведь или так далее. То есть что у нас получается? У нас у каждого из этих точек пространства, это пример сгенерированный, специально для некой визуализации. Самое главное, что у нас у каждого из этих элементов, если кому нравится в трехмерном варианте, можно в трехмерном варианте посмотреть что-то. Вот здесь у нас про каких-нибудь животных, которые где-то бегают. Здесь у нас про каких-то птиц. Если бы мы посмотрели здесь, кто-то где-то летает. Но трехмерное пространство мало, их там 4000 мерное пространство. Для чего нам это надо? А можно вопрос? Да, давайте так. А координаты у них как-то зафиксированы, базовые? Ну, в смысле, ноли одинаковые? Ноли одинаковые? Не знаю, мне кажется, мы так даже не мыслим. Нет, ну в смысле тысячемерная, а нули относительно друг друга, вернее, как-то они сочетаются друг с другом? Это матрица. Давайте так, есть модель, которая преобразовывает текст в этот вектор. Вот в рамках вот этой модели, вот в рамках вот этой выбранной модели, допустим, OpenAI отдельная модель, у QN, у WPSEC другая модель, там есть кодеры так называемые. Вот в рамках их собственного пространства, у них все пространство, оно как бы фиксировано. В ноль везде, условно везде один. Но между разными моделями, естественно, там будет разное. А почему тысячемерное тогда? Ну, потому что... Ну, в смысле... Ну, облако, получается... У него тысячи координат. Вот смотрите. Двухмерное пространство – это XY. Трехмерное – XYZ. Четырехмерное – XYZ. Там Y. И там тысячемерное пространство. Мы просто не можем себе представить. Мы как бы это... И не надо. А? Там точка на сфере. На многомерной сфере. Слушайте, давайте так. Мало кто из нас здесь способен представить многомерное пространство. Попробуйте представить четырехмерное. У нас не хватит нашего математического абстракции для того, чтобы хорошо рассуждать в многомерных пространствах. У нас просто не хватит антологии базовой. Поэтому давайте даже туда побужаться не будем. Считайте тысячи признаков. Или четыре тысячи признаков. Может быть, так даже проще. Что нам это дает? Нам это дает несколько интересных вещей. Ну, во-первых, чисто прикладная вещь, которая нам совершенно без всяких языковых моделей может пригодиться. Вы сейчас скажете, что это может быть. Следующая абстракция. У меня было 300 афоризмов. За афоризмом находится смысл. Я взял каждый афоризм, представил его в таком векторном виде. А потом я начал задавать всякие вопросы. в чем сила, афоризм про что-то еще. И я сам вопрос тоже привел в числовой вид, векторный. И я посмотрел, а какие афоризмы математически ближе всего к моему запросу. И когда я задал вопрос, в чем сила, и отсортировав это от начала до конца, оказалось, что из всех трех афоризмов, математически, обращение к вопросу, в чем сила, был близок афоризм, сила в ньютонах. Обратите внимание на второе. тень Чака Норриса сминает все, на что он нападает. То есть тень Чака Норриса сминает все, на что он нападает. Если в первом случае мы можем сказать, ну там совпадение по слову, слово силы и так далее, то мы понимаем, что во втором примере у нас вообще нет никакого пересечения. Это настолько большая абстракция, причем Чак Норрис, причем сила. Но в нашем культурном коде и в большом количестве текстов оказалось, что Чак Норрис всегда ассоциировался с какой-то сверхсилой, И поэтому по многим параметрам они были близки из всех остальных. То есть мы работаем с форизмом. А теперь давайте перенесем вот этот абстрактный пример на что-нибудь такое более-менее реальное у нас. Где мы можем это использовать? Ну где это вот такое свойство умения работать по смыслу? Включим абстракцию немножко. Тепло. Тепло-тепло. Еще, ну, нет, безусловно. То есть, вот все, что вы называете, абсолютно правильно. Действительно, классификация, более того, когда вы размечали во второй игре, там за каждым этим отзывом стояло математическое представление. А где, в каких случаях удобно делать приоритетность поблизости? В ранжировании только чего, ну, докручиваю мысль. Ну, вот близко, близко ходите. Ну, смотрите, ладно, не буду мучить. О! Да, хочется не ходить. Да, сейчас сядут, ну. Не дому еще, ладно, здесь сделаю по старинке. Можете ловить? В чем смысл? Смысл в том, что если вы будете, допустим, у вас будет на сайте какая-нибудь система поле, где человек может вести, выбрать какой-нибудь продукт. Если вы будете делать запрос просто классическим способом, найди мне, пожалуйста, я хочу какую-нибудь хрень железную с резьбой на 2 мм. При алгоритмическом поиске 1С вообще вам ничего не выдаст. Но если вы переведете запрос в векторный вид, то окажется, что такая железная фигня с резьбой 2 мм будет очень близко к болту М2. А если железная фигня с гайкой, то это винт. Она просто это поймет. То есть идея заключается в том, что мы можем перейти от поиска по ключевым словам по парсингу к поиску по смыслу. Потому что смысл теперь возможен в математическом виде. Это хорошая история, когда вам нужно сделать, допустим, на практике. У вас есть какая-нибудь форма на вашем сайте, и люди должны набрать какой-то продукт. Было бы здорово, когда они набирают продукт, у него возникает список наиболее близких по смыслу, а не по буквам. Это дает большой прирост. Но плюс это еще используется. Это же технология в рагах. Я думаю, что про раг слышали, наверное. Только там в качестве выбора идет не маленькие кусочки текста, а достаточно большие. Ну и давайте так. Некий шаг. Для чего я это рассказываю? Вообще работа с текстом как со смыслом, она имеет практическую основу. Но это дало нам возможность создавать большие языковые модели. Потому что в языковых моделях происходит еще один красивый шаг. Сейчас попробую, конечно, установить. Не, не работает. А, не работает, потому что я не тот взял. Где у нас там какая-нибудь картиночка красивая будет у нас? Сейчас найду. Большая языковая модель, когда вы получаете к ней запрос, она преобразует ваш запрос в этот векторный вид. А дальше происходит очень интересная магия. Сама по себе языковая модель хранит в себе огромное количество ассоциативных связей. Представим себе, что вот здесь в вашей векторной пространстве находится кошка. Вот здесь находится котенок. Кошка-котенок. Я переношу в другое место, и это указывает на собаку. Это на что будет указывать? Поняли смысл? Смотрите, кошка, котенок. Переношу. Собака, щенок. Столица, Москва, Минск, Вашингтон. И у нас получается система многомерных ассоциативных связей, которые находятся в многомерном пространстве. Так вот, большая языковая модель, когда мы говорим, что такое знание большой языковой модели? Это огромный массив Вот этих ассоциативных связей Не база данных Не листочки в которых он там ищет Не текст Это система взаимосвязанных между собой Ассоциативных рядов Которая оперирует вот этими числовыми пространствами Что это смысл в этом Это смысл в этом И если крайне упрощенно говорить Мы сейчас не говорим про внимание Аттеншин и все остальное То попадая в жернова большой языковой модели Ваш запрос преобразовывается сначала в его числовую форму, проходит через фильтры ассоциативной связи и выплевывается новый вектор, который снова преврожается в текст. Поэтому из этой позиции, как вы думаете, почему люди, точнее так, помните в самом начале, когда мы пытались зайти в чат GPT и попросили его найти что-то, там года два или три назад, Он писал всякую фигню. Давайте так, почему так происходило? Мы говорим, найди меня, пожалуйста, в интернете. Какой-нибудь страничка, там, не знаю, просколковая. Вот что в этот момент происходило? То есть мы ожидали, что он пойдет в интернет, давайте так. Как поисковая система. Он пойдет в интернет, там что-то найдет. Но на самом деле ничего не происходило. В этот момент он кидал этот запрос в свою внутреннюю ассоциативную связь и пытался понять, сгенерировать слово за словом наиболее близкое математическое представление и ответ на ваш вопрос. Используя ассоциативную связь. То есть, так же, как мы, когда люди слышим какую-то новую… Как работает ассоциация? Давайте просто маленький такой пример, чтобы вы почувствовали. Вспомните, сколько пар животных взял с собой в ковчег Моисей. Сколько? Каждый тварь по паре, а сколько конкретно? Сколько взял Моисей в ковчег, этих животных себе в ковчег? Смотрите, во-первых, что сейчас произошло? Моисей никаких животных не брал, брал ной. У нас возникла ассоциативная связь. Второе, вы сказали 40. А вы знаете, что практически все говорят 40? Потому что Моисей связан со словом 40. Там 40 заповедей и там все остальное. 40 лет, он водил. Вот что такое, вот сейчас у нас прошла типичная галлюцинация языковой модели. Теперь вы понимаете, теперь вы переживаете. Смотрите, что сейчас произошло. Во-первых, 40 лет в отдел, у нас первая ассоциативная связь. Моисей, что-то с 40 связанных. Во-вторых, ковчег, нам это не важно. Мы услышали слово ковчег. Мы не проявили ассоциативный связь, что Моисей и Ковчег у нас не было. И вот такими же ассоциативными связями владеет генеративная модель. Понятно, что она более внимательна. Но у нее происходит то же самое. Она нигде не ищет. Она из себя выдает. Она пытается найти наиболее близкие математические формулы, чтобы выдать вам результат. Но это вы сейчас понимаете. И, кстати, вы сейчас себе почувствовали, что это галлюцинация. Галлюцинация – это как раз неправильно вот эти ассоциативные связи, которые вытащились. Вот они так вытащились. Потому что когда вы отвечали, вы были более-менее уверены в этом. И машина в этом более-менее уверена была. В общем, заметьте, все отвечают 40. Все отвечают 40. Кстати, кому будет интересно, Даниэль Канеман, думаю, медленно решай быстро, у него такая история как раз про Ковчег и про Ноэ. Да. Вопрос? Генеративный понятно, а ML также работает? Слушайте, в ML немножко попроще, потому что, смотрите, ML у ней… Генеративная модель – это как бы следующий уровень ML, но следующий уровень. В ML там все-таки на базовом… Если в генеративной модели, во-первых, сами по себе ассоциативные связи не в состоянии описать все возможные комбинации, во-вторых, в генеративных моделях на последних шагах всегда возникает так называемый выбор случайного числа. Там даже параметр такой, если там топ-к или топ-п. Если в обычной ML там нет никакого случайного, вход и выход, он в принципе всегда стабильный. Он всегда стабильный. Он может не соответствует действительности, но он стабильный. А в генеративных моделях там специально сделаны слои, которые включают определенного рода случайное число. Поэтому у вас никогда не будет одинакового ответа. Это просто архитектурно так сделано, потому что посчитаешь, что так будет лучше. Если бы этого не было бы, она бы, кстати, давала бы... И там, и там, и там. В случае генеративных моделей вектора это то, что отражает смысл. А в обычной ML-ке какие-то характеристики, циферки, параметры и так далее. Ну и давайте так, чтобы у вас просто завершилось это все понимание, вот эта способность генеративных моделей на входе принимать какое-то число, преобразовывать, исходя из своих внутренних связей, давать что-то другое, было обернуто в совершенно разные виды приложений. И мы как раз на этом и завершим. А в какие виды приложений это было обернуто? Мы сейчас не будем показывать, какие бывают типы преобразований, нам это сейчас просто уже тяжеловато. просто зафиксируем следующее. Сначала это обернули в так называемые чат-боты, в чат GPT. Мы все заходим и там с ними работаем. Проблема заключается в том, две проблемы. Первая проблема, что все начали думать, что это поисковая система, хотя она из себя выдавала, и только через полтора года прикрутили к чату GPT, ко всем моделям, к Яндексу, к чату функции поиска. Кстати, что он в этот момент делает? Он понимает, что тебе запрос пользователя идет на поиск, вызывает отдельную поисковую систему, совершенно отдельную, та ищет, возвращает обратно результат, а она дальше отрабатывается. То есть она поднимает какую-то металл, говорит, я ничего не знаю, вот, пожалуйста, я могу сделать машину, языковая модель может сформулировать поисковый запрос, но она не может сделать действий. Но при начале это было обернуто исключительно просто в какое-то личное общение. Следующее, какие бывают уровни. Мы можем использовать эту функцию преобразования вообще без интерфейса. У вас может быть несколько систем, у нас, по-моему, вчера был такой вопрос. Да, вот был. У тебя может быть несколько систем, допустим, мэппинг каких-нибудь, вот у меня был такой клиент один, у него есть база данных, как бы управленческая база данных, где указывается, где рабочий что делает, и была кадровая база данных, где написаны вы должностные обязанности, ну, по договору. Система каждый вечер проверяла, там десятки тысяч рабочих, проверяла, соответствуют ли назначенные на местах работы этому человеку его должностным инструкциям. И если она не соответствовала, то формировалась на утро задача для кадрового отдела либо поговорить с филиалом, сказать, почему ты назначил Васю на ту работу, которая не подосмотрена, либо изменить договор. Здесь нет никакого интерфейса. Для кадровика это просто был набор задач на сегодняшний день. Пойди посмотри какой-то филиал. ввелял номер А, там, Вася Петров, у него изменился его список работ, который теперь не соответствует должностным инструкциям. Ну, такая там у них система была. Здесь нет никакого интерфейса вообще. Или система, когда вот BitFinance, например, система распределения заявок по расходам. То, пожалуйста, приходит заявка, сама распределяется, назначается расход и идет дальше. Следующая история, когда мы этот чат-бот, как элемент реализации, мы его начинаем дополнять разными функциями. Это talk call, который был назван. Но эти функции только ваши уже. И в отличие от первого варианта, если в первом варианте поведение, системную инструкцию задают разработчики, поэтому иногда сливают все системные инструкции, клоды, системные инструкции, чат-жепети. Это большая многостраничная правила, по которым должен общаться. Ты должен вежливо общаться с людьми, ты должен отвечать на все вопросы и так далее. Но в случае, когда ассистент делается как чат-бот или ваш внутренний. Вы сами определяете эти правила. Вы как бы сами определяете, сами отвечаете. Следующая история. Ассистент как часть большого приложения. Это что, например? У меня есть открытый один вариант. Мы делали для IECа систему распознавания схем электрических. Сейчас я посмотрю, если у меня тут какой-нибудь примерчик. Схема. Схема. Схема, схема. маленькую возьмем, откроется. Смысл в том, что это электрохимическая схема, схема щитка электрического, которая на входе является PDF-ным файлом в качестве типа этого артефакта. А на выходе у нас спецификация. Вот созданное вот это приложение, посмотрим, что у нас там получится, и получится ли, потому что я даже не знаю, достаточно большая схемка у нас получилась. Ладно, пусть думает. Пусть думает, пусть думает. Как сюда? Эффект демонстрации. Проверка, наличие схем, определение их сложности, как первое. А для того, чтобы можно было преобразовывать, допустим, сейчас мы возьмем простенький. Да, вот мы берем какую-нибудь такую спецификацию, когда приходит от прораба, а из нее нужно сделать полноценное коммерческое предложение. В случае у коллеги это было в виде Word, а здесь вот так приходит клиент. У них процесс такой. На входе у них PDF, на выходе у них спецификация с оборудованием. Это процесс передела. Соответственно, я просто руками это сделаю, мы не будем делать это автоматически. Вот у нас есть какой-то элемент. Мы нажимаем распознать. Он понимает, что там нарисовано, какой тип автомата с его характеристиками. дальше можно скачать это в Excel. Так вот, в этом решении условно затраты по времени, что он здесь написал? Он здесь написал автоматический выключатель 3 штуки. Ну все правильно. Автоматический выключатель. А он у вас сравнивается с спецификацией? Нет, он создает спецификацию. Нет, но там же в этом же проекте есть спецификация. Нет, это на входе. На входе в таком виде приходит. Не сложно прокомментировать. Ну, приходит, приезжает, коллеги, пожалуйста, посчитайте, сколько стоит создание такого электрического щитка. Вот вам PDF с его однолинейной схемой. На входе PDF, на выходе спецификации. Ценой. Без текста. Ну, просто обычный проект идет там с как минимум общей данной, планы. Ну, Ну, то есть, я не сказал, что это… Ну, короче, бывает по-разному. Я сейчас просто… Мне даже вопрос не сколько конкретного этого примера, а сколько что вот здесь… Да, да, да. Да, то есть, здесь основная задача была… Здесь история генеративных моделей, просто маленькая часть во время распознавания. Маленькая часть. А вся остальная обвеска, авторизация, сохранение и так далее – это история алгоритмическая и обернута в какое-то приложение. И на мой взгляд, вот эти приложения являются и очень интересным, и, наверное, самым перспективным решением, позволяющим тебе действительно делать маленькие такие копи-лотики. Раньше была мода на чат-боты, но, честно, на мой взгляд, история с интерфейсом в виде чат-бота является ограниченной. А вот история с приложениями, где у тебя есть поля, кнопочки, особенно когда ты встроен в твой интерфейс, вот это имеет хороший большой потенциал. Есть еще одна история, это агентские системы. Это когда, ну, в моем случае, курсор, ну, про курсор, потому что мы все не видели, Курсор Manus там вчера показывали. Это агентские системы, которые позволят вам создавать другие приложения. Вы скорее будете их использовать, чем создавать самостоятельно. У нас уже 7.25. Я предлагаю, может быть, завершить какую-то базовую часть. И, возможно, есть какие-то вопросы, которые хотелось бы обсуждать. Да, давайте. Вопрос как гипотеза, наверное, появился. Может быть, был опыт и как-то делали. что если загнать клиентскую базу в векторную? Ну, то есть все их атрибуты, по сделкам, по компаниям, по контактам. Сможет ли он проводить анализ? Анализ чего? Ну, допустим, выявить какие-то паттерны, которые классифицируют клиента. Ну, то есть такой анализ, который можно использовать для дальнейших, я не знаю, гипотез по маркетингу. Это интересный вопрос. Потому что вот он мне пришел сейчас, и я такой думаю, а что мне попробовать? Ну, может, было бы. Первое, я попробовал бы. Вообще, смотрите, почему я завершил на этом историей, потому что все, о чем мы с вами говорили до этого, оно в любом случае будет в конечном итоге приземляться в какое-то приложение. Она будет приземляться либо в чат-бот какой-то, либо она будет в приложение, либо в какую-то middleware, если мы говорим про генеративку. Понятно, что вам в рамках автоматизации процесса вам нужно смотреть не только на зелененькую часть, которую здесь играют, вам нужно смотреть вообще на весь процесс. И, скорее всего, у вас будет там целое большое сочетание. Но она часто сопровождает создание, в том числе проверки гипотезы. Я бы попробовал, хотя сейчас скажу, это интересно. Знаете, как можно попробовать? Может быть, вам имеет смысл добавить в качестве вектора еще какую-нибудь качественную информацию, может быть, о клиенте, какую-нибудь информацию, может, чем занимается. Ну, это чтобы у вас... Ну, там главное не попасть на ограничение кантин, потому что чем больше, тем больше будет размыто. Ну, короче, пробуйте. Давайте вот первый шаг превратить текст, описание клиента в вектор, он понятный. Интернерный вектор, сделать кластеризацию, Это уже ML. Вот, кстати, в вашем случае это сочетание ML, кластеризация, о чем Дмитрий говорил, и первого шага генетики. Вы с помощью превращения эмбейдингов, вы превращаете текст в вектор, и все сводите к обычной ML. Поэтому да, прикольно, и незачем получится. Слушайте, ну вам, вот, к сожалению, слово превращение в вектор, это так, это утилитарная задача, она не выведена в интерфейс в явном виде, Я даже не помню. Поэтому это скорее больше инженерная задача. Там есть специальные опишки. У OpenAI есть, у Яндекса есть, у Гигачата, они вообще все эти модели там по-разному бывают. Либо они от себя отделяют, например, у меня есть QAN3, у меня есть QAN кодер. У меня есть OpenAI от GPT, у меня есть там текст имбеддинг. Грок, например, от себя не отделил это. У Яндекса отделил Яндекса Яндекс имбеддинг. То есть некоторые компании отделяют от себя как отдельную у Microsoft есть много open-source моделей, а некоторые не отделяют. Это как бы первый тезис. Но их можно поставить, они прям есть, они готовые. Есть специальные лидер-борды отдельные, там есть специальные тесты. Но побеждают. Последние облачные модели, последние квен-кодеры очень хорошие. Но есть там совсем золотые. Золотой стандарт, типа Microsoft E5 называется. Либо совсем древний Берт. Поэтому вы ставите себе, ну это используйте технически. Но это нужно программист какой-нибудь, хотя базовый, который с API-шками работает. Там очень простой вопрос, на входе текст, на выходе вектор. А дальше вы там математически его записываете. Ну либо базу данных, либо векторную базу данных, либо вазгря, их там много всего. Квадрант, минус, и остальное. А вот если говорить про безинтерфейсные системы, а что они себе представляют? То есть это экзешник исполняемый? Это обычно выворачивается какой-нибудь микросервис. Микросервис совсем. Обычно. Либо это может быть совсем растворено. Традиционно люди пишут с микросервисами, но это может быть просто растворено в коде. Совсем растворено в коде. Единственный момент. У вас все равно придется вызов по опишке, потому что практически любая библиотека всегда содержит вызов по опишке какой-нибудь либо облачный, либо локальный. Поэтому совсем-совсем локально мало что может. Хотя есть такие варианты тоже. Так. Ну что? Все, все. Да, все, уже перегруз. Перегруз, я понял. Чувствую, что бетонная плита на классе лежит. Друзья, значит, мы помним, задачка не отменяется. Мы все это рассказываем не просто ради просветительства, а ради того, чтобы мы потом пособирали свои процессы и разметили их. Завтра еще раз мы этот шаг сделаем по разметке процесса. Сегодня мы заканчиваем, выдыхаем. А завтра у нас, соответственно, мы в агентную часть зайдем, еще ее сильнее расковыряем. Она у тебя здесь как раз нижнего уровня. И я ее сегодня у Дмитрия, который был до тебя, он тоже на нее заходил. А завтра мы там в платформах ее, в морфологии, во всем посмотрим и расковыряем эту агентную часть. И, соответственно, к вечеру и, соответственно, к воскресенью нужно будет уже с прототипами разбираться. Какие прототипы делаем, на чем делаем. И уходим отсюда с гипотезами о прототипах. В воскресенье защищаем гипотезы о прототипах. Смотрим на них. И это будет уже как бы сильный шаг, с которым мы уйдем. Мы до следующего модуля еще попробуем что-то сделать. Работы. Непочатый край. Поэтому, если у кого-то есть желание поработать в группах сейчас, Я не буду вас останавливать Но еще, конечно, открытый бар Там и все такое Итак, все, спасибо большое Поговорим Дмитрия Завтра Ань, в 9 же, правильно? Да, завтра в 9 Йоги нет В душ не ходите В 9 часов прямо здесь Без душа, да, бездушные